{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P34972_1_500\n",
      "model_file/0_GAFSE_Ki_P34972_1_500_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P34972_1_500_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P34972_1_500_test.csv\"\n",
    "test_active = 500\n",
    "val_rate = 0.04\n",
    "random_seed = 2\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0           CCCCCC1=NN(C(=C1)C2=CC=CC=C2)C3=CC=CC=C3 -3.468790\n",
      "1  CCCNC(=O)C1=NN(C(=C1C)N2C(=CC=C2C)C)C3=C(C=C(C... -3.290769\n",
      "2  CC(C)(C)C1=CC(=CC(=C1O)C(C)(C)C)C2=CSC(=N2)C3(... -2.247973\n",
      "3  C1CCCC(CC1)NC(=O)C2=CC3=C(N=CC=C3)N(C2=O)CC4=C... -1.342423\n",
      "4         CCCCN1C(=C(C=C(C1=S)OC2=NC3=CC=CC=C3O2)C)C -2.004321\n",
      "number of all smiles:  2591\n",
      "number of successfully processed smiles:  2591\n",
      "                                              smiles     value  \\\n",
      "0           CCCCCC1=NN(C(=C1)C2=CC=CC=C2)C3=CC=CC=C3 -3.468790   \n",
      "1  CCCNC(=O)C1=NN(C(=C1C)N2C(=CC=C2C)C)C3=C(C=C(C... -3.290769   \n",
      "2  CC(C)(C)C1=CC(=CC(=C1O)C(C)(C)C)C2=CSC(=N2)C3(... -2.247973   \n",
      "3  C1CCCC(CC1)NC(=O)C2=CC3=C(N=CC=C3)N(C2=O)CC4=C... -1.342423   \n",
      "4         CCCCN1C(=C(C=C(C1=S)OC2=NC3=CC=CC=C3O2)C)C -2.004321   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0                 CCCCCc1cc(-c2ccccc2)n(-c2ccccc2)n1  \n",
      "1   CCCNC(=O)c1nn(-c2ccc(Cl)cc2Cl)c(-n2c(C)ccc2C)c1C  \n",
      "2  CC(C)(C)c1cc(-c2csc(C3(N4CCOCC4)CCOCC3)n2)cc(C...  \n",
      "3         O=C(NC1CCCCCC1)c1cc2cccnc2n(Cc2ccccc2)c1=O  \n",
      "4               CCCCn1c(C)c(C)cc(Oc2nc3ccccc3o2)c1=S  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1314\n",
      "number of successfully processed smiles:  1314\n",
      "(1314, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CN(CCC1(CNS(=O)(=O)C(F)(F)F)C#N)S(=O)(=O)C2=... -1.574031   \n",
      "1  CCN(CC)C1=CC=C(C=C1)CN(C2=CC=C(C=C2)C)S(=O)(=O... -0.477121   \n",
      "2  CC1(C2CCC(C2)(C1NC(=O)C3=CN(C4=C3C=CC=C4OC)CCN... -1.113943   \n",
      "3  CCCN(CCC)C(=O)C1=CC2=C(C=C1)N(C(=N2)CC3=CC=C(C... -0.447158   \n",
      "4  CC1=CC=C(C=C1)CN2C=C(C=C2C3=CC(=C(C=C3)Cl)C)CN... -2.535294   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  N#CC1(CNS(=O)(=O)C(F)(F)F)CCN(S(=O)(=O)c2ccc(C...  \n",
      "1  CCN(CC)c1ccc(CN(c2ccc(C)cc2)S(=O)(=O)c2ccc(Cl)...  \n",
      "2  COc1cccc2c(C(=O)NC3C4(C)CCC(C4)C3(C)C)cn(CCN3C...  \n",
      "3  CCCN(CCC)C(=O)c1ccc2c(c1)nc(Cc1ccc(OCC)cc1)n2C...  \n",
      "4  Cc1ccc(Cn2cc(CNC3C4(C)CCC(C4)C3(C)C)cc2-c2ccc(...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P34972_1_500_train.pickle\n",
      "./data/benchmark/Ki_P34972_1_500_train\n",
      "3905\n",
      "feature dicts file saved as ./data/benchmark/Ki_P34972_1_500_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2487, 3) (104, 3) (1314, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P34972_1_500_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3289366/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 248 Index:0.0756 R2:0.0213 0.0756 0.0098 RMSE:1.1397 1.2112 1.1317 Tau:0.0952 0.1876 0.0498\n",
      "Epoch: 2 Step: 496 Index:0.1546 R2:0.1048 0.1546 0.0810 RMSE:1.1220 1.1993 1.1172 Tau:0.2234 0.2628 0.1834\n",
      "Epoch: 3 Step: 744 Index:0.2195 R2:0.1994 0.2195 0.1707 RMSE:1.0410 1.1337 1.0395 Tau:0.3182 0.3308 0.2713\n",
      "Epoch: 4 Step: 992 Index:0.2351 R2:0.2161 0.2351 0.1878 RMSE:1.0197 1.1055 1.0202 Tau:0.3337 0.3562 0.2902\n",
      "Epoch: 5 Step: 1240 Index:0.2374 R2:0.2385 0.2374 0.2106 RMSE:1.0020 1.1029 1.0018 Tau:0.3529 0.3655 0.3144\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 6 Step: 1488 Index:0.2242 R2:0.2491 0.2242 0.2093 RMSE:1.0257 1.1313 1.0327 Tau:0.3625 0.3588 0.3197\n",
      "Epoch: 7 Step: 1736 Index:0.2445 R2:0.2624 0.2445 0.2267 RMSE:0.9874 1.0964 0.9927 Tau:0.3710 0.3745 0.3335\n",
      "Epoch: 8 Step: 1984 Index:0.2505 R2:0.2729 0.2505 0.2393 RMSE:1.0162 1.1340 1.0181 Tau:0.3793 0.3775 0.3501\n",
      "Epoch: 9 Step: 2232 Index:0.2526 R2:0.2774 0.2526 0.2317 RMSE:1.0216 1.1245 1.0353 Tau:0.3814 0.3805 0.3394\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 10 Step: 2480 Index:0.2446 R2:0.2905 0.2446 0.2498 RMSE:0.9728 1.1020 0.9799 Tau:0.3903 0.3760 0.3571\n",
      "Epoch: 11 Step: 2728 Index:0.2645 R2:0.2951 0.2645 0.2525 RMSE:0.9880 1.0985 0.9965 Tau:0.3926 0.3827 0.3601\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 12 Step: 2976 Index:0.2596 R2:0.3052 0.2596 0.2590 RMSE:0.9838 1.1030 0.9949 Tau:0.4002 0.3895 0.3643\n",
      "Epoch: 13 Step: 3224 Index:0.2664 R2:0.3129 0.2664 0.2682 RMSE:0.9526 1.0810 0.9644 Tau:0.4020 0.3939 0.3747\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 14 Step: 3472 Index:0.2656 R2:0.3186 0.2656 0.2656 RMSE:0.9741 1.1141 0.9913 Tau:0.4103 0.3898 0.3822\n",
      "Epoch: 15 Step: 3720 Index:0.2859 R2:0.3268 0.2859 0.2709 RMSE:0.9503 1.0706 0.9682 Tau:0.4143 0.4022 0.3785\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 16 Step: 3968 Index:0.2694 R2:0.3332 0.2694 0.2777 RMSE:0.9729 1.1143 0.9911 Tau:0.4184 0.3917 0.3821\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 17 Step: 4216 Index:0.2714 R2:0.3376 0.2714 0.2842 RMSE:0.9501 1.0847 0.9682 Tau:0.4173 0.3984 0.3790\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 18 Step: 4464 Index:0.2771 R2:0.3498 0.2771 0.3006 RMSE:0.9332 1.0796 0.9497 Tau:0.4253 0.4096 0.3877\n",
      "Epoch: 19 Step: 4712 Index:0.2926 R2:0.3593 0.2926 0.2954 RMSE:0.9256 1.0679 0.9488 Tau:0.4349 0.4055 0.3952\n",
      "Epoch: 20 Step: 4960 Index:0.3043 R2:0.3613 0.3043 0.2989 RMSE:0.9201 1.0548 0.9442 Tau:0.4343 0.4152 0.3987\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 21 Step: 5208 Index:0.3005 R2:0.3745 0.3005 0.3084 RMSE:0.9132 1.0558 0.9387 Tau:0.4432 0.4089 0.3979\n",
      "Epoch: 22 Step: 5456 Index:0.3123 R2:0.3788 0.3123 0.3093 RMSE:0.9228 1.0566 0.9493 Tau:0.4456 0.4171 0.3975\n",
      "Epoch: 23 Step: 5704 Index:0.3189 R2:0.3771 0.3189 0.3074 RMSE:0.9246 1.0519 0.9539 Tau:0.4454 0.4138 0.3963\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 24 Step: 5952 Index:0.2903 R2:0.3834 0.2903 0.3190 RMSE:0.9099 1.0638 0.9366 Tau:0.4469 0.4100 0.3952\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 25 Step: 6200 Index:0.3055 R2:0.3864 0.3055 0.3186 RMSE:0.9191 1.0607 0.9476 Tau:0.4480 0.4212 0.3994\n",
      "Epoch: 26 Step: 6448 Index:0.3351 R2:0.3975 0.3351 0.3338 RMSE:0.8954 1.0299 0.9221 Tau:0.4558 0.4358 0.4101\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 27 Step: 6696 Index:0.3232 R2:0.4031 0.3232 0.3342 RMSE:0.8889 1.0363 0.9197 Tau:0.4610 0.4250 0.4035\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 28 Step: 6944 Index:0.3233 R2:0.4041 0.3233 0.3395 RMSE:0.8988 1.0514 0.9283 Tau:0.4611 0.4179 0.4067\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 29 Step: 7192 Index:0.3299 R2:0.4095 0.3299 0.3412 RMSE:0.8914 1.0353 0.9219 Tau:0.4644 0.4347 0.4055\n",
      "Epoch: 30 Step: 7440 Index:0.3369 R2:0.4063 0.3369 0.3357 RMSE:0.9161 1.0475 0.9448 Tau:0.4651 0.4324 0.4067\n",
      "Epoch: 31 Step: 7688 Index:0.3472 R2:0.4091 0.3472 0.3435 RMSE:0.8866 1.0221 0.9193 Tau:0.4664 0.4380 0.4128\n",
      "Epoch: 32 Step: 7936 Index:0.3487 R2:0.4256 0.3487 0.3583 RMSE:0.8786 1.0212 0.9068 Tau:0.4760 0.4407 0.4134\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 8184 Index:0.3429 R2:0.4297 0.3429 0.3519 RMSE:0.8779 1.0353 0.9174 Tau:0.4788 0.4306 0.4188\n",
      "Epoch: 34 Step: 8432 Index:0.3535 R2:0.4336 0.3535 0.3577 RMSE:0.8709 1.0233 0.9070 Tau:0.4822 0.4470 0.4176\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 35 Step: 8680 Index:0.3531 R2:0.4306 0.3531 0.3630 RMSE:0.8682 1.0160 0.9019 Tau:0.4785 0.4455 0.4202\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 36 Step: 8928 Index:0.3531 R2:0.4408 0.3531 0.3667 RMSE:0.8634 1.0152 0.8984 Tau:0.4839 0.4358 0.4153\n",
      "Epoch: 37 Step: 9176 Index:0.3705 R2:0.4511 0.3705 0.3813 RMSE:0.8663 1.0097 0.8949 Tau:0.4917 0.4623 0.4260\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 38 Step: 9424 Index:0.3252 R2:0.4261 0.3252 0.3516 RMSE:0.8809 1.0494 0.9177 Tau:0.4757 0.4242 0.4037\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 39 Step: 9672 Index:0.3377 R2:0.4499 0.3377 0.3768 RMSE:0.8591 1.0253 0.8947 Tau:0.4895 0.4351 0.4188\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 40 Step: 9920 Index:0.3326 R2:0.4552 0.3326 0.3689 RMSE:0.8737 1.0590 0.9189 Tau:0.4920 0.4321 0.4201\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 41 Step: 10168 Index:0.3526 R2:0.4649 0.3526 0.3915 RMSE:0.8415 1.0119 0.8787 Tau:0.5006 0.4496 0.4308\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 42 Step: 10416 Index:0.3556 R2:0.4690 0.3556 0.3982 RMSE:0.8407 1.0107 0.8745 Tau:0.5033 0.4448 0.4300\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 43 Step: 10664 Index:0.3449 R2:0.4724 0.3449 0.3990 RMSE:0.8709 1.0577 0.9088 Tau:0.5046 0.4425 0.4239\n",
      "Epoch: 44 Step: 10912 Index:0.3755 R2:0.4591 0.3755 0.3881 RMSE:0.8628 1.0084 0.8982 Tau:0.4971 0.4650 0.4325\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 45 Step: 11160 Index:0.3413 R2:0.4564 0.3413 0.3841 RMSE:0.8652 1.0306 0.8972 Tau:0.4961 0.4452 0.4237\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 46 Step: 11408 Index:0.3590 R2:0.4944 0.3590 0.4116 RMSE:0.8318 1.0107 0.8693 Tau:0.5199 0.4489 0.4331\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 47 Step: 11656 Index:0.3482 R2:0.4820 0.3482 0.4113 RMSE:0.8320 1.0160 0.8664 Tau:0.5098 0.4433 0.4380\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 48 Step: 11904 Index:0.3741 R2:0.4955 0.3741 0.4158 RMSE:0.8190 0.9979 0.8625 Tau:0.5195 0.4638 0.4336\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 49 Step: 12152 Index:0.3524 R2:0.4906 0.3524 0.4033 RMSE:0.8266 1.0164 0.8750 Tau:0.5173 0.4478 0.4269\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 50 Step: 12400 Index:0.3740 R2:0.5002 0.3740 0.4213 RMSE:0.8140 0.9967 0.8584 Tau:0.5229 0.4646 0.4421\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 51 Step: 12648 Index:0.3509 R2:0.5063 0.3509 0.4219 RMSE:0.8107 1.0129 0.8567 Tau:0.5274 0.4358 0.4314\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 52 Step: 12896 Index:0.3378 R2:0.5004 0.3378 0.4144 RMSE:0.8137 1.0239 0.8621 Tau:0.5234 0.4317 0.4272\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 53 Step: 13144 Index:0.3551 R2:0.5116 0.3551 0.4136 RMSE:0.8140 1.0145 0.8730 Tau:0.5327 0.4493 0.4339\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 54 Step: 13392 Index:0.3416 R2:0.5000 0.3416 0.4057 RMSE:0.8202 1.0283 0.8774 Tau:0.5211 0.4444 0.4275\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 55 Step: 13640 Index:0.3693 R2:0.5238 0.3693 0.4243 RMSE:0.8003 1.0061 0.8624 Tau:0.5404 0.4545 0.4461\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 56 Step: 13888 Index:0.3665 R2:0.5094 0.3665 0.4263 RMSE:0.8060 1.0009 0.8535 Tau:0.5317 0.4575 0.4456\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 57 Step: 14136 Index:0.3508 R2:0.5288 0.3508 0.4356 RMSE:0.8215 1.0302 0.8731 Tau:0.5396 0.4481 0.4381\n",
      "Epoch: 58 Step: 14384 Index:0.3784 R2:0.5210 0.3784 0.4376 RMSE:0.8200 1.0089 0.8723 Tau:0.5388 0.4642 0.4481\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 59 Step: 14632 Index:0.3719 R2:0.5416 0.3719 0.4504 RMSE:0.7852 0.9980 0.8377 Tau:0.5493 0.4545 0.4480\n",
      "Epoch: 60 Step: 14880 Index:0.3841 R2:0.5430 0.3841 0.4515 RMSE:0.7770 0.9872 0.8345 Tau:0.5508 0.4627 0.4437\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 61 Step: 15128 Index:0.3578 R2:0.5327 0.3578 0.4400 RMSE:0.7891 1.0080 0.8436 Tau:0.5488 0.4560 0.4473\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 62 Step: 15376 Index:0.3757 R2:0.5428 0.3757 0.4494 RMSE:0.7786 0.9937 0.8361 Tau:0.5500 0.4616 0.4436\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 63 Step: 15624 Index:0.3678 R2:0.5250 0.3678 0.4391 RMSE:0.7912 1.0027 0.8441 Tau:0.5384 0.4646 0.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 64 Step: 15872 Index:0.3548 R2:0.5348 0.3548 0.4403 RMSE:0.7933 1.0136 0.8490 Tau:0.5465 0.4448 0.4483\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 65 Step: 16120 Index:0.3804 R2:0.5516 0.3804 0.4580 RMSE:0.7819 1.0017 0.8397 Tau:0.5544 0.4549 0.4469\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 66 Step: 16368 Index:0.3724 R2:0.5399 0.3724 0.4531 RMSE:0.8391 1.0343 0.8885 Tau:0.5468 0.4642 0.4463\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 67 Step: 16616 Index:0.3742 R2:0.5471 0.3742 0.4508 RMSE:0.7802 0.9972 0.8385 Tau:0.5540 0.4597 0.4595\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 68 Step: 16864 Index:0.3719 R2:0.5629 0.3719 0.4686 RMSE:0.7654 1.0011 0.8247 Tau:0.5640 0.4552 0.4488\n",
      "Epoch: 69 Step: 17112 Index:0.3890 R2:0.5596 0.3890 0.4476 RMSE:0.7698 0.9869 0.8449 Tau:0.5655 0.4691 0.4471\n",
      "Epoch: 70 Step: 17360 Index:0.4009 R2:0.5648 0.4009 0.4833 RMSE:0.7795 0.9908 0.8256 Tau:0.5665 0.4833 0.4527\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 71 Step: 17608 Index:0.3947 R2:0.5778 0.3947 0.4770 RMSE:0.7513 0.9807 0.8181 Tau:0.5732 0.4751 0.4524\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 72 Step: 17856 Index:0.4008 R2:0.5757 0.4008 0.4708 RMSE:0.7513 0.9732 0.8197 Tau:0.5708 0.4799 0.4574\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 73 Step: 18104 Index:0.3939 R2:0.5785 0.3939 0.4731 RMSE:0.7479 0.9804 0.8187 Tau:0.5715 0.4721 0.4507\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 74 Step: 18352 Index:0.3805 R2:0.5835 0.3805 0.4789 RMSE:0.7547 0.9924 0.8191 Tau:0.5798 0.4638 0.4620\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 75 Step: 18600 Index:0.3966 R2:0.5972 0.3966 0.5019 RMSE:0.7326 0.9778 0.7957 Tau:0.5868 0.4758 0.4630\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 76 Step: 18848 Index:0.3995 R2:0.5831 0.3995 0.4840 RMSE:0.7415 0.9768 0.8099 Tau:0.5779 0.4863 0.4547\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 77 Step: 19096 Index:0.3863 R2:0.5897 0.3863 0.4965 RMSE:0.7386 0.9888 0.8007 Tau:0.5815 0.4582 0.4536\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 78 Step: 19344 Index:0.3918 R2:0.5956 0.3918 0.4929 RMSE:0.7333 0.9823 0.8026 Tau:0.5862 0.4698 0.4549\n",
      "Epoch: 79 Step: 19592 Index:0.4095 R2:0.6092 0.4095 0.5091 RMSE:0.7246 0.9674 0.7913 Tau:0.5939 0.4780 0.4654\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 80 Step: 19840 Index:0.3903 R2:0.6044 0.3903 0.4950 RMSE:0.7373 0.9867 0.8078 Tau:0.5892 0.4616 0.4603\n",
      "Epoch: 81 Step: 20088 Index:0.4101 R2:0.6018 0.4101 0.4979 RMSE:0.7397 0.9737 0.8074 Tau:0.5897 0.4889 0.4594\n",
      "Epoch: 82 Step: 20336 Index:0.4140 R2:0.6174 0.4140 0.5005 RMSE:0.7391 0.9731 0.8132 Tau:0.5995 0.4859 0.4599\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 83 Step: 20584 Index:0.4011 R2:0.6197 0.4011 0.5052 RMSE:0.7204 0.9787 0.7977 Tau:0.6003 0.4870 0.4595\n",
      "Epoch: 84 Step: 20832 Index:0.4179 R2:0.6220 0.4179 0.5022 RMSE:0.7087 0.9628 0.7977 Tau:0.6009 0.4881 0.4572\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 85 Step: 21080 Index:0.4034 R2:0.6263 0.4034 0.5077 RMSE:0.7130 0.9732 0.7934 Tau:0.6038 0.4728 0.4598\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 86 Step: 21328 Index:0.3908 R2:0.5962 0.3908 0.4961 RMSE:0.7317 0.9828 0.7999 Tau:0.5859 0.4810 0.4533\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 87 Step: 21576 Index:0.4141 R2:0.6235 0.4141 0.5137 RMSE:0.7625 0.9961 0.8328 Tau:0.6031 0.4836 0.4587\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 88 Step: 21824 Index:0.4150 R2:0.6308 0.4150 0.5192 RMSE:0.7055 0.9644 0.7865 Tau:0.6051 0.4836 0.4615\n",
      "Epoch: 89 Step: 22072 Index:0.4314 R2:0.6380 0.4314 0.5244 RMSE:0.7028 0.9518 0.7857 Tau:0.6133 0.5005 0.4606\n",
      "Epoch: 90 Step: 22320 Index:0.4325 R2:0.6434 0.4325 0.5215 RMSE:0.7099 0.9605 0.7939 Tau:0.6148 0.5087 0.4594\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 91 Step: 22568 Index:0.4003 R2:0.6067 0.4003 0.5063 RMSE:0.7243 0.9784 0.7949 Tau:0.5911 0.4919 0.4552\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 92 Step: 22816 Index:0.4152 R2:0.6266 0.4152 0.5112 RMSE:0.7028 0.9663 0.7906 Tau:0.6025 0.5008 0.4470\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 93 Step: 23064 Index:0.4273 R2:0.6470 0.4273 0.5190 RMSE:0.7108 0.9695 0.8042 Tau:0.6160 0.5135 0.4577\n",
      "Epoch: 94 Step: 23312 Index:0.4369 R2:0.6490 0.4369 0.5156 RMSE:0.6895 0.9450 0.7841 Tau:0.6212 0.5053 0.4578\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 95 Step: 23560 Index:0.4333 R2:0.6579 0.4333 0.5296 RMSE:0.6951 0.9670 0.7929 Tau:0.6255 0.5057 0.4624\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 96 Step: 23808 Index:0.4042 R2:0.6267 0.4042 0.4876 RMSE:0.7567 1.0193 0.8617 Tau:0.6038 0.4911 0.4477\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 97 Step: 24056 Index:0.4124 R2:0.6258 0.4124 0.4965 RMSE:0.7092 0.9714 0.8089 Tau:0.6064 0.4960 0.4589\n",
      "Epoch: 98 Step: 24304 Index:0.4375 R2:0.6655 0.4375 0.5314 RMSE:0.6687 0.9429 0.7722 Tau:0.6312 0.5046 0.4595\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 99 Step: 24552 Index:0.4275 R2:0.6670 0.4275 0.5237 RMSE:0.6730 0.9591 0.7828 Tau:0.6326 0.5031 0.4637\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 100 Step: 24800 Index:0.4285 R2:0.6521 0.4285 0.5298 RMSE:0.6876 0.9508 0.7740 Tau:0.6201 0.5061 0.4525\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 101 Step: 25048 Index:0.4266 R2:0.6676 0.4266 0.5406 RMSE:0.6691 0.9567 0.7668 Tau:0.6301 0.4949 0.4610\n",
      "Epoch: 102 Step: 25296 Index:0.4447 R2:0.6786 0.4447 0.5454 RMSE:0.6751 0.9425 0.7734 Tau:0.6388 0.5165 0.4655\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 103 Step: 25544 Index:0.4400 R2:0.6848 0.4400 0.5530 RMSE:0.6532 0.9442 0.7575 Tau:0.6432 0.5061 0.4693\n",
      "Epoch: 104 Step: 25792 Index:0.4452 R2:0.6779 0.4452 0.5447 RMSE:0.6567 0.9376 0.7630 Tau:0.6371 0.5106 0.4603\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 105 Step: 26040 Index:0.4355 R2:0.6725 0.4355 0.5609 RMSE:0.6680 0.9494 0.7536 Tau:0.6340 0.5072 0.4608\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 106 Step: 26288 Index:0.4274 R2:0.6801 0.4274 0.5490 RMSE:0.6512 0.9553 0.7582 Tau:0.6378 0.4971 0.4587\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 107 Step: 26536 Index:0.4368 R2:0.6903 0.4368 0.5403 RMSE:0.6473 0.9467 0.7677 Tau:0.6454 0.4967 0.4607\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 108 Step: 26784 Index:0.4362 R2:0.6825 0.4362 0.5457 RMSE:0.6505 0.9445 0.7607 Tau:0.6398 0.5042 0.4686\n",
      "Epoch: 109 Step: 27032 Index:0.4587 R2:0.6829 0.4587 0.5599 RMSE:0.6664 0.9354 0.7649 Tau:0.6399 0.5177 0.4693\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 110 Step: 27280 Index:0.4362 R2:0.6664 0.4362 0.5204 RMSE:0.6780 0.9456 0.7837 Tau:0.6312 0.5147 0.4552\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 111 Step: 27528 Index:0.4485 R2:0.6808 0.4485 0.5373 RMSE:0.6495 0.9375 0.7719 Tau:0.6383 0.5113 0.4623\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 112 Step: 27776 Index:0.4483 R2:0.6966 0.4483 0.5569 RMSE:0.6438 0.9443 0.7541 Tau:0.6489 0.5076 0.4671\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 113 Step: 28024 Index:0.4500 R2:0.6874 0.4500 0.5576 RMSE:0.6540 0.9366 0.7560 Tau:0.6425 0.5098 0.4570\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 114 Step: 28272 Index:0.4452 R2:0.6988 0.4452 0.5464 RMSE:0.6369 0.9397 0.7627 Tau:0.6525 0.5135 0.4647\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 115 Step: 28520 Index:0.4437 R2:0.6963 0.4437 0.5469 RMSE:0.6340 0.9402 0.7638 Tau:0.6483 0.5173 0.4588\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 116 Step: 28768 Index:0.4464 R2:0.7041 0.4464 0.5641 RMSE:0.6355 0.9374 0.7443 Tau:0.6541 0.5165 0.4637\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 117 Step: 29016 Index:0.4568 R2:0.7058 0.4568 0.5510 RMSE:0.6284 0.9287 0.7578 Tau:0.6547 0.5158 0.4587\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 118 Step: 29264 Index:0.4456 R2:0.7097 0.4456 0.5635 RMSE:0.6223 0.9390 0.7446 Tau:0.6581 0.5091 0.4746\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 119 Step: 29512 Index:0.4369 R2:0.6996 0.4369 0.5490 RMSE:0.6650 0.9579 0.7821 Tau:0.6528 0.5184 0.4658\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 120 Step: 29760 Index:0.4438 R2:0.7014 0.4438 0.5523 RMSE:0.6299 0.9391 0.7597 Tau:0.6511 0.5083 0.4586\n",
      "Epoch: 121 Step: 30008 Index:0.4670 R2:0.7104 0.4670 0.5631 RMSE:0.6298 0.9366 0.7606 Tau:0.6589 0.5259 0.4704\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 122 Step: 30256 Index:0.4486 R2:0.7049 0.4486 0.5428 RMSE:0.6326 0.9351 0.7660 Tau:0.6547 0.5109 0.4606\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 123 Step: 30504 Index:0.4487 R2:0.7054 0.4487 0.5654 RMSE:0.6325 0.9398 0.7432 Tau:0.6545 0.5124 0.4702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 124 Step: 30752 Index:0.4492 R2:0.7206 0.4492 0.5591 RMSE:0.6151 0.9339 0.7493 Tau:0.6655 0.5132 0.4677\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 125 Step: 31000 Index:0.4324 R2:0.7183 0.4324 0.5610 RMSE:0.6144 0.9480 0.7488 Tau:0.6632 0.4960 0.4682\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 126 Step: 31248 Index:0.4361 R2:0.7295 0.4361 0.5475 RMSE:0.6320 0.9528 0.7774 Tau:0.6717 0.4941 0.4684\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 127 Step: 31496 Index:0.4521 R2:0.7166 0.4521 0.5761 RMSE:0.6190 0.9308 0.7338 Tau:0.6640 0.5117 0.4746\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 128 Step: 31744 Index:0.4588 R2:0.7175 0.4588 0.5326 RMSE:0.6172 0.9306 0.7726 Tau:0.6653 0.5128 0.4627\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 129 Step: 31992 Index:0.4455 R2:0.7217 0.4455 0.5714 RMSE:0.6077 0.9391 0.7384 Tau:0.6673 0.5102 0.4774\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 130 Step: 32240 Index:0.4635 R2:0.7256 0.4635 0.5661 RMSE:0.6644 0.9508 0.7986 Tau:0.6688 0.5236 0.4674\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 131 Step: 32488 Index:0.4386 R2:0.7274 0.4386 0.5560 RMSE:0.6070 0.9569 0.7611 Tau:0.6697 0.5005 0.4699\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 132 Step: 32736 Index:0.4653 R2:0.7140 0.4653 0.5489 RMSE:0.6576 0.9518 0.7982 Tau:0.6587 0.5214 0.4595\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 133 Step: 32984 Index:0.4606 R2:0.7296 0.4606 0.5747 RMSE:0.6067 0.9255 0.7416 Tau:0.6711 0.5229 0.4653\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 134 Step: 33232 Index:0.4449 R2:0.7220 0.4449 0.5363 RMSE:0.6148 0.9392 0.7765 Tau:0.6658 0.5061 0.4674\n",
      "Epoch: 135 Step: 33480 Index:0.4754 R2:0.7302 0.4754 0.5725 RMSE:0.6032 0.9187 0.7467 Tau:0.6708 0.5337 0.4642\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 136 Step: 33728 Index:0.4673 R2:0.7421 0.4673 0.5697 RMSE:0.5900 0.9188 0.7439 Tau:0.6806 0.5207 0.4712\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 137 Step: 33976 Index:0.4409 R2:0.7301 0.4409 0.5602 RMSE:0.6180 0.9531 0.7674 Tau:0.6708 0.5020 0.4599\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 138 Step: 34224 Index:0.4277 R2:0.7367 0.4277 0.5642 RMSE:0.5905 0.9586 0.7459 Tau:0.6770 0.5012 0.4778\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 139 Step: 34472 Index:0.4509 R2:0.7358 0.4509 0.5641 RMSE:0.6118 0.9384 0.7598 Tau:0.6752 0.5177 0.4746\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 140 Step: 34720 Index:0.4752 R2:0.7334 0.4752 0.5813 RMSE:0.6051 0.9282 0.7416 Tau:0.6731 0.5244 0.4743\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 141 Step: 34968 Index:0.4528 R2:0.7333 0.4528 0.5759 RMSE:0.5986 0.9393 0.7372 Tau:0.6743 0.5098 0.4716\n",
      "Epoch: 142 Step: 35216 Index:0.4780 R2:0.7490 0.4780 0.5956 RMSE:0.5839 0.9207 0.7232 Tau:0.6855 0.5281 0.4828\n",
      "Epoch: 143 Step: 35464 Index:0.4854 R2:0.7500 0.4854 0.5851 RMSE:0.5938 0.9066 0.7431 Tau:0.6853 0.5274 0.4733\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 144 Step: 35712 Index:0.4666 R2:0.7431 0.4666 0.5803 RMSE:0.5840 0.9242 0.7389 Tau:0.6795 0.5270 0.4745\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 145 Step: 35960 Index:0.4697 R2:0.7616 0.4697 0.5817 RMSE:0.5966 0.9268 0.7572 Tau:0.6942 0.5192 0.4704\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 146 Step: 36208 Index:0.4614 R2:0.7565 0.4614 0.5727 RMSE:0.5683 0.9250 0.7389 Tau:0.6896 0.5158 0.4715\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 147 Step: 36456 Index:0.4621 R2:0.7605 0.4621 0.5745 RMSE:0.5660 0.9239 0.7374 Tau:0.6921 0.5169 0.4739\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 148 Step: 36704 Index:0.4582 R2:0.7462 0.4582 0.5825 RMSE:0.5912 0.9298 0.7430 Tau:0.6814 0.5139 0.4754\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 149 Step: 36952 Index:0.4571 R2:0.7386 0.4571 0.5473 RMSE:0.6071 0.9393 0.7651 Tau:0.6753 0.5248 0.4612\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 150 Step: 37200 Index:0.4509 R2:0.7504 0.4509 0.5749 RMSE:0.5973 0.9374 0.7528 Tau:0.6866 0.5165 0.4782\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 151 Step: 37448 Index:0.4727 R2:0.7604 0.4727 0.5769 RMSE:0.5638 0.9137 0.7354 Tau:0.6927 0.5214 0.4723\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 152 Step: 37696 Index:0.4761 R2:0.7662 0.4761 0.5803 RMSE:0.5825 0.9192 0.7501 Tau:0.6969 0.5244 0.4755\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 153 Step: 37944 Index:0.4409 R2:0.7561 0.4409 0.5535 RMSE:0.5705 0.9446 0.7610 Tau:0.6897 0.4964 0.4770\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 154 Step: 38192 Index:0.4385 R2:0.7590 0.4385 0.5583 RMSE:0.5810 0.9438 0.7593 Tau:0.6931 0.5023 0.4696\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 155 Step: 38440 Index:0.4426 R2:0.7522 0.4426 0.5753 RMSE:0.5940 0.9485 0.7563 Tau:0.6853 0.5050 0.4770\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 156 Step: 38688 Index:0.4082 R2:0.7426 0.4082 0.5547 RMSE:0.6091 1.0059 0.7721 Tau:0.6790 0.4799 0.4691\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 157 Step: 38936 Index:0.4734 R2:0.7792 0.4734 0.5863 RMSE:0.5613 0.9170 0.7389 Tau:0.7072 0.5177 0.4729\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 158 Step: 39184 Index:0.4705 R2:0.7741 0.4705 0.5874 RMSE:0.5590 0.9229 0.7248 Tau:0.7013 0.5180 0.4669\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 159 Step: 39432 Index:0.4278 R2:0.7617 0.4278 0.5795 RMSE:0.6082 0.9765 0.7732 Tau:0.6931 0.4949 0.4765\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 160 Step: 39680 Index:0.4422 R2:0.7366 0.4422 0.5575 RMSE:0.6253 0.9679 0.7985 Tau:0.6749 0.5008 0.4661\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 161 Step: 39928 Index:0.4477 R2:0.7689 0.4477 0.5680 RMSE:0.5550 0.9411 0.7497 Tau:0.6989 0.5102 0.4726\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 162 Step: 40176 Index:0.4568 R2:0.7635 0.4568 0.5891 RMSE:0.5666 0.9345 0.7317 Tau:0.6919 0.5150 0.4732\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 163 Step: 40424 Index:0.4511 R2:0.7666 0.4511 0.6019 RMSE:0.5632 0.9432 0.7220 Tau:0.6974 0.5165 0.4813\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 164 Step: 40672 Index:0.4539 R2:0.7832 0.4539 0.5809 RMSE:0.5429 0.9383 0.7414 Tau:0.7085 0.5128 0.4698\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 165 Step: 40920 Index:0.4738 R2:0.7880 0.4738 0.5932 RMSE:0.5326 0.9139 0.7214 Tau:0.7143 0.5236 0.4748\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 166 Step: 41168 Index:0.4549 R2:0.7908 0.4549 0.5781 RMSE:0.5965 0.9606 0.7929 Tau:0.7156 0.5154 0.4669\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 167 Step: 41416 Index:0.4675 R2:0.7820 0.4675 0.5919 RMSE:0.5621 0.9240 0.7400 Tau:0.7083 0.5233 0.4731\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 168 Step: 41664 Index:0.4182 R2:0.7205 0.4182 0.5708 RMSE:0.6081 0.9677 0.7396 Tau:0.6633 0.4945 0.4686\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 169 Step: 41912 Index:0.4503 R2:0.7726 0.4503 0.6003 RMSE:0.5540 0.9349 0.7127 Tau:0.7005 0.5050 0.4791\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 170 Step: 42160 Index:0.4486 R2:0.7750 0.4486 0.5636 RMSE:0.5504 0.9369 0.7532 Tau:0.7019 0.5102 0.4695\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 171 Step: 42408 Index:0.4484 R2:0.7858 0.4484 0.5942 RMSE:0.5572 0.9394 0.7363 Tau:0.7111 0.4960 0.4816\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 172 Step: 42656 Index:0.4452 R2:0.7820 0.4452 0.5873 RMSE:0.5437 0.9402 0.7358 Tau:0.7088 0.5121 0.4715\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 173 Step: 42904 Index:0.4315 R2:0.7579 0.4315 0.5391 RMSE:0.5877 0.9737 0.7964 Tau:0.6891 0.5046 0.4540\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 173 r2:0.5851 RMSE:0.7431 WTI:0.3504 AP:0.6589 Tau:0.4733 \n",
      " \n",
      " Top-1:0.3846 Top-1-fp:0.0769 \n",
      " Top-5:0.5231 Top-5-fp:0.0615 \n",
      " Top-10:0.5954 Top-10-fp:0.0687 \n",
      " Top-15:0.7005 Top-15-fp:0.1168 \n",
      " Top-20:0.7099 Top-20-fp:0.1794 \n",
      " Top-25:0.7165 Top-25-fp:0.2256 \n",
      " Top-30:0.6929 Top-30-fp:0.2716 \n",
      " Top-40:0.6820 Top-40-fp:0.3505 \n",
      " Top-50:0.7800 Top-50-fp:0.4064 \n",
      " \n",
      " Top50:0.4400 Top50-fp:0.0600 \n",
      " Top100:0.6300 Top100-fp:0.0400 \n",
      " Top150:0.6200 Top150-fp:0.1000 \n",
      " Top200:0.7050 Top200-fp:0.1200 \n",
      " Top250:0.7120 Top250-fp:0.1640 \n",
      " Top300:0.7067 Top300-fp:0.2000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
