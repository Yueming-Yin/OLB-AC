{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P41145_0.3333333333333333_175\n",
      "model_file/0_GAFSE_Ki_P41145_0.3333333333333333_175_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P41145_0.3333333333333333_175_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P41145_0.3333333333333333_175_test.csv\"\n",
    "test_active = 175\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC(C)(C)C1=CC=C(C=C1)C(=O)NC2CCC3(C4CC5=C6C3(C...  0.431798\n",
      "1  CC(CCC1CCCCC1)(C2CC34CCC2(C5C36CCN(C4CC7=C6C(=...  0.744727\n",
      "2    COC(=O)C1=CC=C(C=C1)C2=CC3(CCNCC3)OC4=CC=CC=C42 -2.716003\n",
      "3  CC12CCC3C(=O)OC(CC3(C1C(=O)C(CC2C(=O)OC)OC(=O)... -1.954243\n",
      "4           CCCCCN(CCN1CCCC1)C(=O)CC2=C3C=CSC3=CC=C2 -3.301030\n",
      "number of all smiles:  932\n",
      "number of successfully processed smiles:  932\n",
      "                                              smiles     value  \\\n",
      "0  CC(C)(C)C1=CC=C(C=C1)C(=O)NC2CCC3(C4CC5=C6C3(C...  0.431798   \n",
      "1  CC(CCC1CCCCC1)(C2CC34CCC2(C5C36CCN(C4CC7=C6C(=...  0.744727   \n",
      "2    COC(=O)C1=CC=C(C=C1)C2=CC3(CCNCC3)OC4=CC=CC=C42 -2.716003   \n",
      "3  CC12CCC3C(=O)OC(CC3(C1C(=O)C(CC2C(=O)OC)OC(=O)... -1.954243   \n",
      "4           CCCCCN(CCN1CCCC1)C(=O)CC2=C3C=CSC3=CC=C2 -3.301030   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CC(C)(C)c1ccc(C(=O)NC2CCC3(O)C4Cc5ccc(O)c6c5C3...  \n",
      "1  COC12CCC3(CC1C(C)(O)CCC1CCCCC1)C1Cc4ccc(O)c5c4...  \n",
      "2          COC(=O)c1ccc(C2=CC3(CCNCC3)Oc3ccccc32)cc1  \n",
      "3  COC(=O)C1CC(OC(=O)c2ccccc2)C(=O)C2C1(C)CCC1C(=...  \n",
      "4               CCCCCN(CCN1CCCC1)C(=O)Cc1cccc2sccc12  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  744\n",
      "number of successfully processed smiles:  744\n",
      "(744, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CC(C1CC23C=CC1(C4C25CCN(C3CC6=C5C(=C(C=C6)O)O4... -0.591065   \n",
      "1  C1CC1CN2CCC34C5C6=C(CC3(C2CC7=C4C(=C(C=C7)O)O5...  0.180456   \n",
      "2  CC(=O)OC1CC(C2(CCC3C(=O)OC(CC3(C2C1=O)C)C4CCOC... -1.146128   \n",
      "3  COC1=C2C3=C(CC4C56C3(CCN4CC7CC7)C(O2)C(CC5)(C(... -1.499824   \n",
      "4  CN1CC2(C(N(C(C(C1)(C2=O)C(=O)OC)C3=CC=CC=N3)C)... -1.176091   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  COC(C)C1CC23C=CC1(OC)C1Oc4c(O)ccc5c4C12CCN(C)C3C5  \n",
      "1  NC(=NCc1ccc(Cl)cc1)Nc1ccc2[nH]c3c(c2c1)CC1(O)C...  \n",
      "2  COC(=O)C1CC(OC(C)=O)C(=O)C2C1(C)CCC1C(=O)OC(C3...  \n",
      "3  COc1ccc2c3c1OC1C4(OC)CCC5(CC4COCc4ccc(OCC(=O)O...  \n",
      "4  COC(=O)C12CN(C)CC(C(=O)OC)(C1=O)C(c1ccccn1)N(C...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P41145_0.3333333333333333_175_train.pickle\n",
      "./data/benchmark/Ki_P41145_0.3333333333333333_175_train\n",
      "1676\n",
      "feature dicts file saved as ./data/benchmark/Ki_P41145_0.3333333333333333_175_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 3) (186, 3) (744, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P41145_0.3333333333333333_175_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2753852/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 74 Index:-1.5601 R2:0.0265 0.0272 0.0237 RMSE:1.3788 1.4567 1.4487 Tau:-0.1013 -0.1034 -0.0208\n",
      "Epoch: 2 Step: 148 Index:-1.3539 R2:0.0648 0.0068 0.0463 RMSE:1.3027 1.4381 1.4076 Tau:0.1844 0.0842 0.0702\n",
      "Epoch: 3 Step: 222 Index:-1.2055 R2:0.1255 0.0345 0.1023 RMSE:1.2680 1.4008 1.3458 Tau:0.2877 0.1953 0.1065\n",
      "Epoch: 4 Step: 296 Index:-1.1539 R2:0.1729 0.0751 0.1401 RMSE:1.2323 1.3660 1.3196 Tau:0.3027 0.2121 0.1264\n",
      "Epoch: 5 Step: 370 Index:-1.1313 R2:0.1951 0.1326 0.1655 RMSE:1.2271 1.3765 1.3562 Tau:0.3091 0.2452 0.1419\n",
      "Epoch: 6 Step: 444 Index:-1.1145 R2:0.2131 0.1260 0.1727 RMSE:1.1972 1.3523 1.3265 Tau:0.3195 0.2379 0.1460\n",
      "Epoch: 7 Step: 518 Index:-1.0952 R2:0.2173 0.1167 0.1751 RMSE:1.2320 1.3408 1.2893 Tau:0.3317 0.2455 0.1519\n",
      "Epoch: 8 Step: 592 Index:-1.0606 R2:0.2404 0.1519 0.1924 RMSE:1.1841 1.3236 1.2977 Tau:0.3438 0.2630 0.1684\n",
      "Epoch: 9 Step: 666 Index:-1.0383 R2:0.2532 0.1752 0.2039 RMSE:1.1715 1.3204 1.3069 Tau:0.3509 0.2821 0.1739\n",
      "Epoch: 10 Step: 740 Index:-1.0356 R2:0.2577 0.1993 0.2167 RMSE:1.1837 1.3368 1.3332 Tau:0.3588 0.3012 0.1861\n",
      "Epoch: 11 Step: 814 Index:-0.9968 R2:0.2799 0.1896 0.2234 RMSE:1.1559 1.2941 1.2725 Tau:0.3712 0.2973 0.1806\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 12 Step: 888 Index:-0.9994 R2:0.2860 0.2020 0.2277 RMSE:1.1479 1.3051 1.3053 Tau:0.3750 0.3057 0.1815\n",
      "Epoch: 13 Step: 962 Index:-0.9689 R2:0.2964 0.2089 0.2352 RMSE:1.1300 1.2791 1.2755 Tau:0.3820 0.3102 0.1874\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 14 Step: 1036 Index:-1.0259 R2:0.3072 0.2201 0.2406 RMSE:1.1734 1.3439 1.3568 Tau:0.3904 0.3180 0.1830\n",
      "Epoch: 15 Step: 1110 Index:-0.9354 R2:0.3228 0.2541 0.2535 RMSE:1.1285 1.2769 1.2988 Tau:0.4005 0.3415 0.2184\n",
      "Epoch: 16 Step: 1184 Index:-0.9021 R2:0.3273 0.2658 0.2616 RMSE:1.1141 1.2478 1.2661 Tau:0.4014 0.3457 0.2213\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 17 Step: 1258 Index:-0.9229 R2:0.3304 0.2446 0.2575 RMSE:1.1015 1.2508 1.2677 Tau:0.4034 0.3279 0.2090\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 18 Step: 1332 Index:-0.9148 R2:0.3463 0.2548 0.2670 RMSE:1.0980 1.2525 1.2711 Tau:0.4144 0.3378 0.2087\n",
      "Epoch: 19 Step: 1406 Index:-0.8792 R2:0.3558 0.2615 0.2743 RMSE:1.0867 1.2239 1.2234 Tau:0.4236 0.3447 0.2063\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 20 Step: 1480 Index:-0.9291 R2:0.3350 0.2367 0.2585 RMSE:1.0944 1.2504 1.2593 Tau:0.4075 0.3214 0.1897\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 21 Step: 1554 Index:-0.9223 R2:0.3447 0.2427 0.2674 RMSE:1.0873 1.2488 1.2626 Tau:0.4140 0.3265 0.1897\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 22 Step: 1628 Index:-0.9185 R2:0.3538 0.2580 0.2760 RMSE:1.0978 1.2582 1.2815 Tau:0.4213 0.3397 0.1945\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 23 Step: 1702 Index:-0.8829 R2:0.3579 0.2569 0.2740 RMSE:1.0774 1.2245 1.2276 Tau:0.4229 0.3416 0.2062\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 24 Step: 1776 Index:-0.9370 R2:0.3562 0.2875 0.2726 RMSE:1.1353 1.2976 1.3686 Tau:0.4217 0.3607 0.2265\n",
      "Epoch: 25 Step: 1850 Index:-0.8264 R2:0.3835 0.2992 0.2990 RMSE:1.0662 1.2004 1.2125 Tau:0.4405 0.3741 0.2138\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 26 Step: 1924 Index:-0.8938 R2:0.3694 0.2733 0.2927 RMSE:1.0835 1.2438 1.2738 Tau:0.4303 0.3500 0.1927\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 27 Step: 1998 Index:-0.8366 R2:0.3793 0.3004 0.3052 RMSE:1.0677 1.2043 1.2224 Tau:0.4388 0.3677 0.1998\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 28 Step: 2072 Index:-0.8368 R2:0.3899 0.3043 0.3011 RMSE:1.0587 1.2102 1.2458 Tau:0.4434 0.3734 0.2206\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 29 Step: 2146 Index:-0.8468 R2:0.3940 0.3200 0.3124 RMSE:1.0808 1.2330 1.2825 Tau:0.4447 0.3863 0.2262\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 30 Step: 2220 Index:-0.8551 R2:0.3936 0.3139 0.3164 RMSE:1.0827 1.2338 1.2764 Tau:0.4463 0.3787 0.2132\n",
      "Epoch: 31 Step: 2294 Index:-0.8230 R2:0.3956 0.3275 0.3126 RMSE:1.0714 1.2158 1.2646 Tau:0.4425 0.3929 0.2311\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 32 Step: 2368 Index:-0.8844 R2:0.4008 0.3000 0.3071 RMSE:1.0816 1.2546 1.3137 Tau:0.4497 0.3702 0.2150\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 33 Step: 2442 Index:-0.8328 R2:0.4034 0.3185 0.3131 RMSE:1.0595 1.2171 1.2727 Tau:0.4507 0.3843 0.2085\n",
      "Epoch: 34 Step: 2516 Index:-0.7905 R2:0.4081 0.3170 0.3188 RMSE:1.0463 1.1772 1.1791 Tau:0.4555 0.3867 0.2206\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 35 Step: 2590 Index:-0.8702 R2:0.4111 0.3154 0.3246 RMSE:1.0870 1.2537 1.3163 Tau:0.4583 0.3835 0.2048\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 36 Step: 2664 Index:-0.8125 R2:0.4083 0.3012 0.3255 RMSE:1.0469 1.1883 1.1693 Tau:0.4550 0.3758 0.2135\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 37 Step: 2738 Index:-0.8293 R2:0.4142 0.3265 0.3321 RMSE:1.0622 1.2194 1.2637 Tau:0.4568 0.3901 0.2249\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 38 Step: 2812 Index:-1.0787 R2:0.3973 0.3280 0.3132 RMSE:1.2964 1.4703 1.5595 Tau:0.4462 0.3916 0.2252\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 39 Step: 2886 Index:-0.8731 R2:0.3954 0.2859 0.3125 RMSE:1.0567 1.2297 1.2575 Tau:0.4481 0.3566 0.1897\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 40 Step: 2960 Index:-0.8131 R2:0.4226 0.3248 0.3320 RMSE:1.0403 1.2026 1.2424 Tau:0.4636 0.3895 0.2104\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 41 Step: 3034 Index:-0.8864 R2:0.4230 0.3348 0.3307 RMSE:1.1129 1.2811 1.3676 Tau:0.4627 0.3946 0.2122\n",
      "Epoch: 42 Step: 3108 Index:-0.7776 R2:0.4285 0.3348 0.3368 RMSE:1.0203 1.1734 1.2085 Tau:0.4675 0.3958 0.2109\n",
      "Epoch: 43 Step: 3182 Index:-0.7732 R2:0.4247 0.3354 0.3431 RMSE:1.0451 1.1666 1.1541 Tau:0.4621 0.3935 0.2264\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 44 Step: 3256 Index:-0.7955 R2:0.4246 0.3166 0.3371 RMSE:1.0179 1.1784 1.1927 Tau:0.4655 0.3829 0.2101\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 45 Step: 3330 Index:-0.8021 R2:0.4327 0.3433 0.3509 RMSE:1.0437 1.2051 1.2574 Tau:0.4686 0.4030 0.2174\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 46 Step: 3404 Index:-0.7989 R2:0.4352 0.3327 0.3417 RMSE:1.0262 1.1938 1.2420 Tau:0.4715 0.3949 0.2134\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 47 Step: 3478 Index:-0.8119 R2:0.4324 0.3406 0.3320 RMSE:1.0429 1.2106 1.2790 Tau:0.4670 0.3987 0.2203\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 48 Step: 3552 Index:-0.7787 R2:0.4414 0.3532 0.3536 RMSE:1.0293 1.1849 1.2419 Tau:0.4741 0.4062 0.2229\n",
      "Epoch: 49 Step: 3626 Index:-0.7616 R2:0.4403 0.3464 0.3546 RMSE:1.0140 1.1656 1.1938 Tau:0.4773 0.4039 0.2110\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 50 Step: 3700 Index:-0.7965 R2:0.4469 0.3486 0.3584 RMSE:1.0393 1.2041 1.2597 Tau:0.4820 0.4077 0.2130\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 51 Step: 3774 Index:-0.7776 R2:0.4486 0.3526 0.3607 RMSE:1.0268 1.1878 1.2387 Tau:0.4808 0.4102 0.2203\n",
      "Epoch: 52 Step: 3848 Index:-0.7362 R2:0.4541 0.3580 0.3633 RMSE:1.0024 1.1497 1.1668 Tau:0.4841 0.4135 0.2285\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 53 Step: 3922 Index:-0.7756 R2:0.4533 0.3543 0.3664 RMSE:1.0196 1.1840 1.2381 Tau:0.4842 0.4084 0.2191\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 54 Step: 3996 Index:-0.7814 R2:0.4608 0.3663 0.3671 RMSE:1.0349 1.2018 1.2692 Tau:0.4889 0.4203 0.2171\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 55 Step: 4070 Index:-0.8931 R2:0.3914 0.3173 0.3132 RMSE:1.1247 1.2823 1.3719 Tau:0.4517 0.3892 0.2203\n",
      "Epoch: 56 Step: 4144 Index:-0.7353 R2:0.4588 0.3528 0.3728 RMSE:0.9890 1.1434 1.1477 Tau:0.4901 0.4081 0.2160\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 57 Step: 4218 Index:-0.7964 R2:0.4455 0.3334 0.3618 RMSE:1.0675 1.1930 1.1345 Tau:0.4807 0.3966 0.2160\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 58 Step: 4292 Index:-0.7558 R2:0.4602 0.3653 0.3805 RMSE:1.0143 1.1738 1.2207 Tau:0.4885 0.4180 0.2173\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 59 Step: 4366 Index:-0.7464 R2:0.4513 0.3444 0.3639 RMSE:0.9994 1.1494 1.1472 Tau:0.4852 0.4030 0.2189\n",
      "Epoch: 60 Step: 4440 Index:-0.7298 R2:0.4551 0.3641 0.3870 RMSE:0.9960 1.1410 1.1506 Tau:0.4878 0.4112 0.2138\n",
      "Epoch: 61 Step: 4514 Index:-0.7225 R2:0.4692 0.3626 0.3845 RMSE:0.9785 1.1378 1.1493 Tau:0.4953 0.4153 0.2252\n",
      "Epoch: 62 Step: 4588 Index:-0.7014 R2:0.4633 0.3788 0.3869 RMSE:0.9959 1.1339 1.1403 Tau:0.4935 0.4326 0.2249\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 63 Step: 4662 Index:-0.7434 R2:0.4746 0.3689 0.3876 RMSE:0.9975 1.1647 1.2061 Tau:0.5019 0.4213 0.2210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 64 Step: 4736 Index:-0.7057 R2:0.4780 0.3750 0.3903 RMSE:0.9738 1.1317 1.1550 Tau:0.5024 0.4260 0.2249\n",
      "Epoch: 65 Step: 4810 Index:-0.6903 R2:0.4794 0.3866 0.3975 RMSE:0.9800 1.1224 1.1332 Tau:0.4985 0.4321 0.2489\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 66 Step: 4884 Index:-0.6952 R2:0.4833 0.3919 0.3951 RMSE:0.9851 1.1386 1.1785 Tau:0.4997 0.4434 0.2462\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 67 Step: 4958 Index:-0.7647 R2:0.4742 0.3866 0.3833 RMSE:1.0268 1.1963 1.2778 Tau:0.4949 0.4316 0.2415\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 68 Step: 5032 Index:-0.7135 R2:0.4801 0.3677 0.3995 RMSE:0.9903 1.1341 1.0999 Tau:0.5018 0.4206 0.2381\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 69 Step: 5106 Index:-0.6971 R2:0.4811 0.4023 0.3961 RMSE:0.9883 1.1411 1.1984 Tau:0.4995 0.4441 0.2399\n",
      "Epoch: 70 Step: 5180 Index:-0.6863 R2:0.4881 0.3854 0.4111 RMSE:0.9605 1.1142 1.1168 Tau:0.5084 0.4279 0.2384\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 71 Step: 5254 Index:-0.6894 R2:0.4920 0.3876 0.4112 RMSE:0.9620 1.1228 1.1442 Tau:0.5082 0.4334 0.2502\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 72 Step: 5328 Index:-0.6977 R2:0.4994 0.3943 0.4100 RMSE:0.9703 1.1403 1.1777 Tau:0.5166 0.4427 0.2331\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 73 Step: 5402 Index:-0.7676 R2:0.4656 0.3348 0.3959 RMSE:0.9958 1.1668 1.1088 Tau:0.5009 0.3992 0.2280\n",
      "Epoch: 74 Step: 5476 Index:-0.6845 R2:0.4969 0.3840 0.4194 RMSE:0.9608 1.1147 1.0926 Tau:0.5143 0.4302 0.2399\n",
      "Epoch: 75 Step: 5550 Index:-0.6725 R2:0.4960 0.3934 0.4130 RMSE:0.9598 1.1083 1.1069 Tau:0.5131 0.4358 0.2372\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 76 Step: 5624 Index:-0.7211 R2:0.5077 0.4160 0.4219 RMSE:1.0100 1.1747 1.2490 Tau:0.5175 0.4536 0.2430\n",
      "Epoch: 77 Step: 5698 Index:-0.6629 R2:0.5007 0.4090 0.4114 RMSE:0.9543 1.1097 1.1531 Tau:0.5145 0.4469 0.2484\n",
      "Epoch: 78 Step: 5772 Index:-0.6596 R2:0.5006 0.4013 0.4243 RMSE:0.9501 1.0983 1.0965 Tau:0.5178 0.4387 0.2359\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 79 Step: 5846 Index:-0.7199 R2:0.5101 0.4102 0.4265 RMSE:1.0031 1.1714 1.2354 Tau:0.5203 0.4515 0.2500\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 80 Step: 5920 Index:-0.6858 R2:0.5104 0.4178 0.4242 RMSE:0.9732 1.1383 1.2031 Tau:0.5220 0.4524 0.2419\n",
      "Epoch: 81 Step: 5994 Index:-0.6481 R2:0.5041 0.4105 0.4167 RMSE:0.9448 1.0950 1.1235 Tau:0.5181 0.4469 0.2457\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 82 Step: 6068 Index:-0.6670 R2:0.5103 0.4012 0.4307 RMSE:0.9453 1.1126 1.1362 Tau:0.5231 0.4456 0.2380\n",
      "Epoch: 83 Step: 6142 Index:-0.6467 R2:0.5155 0.4074 0.4303 RMSE:0.9429 1.0936 1.0815 Tau:0.5258 0.4469 0.2452\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 84 Step: 6216 Index:-0.7196 R2:0.5179 0.4285 0.4369 RMSE:1.0238 1.1830 1.2604 Tau:0.5250 0.4634 0.2527\n",
      "Epoch: 85 Step: 6290 Index:-0.6291 R2:0.5231 0.4199 0.4374 RMSE:0.9386 1.0871 1.0837 Tau:0.5304 0.4580 0.2472\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 86 Step: 6364 Index:-0.6354 R2:0.5211 0.4166 0.4446 RMSE:0.9430 1.0878 1.0616 Tau:0.5300 0.4524 0.2522\n",
      "Epoch: 87 Step: 6438 Index:-0.6280 R2:0.5224 0.4350 0.4431 RMSE:0.9415 1.0910 1.1234 Tau:0.5260 0.4630 0.2682\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 88 Step: 6512 Index:-0.6853 R2:0.5057 0.3835 0.4171 RMSE:0.9438 1.1180 1.1172 Tau:0.5217 0.4327 0.2266\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 89 Step: 6586 Index:-0.6824 R2:0.5114 0.3880 0.4363 RMSE:0.9385 1.1181 1.1133 Tau:0.5241 0.4357 0.2348\n",
      "Epoch: 90 Step: 6660 Index:-0.6230 R2:0.5290 0.4244 0.4478 RMSE:0.9207 1.0830 1.1023 Tau:0.5370 0.4600 0.2491\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 91 Step: 6734 Index:-0.6263 R2:0.5327 0.4182 0.4545 RMSE:0.9187 1.0826 1.0706 Tau:0.5385 0.4563 0.2479\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 92 Step: 6808 Index:-0.6258 R2:0.5275 0.4239 0.4443 RMSE:0.9402 1.0922 1.0996 Tau:0.5320 0.4664 0.2640\n",
      "Epoch: 93 Step: 6882 Index:-0.6054 R2:0.5376 0.4367 0.4641 RMSE:0.9285 1.0691 1.0439 Tau:0.5382 0.4637 0.2623\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 94 Step: 6956 Index:-0.6324 R2:0.5178 0.4224 0.4587 RMSE:0.9382 1.0934 1.1085 Tau:0.5265 0.4611 0.2585\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 95 Step: 7030 Index:-0.6304 R2:0.5388 0.4319 0.4597 RMSE:0.9211 1.0914 1.1243 Tau:0.5417 0.4611 0.2482\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 96 Step: 7104 Index:-0.6328 R2:0.5461 0.4360 0.4651 RMSE:0.9258 1.0997 1.1451 Tau:0.5454 0.4669 0.2477\n",
      "Epoch: 97 Step: 7178 Index:-0.6009 R2:0.5319 0.4384 0.4765 RMSE:0.9368 1.0695 1.0389 Tau:0.5351 0.4686 0.2613\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 98 Step: 7252 Index:-0.6202 R2:0.5492 0.4401 0.4720 RMSE:0.9195 1.0906 1.1296 Tau:0.5475 0.4704 0.2629\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 99 Step: 7326 Index:-0.7267 R2:0.5226 0.4324 0.4620 RMSE:1.0376 1.1988 1.2647 Tau:0.5284 0.4721 0.2556\n",
      "Epoch: 100 Step: 7400 Index:-0.5851 R2:0.5490 0.4473 0.4727 RMSE:0.9024 1.0555 1.0549 Tau:0.5477 0.4704 0.2641\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 101 Step: 7474 Index:-0.6140 R2:0.5481 0.4296 0.4697 RMSE:0.9030 1.0755 1.0668 Tau:0.5467 0.4615 0.2496\n",
      "Epoch: 102 Step: 7548 Index:-0.5835 R2:0.5564 0.4558 0.4881 RMSE:0.9237 1.0574 1.0260 Tau:0.5507 0.4740 0.2685\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 103 Step: 7622 Index:-0.5986 R2:0.5544 0.4513 0.4783 RMSE:0.9076 1.0750 1.1064 Tau:0.5508 0.4764 0.2601\n",
      "Epoch: 104 Step: 7696 Index:-0.5575 R2:0.5500 0.4678 0.4776 RMSE:0.9098 1.0470 1.0659 Tau:0.5423 0.4895 0.2798\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 105 Step: 7770 Index:-0.5907 R2:0.5628 0.4443 0.4870 RMSE:0.8946 1.0646 1.0643 Tau:0.5544 0.4738 0.2684\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 106 Step: 7844 Index:-0.5863 R2:0.5664 0.4534 0.4935 RMSE:0.8898 1.0624 1.0814 Tau:0.5597 0.4761 0.2610\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 107 Step: 7918 Index:-0.5855 R2:0.5501 0.4499 0.4834 RMSE:0.9006 1.0540 1.0515 Tau:0.5490 0.4685 0.2587\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 108 Step: 7992 Index:-0.5815 R2:0.5742 0.4771 0.4973 RMSE:0.9064 1.0719 1.1220 Tau:0.5600 0.4904 0.2710\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 109 Step: 8066 Index:-0.6214 R2:0.5590 0.4521 0.4894 RMSE:0.9281 1.0973 1.1440 Tau:0.5534 0.4758 0.2517\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 110 Step: 8140 Index:-0.5834 R2:0.5651 0.4477 0.4978 RMSE:0.8865 1.0565 1.0287 Tau:0.5584 0.4731 0.2664\n",
      "Epoch: 111 Step: 8214 Index:-0.5557 R2:0.5633 0.4669 0.4950 RMSE:0.8906 1.0445 1.0591 Tau:0.5521 0.4888 0.2880\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 112 Step: 8288 Index:-0.5692 R2:0.5791 0.4682 0.5017 RMSE:0.8783 1.0513 1.0739 Tau:0.5648 0.4821 0.2734\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 113 Step: 8362 Index:-0.5653 R2:0.5823 0.4696 0.5132 RMSE:0.8835 1.0529 1.0639 Tau:0.5672 0.4876 0.2741\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 114 Step: 8436 Index:-0.5933 R2:0.5678 0.4791 0.5077 RMSE:0.9253 1.0853 1.1466 Tau:0.5588 0.4920 0.2700\n",
      "Epoch: 115 Step: 8510 Index:-0.5338 R2:0.5910 0.4957 0.5165 RMSE:0.8754 1.0348 1.0768 Tau:0.5718 0.5011 0.2789\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 116 Step: 8584 Index:-0.6051 R2:0.5668 0.4406 0.4932 RMSE:0.8936 1.0798 1.0920 Tau:0.5578 0.4747 0.2677\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 117 Step: 8658 Index:-0.5471 R2:0.5863 0.4868 0.5106 RMSE:0.8823 1.0458 1.0950 Tau:0.5658 0.4987 0.2790\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 118 Step: 8732 Index:-0.5346 R2:0.5885 0.4790 0.5168 RMSE:0.8659 1.0249 1.0051 Tau:0.5717 0.4902 0.2816\n",
      "Epoch: 119 Step: 8806 Index:-0.5327 R2:0.5877 0.4775 0.5218 RMSE:0.8649 1.0262 1.0102 Tau:0.5689 0.4935 0.2896\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 120 Step: 8880 Index:-0.7964 R2:0.5120 0.4410 0.4712 RMSE:1.1357 1.2859 1.3855 Tau:0.5357 0.4895 0.2579\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 121 Step: 8954 Index:-0.5670 R2:0.5846 0.4599 0.5059 RMSE:0.8666 1.0479 1.0483 Tau:0.5695 0.4809 0.2653\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 122 Step: 9028 Index:-0.5839 R2:0.5884 0.4868 0.5108 RMSE:0.9226 1.0846 1.1410 Tau:0.5682 0.5007 0.2931\n",
      "Epoch: 123 Step: 9102 Index:-0.5275 R2:0.6042 0.4869 0.5218 RMSE:0.8478 1.0249 1.0434 Tau:0.5818 0.4973 0.2780\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 124 Step: 9176 Index:-0.5930 R2:0.6045 0.4881 0.5272 RMSE:0.9122 1.0911 1.1632 Tau:0.5821 0.4982 0.2737\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 125 Step: 9250 Index:-0.5294 R2:0.5997 0.4952 0.5476 RMSE:0.8665 1.0319 1.0421 Tau:0.5744 0.5025 0.2726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126 Step: 9324 Index:-0.5129 R2:0.6074 0.4914 0.5320 RMSE:0.8457 1.0127 1.0037 Tau:0.5799 0.4998 0.2863\n",
      "Epoch: 127 Step: 9398 Index:-0.4953 R2:0.6174 0.5031 0.5467 RMSE:0.8323 1.0047 1.0061 Tau:0.5897 0.5093 0.2820\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 128 Step: 9472 Index:-0.5257 R2:0.6039 0.4857 0.5402 RMSE:0.8503 1.0222 0.9773 Tau:0.5809 0.4965 0.2844\n",
      "Epoch: 129 Step: 9546 Index:-0.4770 R2:0.6142 0.5145 0.5484 RMSE:0.8386 0.9941 1.0059 Tau:0.5831 0.5171 0.2986\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 130 Step: 9620 Index:-0.5102 R2:0.6123 0.5090 0.5382 RMSE:0.8527 1.0185 1.0677 Tau:0.5869 0.5083 0.2898\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 131 Step: 9694 Index:-0.4918 R2:0.6213 0.5066 0.5546 RMSE:0.8298 1.0018 1.0026 Tau:0.5908 0.5100 0.2899\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 132 Step: 9768 Index:-0.5043 R2:0.6212 0.4951 0.5498 RMSE:0.8278 1.0096 0.9950 Tau:0.5915 0.5054 0.2852\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 133 Step: 9842 Index:-0.4780 R2:0.6291 0.5169 0.5638 RMSE:0.8243 0.9926 0.9915 Tau:0.5973 0.5146 0.2858\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 134 Step: 9916 Index:-0.5713 R2:0.6219 0.5153 0.5590 RMSE:0.9169 1.0862 1.1718 Tau:0.5924 0.5149 0.2977\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 135 Step: 9990 Index:-0.5397 R2:0.6069 0.4761 0.5317 RMSE:0.8412 1.0301 1.0104 Tau:0.5829 0.4904 0.2920\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 136 Step: 10064 Index:-0.5426 R2:0.6170 0.4785 0.5485 RMSE:0.8337 1.0368 1.0052 Tau:0.5891 0.4942 0.2841\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 137 Step: 10138 Index:-0.5089 R2:0.6283 0.4956 0.5644 RMSE:0.8283 1.0131 0.9484 Tau:0.5982 0.5042 0.2920\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 138 Step: 10212 Index:-0.4891 R2:0.6382 0.5068 0.5691 RMSE:0.8175 1.0000 0.9800 Tau:0.6013 0.5108 0.2981\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 139 Step: 10286 Index:-0.4776 R2:0.6343 0.5165 0.5809 RMSE:0.8177 0.9933 0.9776 Tau:0.5968 0.5157 0.2927\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 140 Step: 10360 Index:-0.4797 R2:0.6069 0.5075 0.5645 RMSE:0.8428 1.0013 0.9915 Tau:0.5777 0.5216 0.3135\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 141 Step: 10434 Index:-0.4878 R2:0.6393 0.5163 0.5760 RMSE:0.8217 1.0069 1.0292 Tau:0.6032 0.5191 0.3022\n",
      "Epoch: 142 Step: 10508 Index:-0.4537 R2:0.6381 0.5324 0.5783 RMSE:0.8239 0.9796 0.9768 Tau:0.5995 0.5258 0.3029\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 143 Step: 10582 Index:-0.4886 R2:0.6471 0.5101 0.5845 RMSE:0.8016 1.0010 0.9866 Tau:0.6077 0.5125 0.2918\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 144 Step: 10656 Index:-0.5069 R2:0.6440 0.5202 0.5801 RMSE:0.8416 1.0272 1.0695 Tau:0.6063 0.5203 0.2913\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 145 Step: 10730 Index:-0.5256 R2:0.6328 0.4946 0.5696 RMSE:0.8157 1.0178 0.9683 Tau:0.5971 0.4921 0.2854\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 146 Step: 10804 Index:-0.4805 R2:0.6555 0.5121 0.5820 RMSE:0.7882 0.9928 0.9602 Tau:0.6143 0.5123 0.2942\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 147 Step: 10878 Index:-0.4655 R2:0.6585 0.5213 0.5827 RMSE:0.7910 0.9843 0.9679 Tau:0.6161 0.5187 0.3044\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 148 Step: 10952 Index:-0.4983 R2:0.6562 0.5114 0.5771 RMSE:0.7962 1.0089 1.0196 Tau:0.6147 0.5106 0.2917\n",
      "Epoch: 149 Step: 11026 Index:-0.4463 R2:0.6591 0.5389 0.5790 RMSE:0.8069 0.9802 1.0032 Tau:0.6121 0.5340 0.3160\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 150 Step: 11100 Index:-0.5307 R2:0.6279 0.4900 0.5473 RMSE:0.8285 1.0288 1.0422 Tau:0.5872 0.4980 0.2863\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 151 Step: 11174 Index:-0.4588 R2:0.6665 0.5339 0.5918 RMSE:0.7891 0.9851 1.0044 Tau:0.6214 0.5263 0.3118\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 152 Step: 11248 Index:-0.4551 R2:0.6664 0.5274 0.5935 RMSE:0.7756 0.9778 0.9522 Tau:0.6205 0.5227 0.3025\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 153 Step: 11322 Index:-0.6651 R2:0.6220 0.4837 0.5650 RMSE:0.9908 1.1626 0.9741 Tau:0.5929 0.4975 0.2907\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 154 Step: 11396 Index:-0.5496 R2:0.6336 0.4767 0.5585 RMSE:0.8255 1.0460 1.0455 Tau:0.5997 0.4964 0.2975\n",
      "Epoch: 155 Step: 11470 Index:-0.4432 R2:0.6609 0.5342 0.6068 RMSE:0.7870 0.9744 0.9591 Tau:0.6165 0.5312 0.3181\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 156 Step: 11544 Index:-0.4500 R2:0.6729 0.5351 0.6051 RMSE:0.7765 0.9807 0.9854 Tau:0.6260 0.5307 0.3144\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 157 Step: 11618 Index:-0.4534 R2:0.6758 0.5265 0.5910 RMSE:0.7693 0.9772 0.9423 Tau:0.6266 0.5237 0.3126\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 158 Step: 11692 Index:-0.4596 R2:0.6721 0.5231 0.5895 RMSE:0.7728 0.9811 0.9447 Tau:0.6225 0.5214 0.2964\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 159 Step: 11766 Index:-0.4516 R2:0.6820 0.5354 0.5937 RMSE:0.7694 0.9806 0.9983 Tau:0.6300 0.5291 0.3098\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 160 Step: 11840 Index:-0.4728 R2:0.6763 0.5335 0.6060 RMSE:0.7847 0.9985 1.0213 Tau:0.6264 0.5257 0.3057\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 161 Step: 11914 Index:-0.4452 R2:0.6863 0.5368 0.6090 RMSE:0.7615 0.9756 0.9722 Tau:0.6339 0.5304 0.3178\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 162 Step: 11988 Index:-0.5145 R2:0.6537 0.4975 0.5871 RMSE:0.8162 1.0339 1.0335 Tau:0.6131 0.5193 0.3154\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 163 Step: 12062 Index:-0.4435 R2:0.6785 0.5302 0.6060 RMSE:0.7738 0.9737 0.9188 Tau:0.6298 0.5301 0.3186\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 164 Step: 12136 Index:-0.4840 R2:0.6676 0.5099 0.5989 RMSE:0.7736 1.0017 0.9403 Tau:0.6229 0.5177 0.3156\n",
      "Epoch: 165 Step: 12210 Index:-0.4351 R2:0.6976 0.5360 0.6081 RMSE:0.7462 0.9672 0.9211 Tau:0.6419 0.5321 0.3129\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 166 Step: 12284 Index:-0.4423 R2:0.6923 0.5312 0.6032 RMSE:0.7502 0.9726 0.9272 Tau:0.6385 0.5303 0.3290\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 167 Step: 12358 Index:-0.4484 R2:0.6490 0.5163 0.5812 RMSE:0.8077 0.9871 0.9462 Tau:0.6115 0.5387 0.3237\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 168 Step: 12432 Index:-0.6567 R2:0.6309 0.4797 0.5485 RMSE:0.9406 1.1448 1.0110 Tau:0.5968 0.4880 0.2944\n",
      "Epoch: 169 Step: 12506 Index:-0.4197 R2:0.6869 0.5472 0.6057 RMSE:0.7550 0.9606 0.9588 Tau:0.6341 0.5408 0.3194\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 170 Step: 12580 Index:-0.4577 R2:0.6656 0.5279 0.5995 RMSE:0.7874 0.9889 0.9875 Tau:0.6212 0.5312 0.3026\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 171 Step: 12654 Index:-0.4790 R2:0.6899 0.5445 0.5996 RMSE:0.7922 1.0093 1.0749 Tau:0.6391 0.5303 0.3081\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 172 Step: 12728 Index:-0.4307 R2:0.6979 0.5458 0.5994 RMSE:0.7487 0.9621 0.9161 Tau:0.6435 0.5314 0.3224\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 173 Step: 12802 Index:-0.4796 R2:0.6943 0.5470 0.6118 RMSE:0.8145 1.0240 1.0871 Tau:0.6409 0.5443 0.3211\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 174 Step: 12876 Index:-0.4412 R2:0.7014 0.5402 0.5958 RMSE:0.7366 0.9707 0.9823 Tau:0.6443 0.5296 0.3196\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 175 Step: 12950 Index:-0.4828 R2:0.6843 0.5126 0.6031 RMSE:0.7547 1.0026 0.9398 Tau:0.6357 0.5198 0.3201\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 176 Step: 13024 Index:-0.4234 R2:0.6903 0.5424 0.5853 RMSE:0.7718 0.9726 0.9925 Tau:0.6347 0.5492 0.3434\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 177 Step: 13098 Index:-0.4490 R2:0.7034 0.5270 0.6193 RMSE:0.7311 0.9794 0.9266 Tau:0.6477 0.5305 0.3231\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 178 Step: 13172 Index:-0.4498 R2:0.7061 0.5352 0.6131 RMSE:0.7749 0.9871 0.8854 Tau:0.6482 0.5373 0.3330\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 179 Step: 13246 Index:-0.4378 R2:0.6948 0.5366 0.6104 RMSE:0.7453 0.9788 0.9623 Tau:0.6391 0.5411 0.3194\n",
      "Epoch: 180 Step: 13320 Index:-0.4137 R2:0.7180 0.5451 0.6320 RMSE:0.7443 0.9683 0.9529 Tau:0.6552 0.5547 0.3265\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 181 Step: 13394 Index:-0.4221 R2:0.7063 0.5445 0.6167 RMSE:0.7348 0.9657 0.9021 Tau:0.6479 0.5436 0.3300\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 182 Step: 13468 Index:-0.4633 R2:0.7007 0.5218 0.5892 RMSE:0.7512 1.0016 1.0239 Tau:0.6452 0.5383 0.3287\n",
      "Epoch: 183 Step: 13542 Index:-0.3988 R2:0.7201 0.5553 0.6093 RMSE:0.7265 0.9565 0.9798 Tau:0.6527 0.5577 0.3385\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 184 Step: 13616 Index:-0.3994 R2:0.7257 0.5527 0.6465 RMSE:0.7139 0.9516 0.8713 Tau:0.6618 0.5522 0.3253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 185 Step: 13690 Index:-0.4074 R2:0.7185 0.5512 0.6197 RMSE:0.7191 0.9600 0.9596 Tau:0.6569 0.5526 0.3363\n",
      "Epoch: 186 Step: 13764 Index:-0.3855 R2:0.7291 0.5637 0.6254 RMSE:0.7084 0.9437 0.9452 Tau:0.6625 0.5583 0.3305\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 187 Step: 13838 Index:-0.4369 R2:0.7279 0.5406 0.6334 RMSE:0.7296 0.9790 0.8664 Tau:0.6605 0.5421 0.3224\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 188 Step: 13912 Index:-0.4132 R2:0.7328 0.5495 0.6227 RMSE:0.7163 0.9704 0.9868 Tau:0.6650 0.5571 0.3385\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 189 Step: 13986 Index:-0.4117 R2:0.7344 0.5474 0.6357 RMSE:0.6943 0.9600 0.9038 Tau:0.6660 0.5483 0.3218\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 190 Step: 14060 Index:-0.5725 R2:0.6687 0.5272 0.5663 RMSE:0.9152 1.1236 1.2367 Tau:0.6177 0.5511 0.3314\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 191 Step: 14134 Index:-0.4929 R2:0.7054 0.5170 0.6173 RMSE:0.7626 1.0210 0.8840 Tau:0.6527 0.5280 0.3311\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 192 Step: 14208 Index:-0.3997 R2:0.7387 0.5565 0.6405 RMSE:0.6939 0.9476 0.8828 Tau:0.6730 0.5479 0.3405\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 193 Step: 14282 Index:-0.4485 R2:0.7159 0.5418 0.6180 RMSE:0.7952 1.0001 0.8809 Tau:0.6562 0.5515 0.3283\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 194 Step: 14356 Index:-0.3931 R2:0.7424 0.5658 0.6300 RMSE:0.6952 0.9482 0.9682 Tau:0.6746 0.5551 0.3369\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 195 Step: 14430 Index:-0.4184 R2:0.7330 0.5538 0.6267 RMSE:0.7045 0.9636 0.9730 Tau:0.6670 0.5453 0.3407\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 196 Step: 14504 Index:-0.4198 R2:0.7373 0.5542 0.6280 RMSE:0.7039 0.9688 0.9916 Tau:0.6727 0.5490 0.3413\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 197 Step: 14578 Index:-0.4396 R2:0.7414 0.5420 0.6384 RMSE:0.7021 0.9844 0.9926 Tau:0.6738 0.5448 0.3383\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 198 Step: 14652 Index:-0.4768 R2:0.7249 0.5380 0.5966 RMSE:0.7889 1.0341 1.1199 Tau:0.6571 0.5572 0.3368\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 199 Step: 14726 Index:-0.4177 R2:0.7344 0.5477 0.6174 RMSE:0.6916 0.9641 0.9240 Tau:0.6688 0.5464 0.3406\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 200 Step: 14800 Index:-0.4320 R2:0.7319 0.5297 0.6291 RMSE:0.7142 0.9776 0.9335 Tau:0.6652 0.5456 0.3254\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 201 Step: 14874 Index:-0.4196 R2:0.7429 0.5538 0.6299 RMSE:0.6840 0.9647 0.9627 Tau:0.6776 0.5451 0.3408\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 202 Step: 14948 Index:-0.4685 R2:0.7464 0.5432 0.6259 RMSE:0.7445 1.0263 1.0780 Tau:0.6765 0.5578 0.3390\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 203 Step: 15022 Index:-0.4650 R2:0.7343 0.5373 0.6254 RMSE:0.7197 1.0031 1.0236 Tau:0.6690 0.5382 0.3336\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 204 Step: 15096 Index:-0.6176 R2:0.6034 0.4217 0.5225 RMSE:0.8520 1.1062 1.0346 Tau:0.5887 0.4886 0.3205\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 205 Step: 15170 Index:-0.5853 R2:0.6799 0.4620 0.5819 RMSE:0.8067 1.0898 1.0992 Tau:0.6335 0.5044 0.3360\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 206 Step: 15244 Index:-0.4920 R2:0.6979 0.4987 0.6046 RMSE:0.7414 1.0179 0.9896 Tau:0.6440 0.5260 0.3393\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 207 Step: 15318 Index:-0.4474 R2:0.6964 0.5118 0.6231 RMSE:0.7511 0.9942 0.8960 Tau:0.6417 0.5468 0.3435\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 208 Step: 15392 Index:-0.4460 R2:0.7277 0.5368 0.6283 RMSE:0.7264 0.9968 1.0078 Tau:0.6658 0.5508 0.3421\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 209 Step: 15466 Index:-0.4583 R2:0.7315 0.5270 0.6295 RMSE:0.7123 1.0023 0.9916 Tau:0.6679 0.5440 0.3424\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 210 Step: 15540 Index:-0.4246 R2:0.7381 0.5236 0.6276 RMSE:0.7036 0.9801 0.9140 Tau:0.6708 0.5555 0.3497\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 211 Step: 15614 Index:-0.4956 R2:0.7294 0.5241 0.6091 RMSE:0.7534 1.0468 1.1028 Tau:0.6620 0.5512 0.3525\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 212 Step: 15688 Index:-0.4319 R2:0.7406 0.5337 0.6265 RMSE:0.6854 0.9831 0.9539 Tau:0.6747 0.5512 0.3507\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 213 Step: 15762 Index:-0.4281 R2:0.7445 0.5349 0.6312 RMSE:0.6889 0.9755 0.8853 Tau:0.6801 0.5474 0.3539\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 214 Step: 15836 Index:-0.4237 R2:0.7508 0.5401 0.6379 RMSE:0.6722 0.9755 0.9288 Tau:0.6811 0.5518 0.3439\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 215 Step: 15910 Index:-0.4016 R2:0.7499 0.5421 0.6203 RMSE:0.6856 0.9616 0.9158 Tau:0.6782 0.5600 0.3541\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 216 Step: 15984 Index:-0.4955 R2:0.7141 0.4912 0.6130 RMSE:0.7286 1.0304 0.9877 Tau:0.6668 0.5349 0.3267\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 216 r2:0.6254 RMSE:0.9452 WTI:0.3372 AP:0.5384 Tau:0.3305 \n",
      " \n",
      " Top-1:0.5714 Top-1-fp:0.0000 \n",
      " Top-5:0.6486 Top-5-fp:0.0541 \n",
      " Top-10:0.7162 Top-10-fp:0.1351 \n",
      " Top-15:0.6577 Top-15-fp:0.2973 \n",
      " Top-20:0.5473 Top-20-fp:0.4459 \n",
      " Top-25:0.5143 Top-25-fp:0.5161 \n",
      " Top-30:0.5714 Top-30-fp:0.5516 \n",
      " Top-40:0.6686 Top-40-fp:0.6061 \n",
      " Top-50:0.7600 Top-50-fp:0.6425 \n",
      " \n",
      " Top50:0.7000 Top50-fp:0.0600 \n",
      " Top100:0.6900 Top100-fp:0.2300 \n",
      " Top150:0.5467 Top150-fp:0.4467 \n",
      " Top200:0.5314 Top200-fp:0.5350 \n",
      " Top250:0.6114 Top250-fp:0.5720 \n",
      " Top300:0.6743 Top300-fp:0.6067 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
