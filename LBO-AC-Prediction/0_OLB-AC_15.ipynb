{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P61169_0.3333333333333333_150\n",
      "model_file/0_GAFSE_Ki_P61169_0.3333333333333333_150_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P61169_0.3333333333333333_150_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P61169_0.3333333333333333_150_test.csv\"\n",
    "test_active = 150\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0       CN(C1CCCCC1N2CCCC2)C(=O)CC3=CC4=CC=CC=C4C=C3 -3.866405\n",
      "1                    CCCNC1CC2=C3C(=CC=C2)NC(=O)N3C1 -3.015779\n",
      "2   COC1=CC=CC=C1C2=CC=C(N2)CN3CCN(CC3)C4=CC=CC=C4OC -0.812913\n",
      "3  C1CN(CCC1C2=NOC3=C2C=CC(=C3)F)CCCCOC4=NN(C(=O)... -1.428944\n",
      "4        COC1=CC=CC=C1NCC2CCN(CC2)CC3COC4=CC=CC=C4O3 -1.838849\n",
      "number of all smiles:  780\n",
      "number of successfully processed smiles:  780\n",
      "                                              smiles     value  \\\n",
      "0       CN(C1CCCCC1N2CCCC2)C(=O)CC3=CC4=CC=CC=C4C=C3 -3.866405   \n",
      "1                    CCCNC1CC2=C3C(=CC=C2)NC(=O)N3C1 -3.015779   \n",
      "2   COC1=CC=CC=C1C2=CC=C(N2)CN3CCN(CC3)C4=CC=CC=C4OC -0.812913   \n",
      "3  C1CN(CCC1C2=NOC3=C2C=CC(=C3)F)CCCCOC4=NN(C(=O)... -1.428944   \n",
      "4        COC1=CC=CC=C1NCC2CCN(CC2)CC3COC4=CC=CC=C4O3 -1.838849   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0            CN(C(=O)Cc1ccc2ccccc2c1)C1CCCCC1N1CCCC1  \n",
      "1                    CCCNC1Cc2cccc3[nH]c(=O)n(c23)C1  \n",
      "2       COc1ccccc1-c1ccc(CN2CCN(c3ccccc3OC)CC2)[nH]1  \n",
      "3  O=c1ccc(OCCCCN2CCC(c3noc4cc(F)ccc34)CC2)nn1-c1...  \n",
      "4              COc1ccccc1NCC1CCN(CC2COc3ccccc3O2)CC1  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  814\n",
      "number of successfully processed smiles:  814\n",
      "(814, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CN(CC2=C1NC3=C2C=C(C=C3)F)CCC4=NC5=CC=CC=C5C... -2.267172   \n",
      "1   COC1=C(C(=CC(=C1)Br)C(=O)NCCN2CCC3=CC=CC=C3C2)OC -2.633317   \n",
      "2  C1CN(CCC1O)CCCCOC2=CC=CC3=C2C(=O)C=C(O3)C4=CC=... -3.977724   \n",
      "3          CCCC1=CC2=C(S1)CNC3C2C4=CC(=C(C=C4CC3)O)O -3.146128   \n",
      "4  CN1CCC2=CC(=CC3=C2C1CC4=C3C(=CC=C4)O)C5=CC=C(C... -0.579784   \n",
      "\n",
      "                                      cano_smiles  \n",
      "0  Cl.Fc1ccc2[nH]c3c(c2c1)CN(CCc1ccc2ccccc2n1)CC3  \n",
      "1        COc1cc(Br)cc(C(=O)NCCN2CCc3ccccc3C2)c1OC  \n",
      "2   O=c1cc(-c2ccccc2)oc2cccc(OCCCCN3CCC(O)CC3)c12  \n",
      "3            CCCc1cc2c(s1)CNC1CCc3cc(O)c(O)cc3C21  \n",
      "4    CN1CCc2cc(-c3ccc(O)cc3)cc3c2C1Cc1cccc(O)c1-3  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P61169_0.3333333333333333_150_train.pickle\n",
      "./data/benchmark/Ki_P61169_0.3333333333333333_150_train\n",
      "1594\n",
      "feature dicts file saved as ./data/benchmark/Ki_P61169_0.3333333333333333_150_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624, 3) (156, 3) (814, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P61169_0.3333333333333333_150_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2909158/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 81 Index:0.0001 R2:0.0146 0.0001 0.0048 RMSE:1.2238 1.1562 1.1869 Tau:-0.0660 0.0092 -0.0805\n",
      "Epoch: 2 Step: 162 Index:0.0002 R2:0.0118 0.0002 0.0049 RMSE:1.1811 1.1322 1.1464 Tau:-0.0545 0.0236 -0.0793\n",
      "Epoch: 3 Step: 243 Index:0.0025 R2:0.0025 0.0025 0.0025 RMSE:1.1512 1.1277 1.1165 Tau:-0.0131 0.0535 -0.0601\n",
      "Epoch: 4 Step: 324 Index:0.0103 R2:0.0269 0.0103 0.0030 RMSE:1.1143 1.0994 1.0968 Tau:0.1067 0.0666 0.0630\n",
      "Epoch: 5 Step: 405 Index:0.0260 R2:0.0431 0.0260 0.0143 RMSE:1.1193 1.1118 1.0902 Tau:0.1308 0.0984 0.0847\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 6 Step: 486 Index:0.0248 R2:0.0544 0.0248 0.0179 RMSE:1.1103 1.1078 1.0872 Tau:0.1483 0.1136 0.0886\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 7 Step: 567 Index:0.0220 R2:0.0572 0.0220 0.0223 RMSE:1.0956 1.0922 1.0869 Tau:0.1524 0.1028 0.0955\n",
      "Epoch: 8 Step: 648 Index:0.0338 R2:0.0670 0.0338 0.0275 RMSE:1.1086 1.1094 1.0850 Tau:0.1651 0.1321 0.0969\n",
      "Epoch: 9 Step: 729 Index:0.0481 R2:0.0752 0.0481 0.0354 RMSE:1.0879 1.0811 1.0754 Tau:0.1736 0.1530 0.0972\n",
      "Epoch: 10 Step: 810 Index:0.0513 R2:0.0813 0.0513 0.0353 RMSE:1.0965 1.0953 1.0788 Tau:0.1828 0.1629 0.0992\n",
      "Epoch: 11 Step: 891 Index:0.0667 R2:0.0954 0.0667 0.0453 RMSE:1.1071 1.1069 1.0833 Tau:0.1992 0.1935 0.0993\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 12 Step: 972 Index:0.0605 R2:0.0977 0.0605 0.0520 RMSE:1.0784 1.0727 1.0809 Tau:0.2019 0.1824 0.1054\n",
      "Epoch: 13 Step: 1053 Index:0.0788 R2:0.1083 0.0788 0.0554 RMSE:1.0802 1.0758 1.0650 Tau:0.2162 0.2094 0.1036\n",
      "Epoch: 14 Step: 1134 Index:0.0966 R2:0.1219 0.0966 0.0612 RMSE:1.0944 1.0909 1.0724 Tau:0.2319 0.2289 0.1050\n",
      "Epoch: 15 Step: 1215 Index:0.0980 R2:0.1233 0.0980 0.0626 RMSE:1.0729 1.0678 1.0620 Tau:0.2278 0.2306 0.1102\n",
      "Epoch: 16 Step: 1296 Index:0.1199 R2:0.1426 0.1199 0.0778 RMSE:1.0667 1.0596 1.0541 Tau:0.2518 0.2552 0.1068\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 17 Step: 1377 Index:0.0970 R2:0.1346 0.0970 0.0766 RMSE:1.0761 1.0676 1.0935 Tau:0.2389 0.2370 0.1099\n",
      "Epoch: 18 Step: 1458 Index:0.1355 R2:0.1586 0.1355 0.0911 RMSE:1.0524 1.0445 1.0450 Tau:0.2698 0.2733 0.1144\n",
      "Epoch: 19 Step: 1539 Index:0.1477 R2:0.1703 0.1477 0.1002 RMSE:1.0443 1.0356 1.0398 Tau:0.2807 0.2868 0.1169\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 20 Step: 1620 Index:0.1395 R2:0.1732 0.1395 0.0991 RMSE:1.0356 1.0299 1.0504 Tau:0.2754 0.2782 0.1151\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 21 Step: 1701 Index:0.1461 R2:0.1803 0.1461 0.1065 RMSE:1.0397 1.0384 1.0363 Tau:0.2840 0.2840 0.1296\n",
      "Epoch: 22 Step: 1782 Index:0.1847 R2:0.1934 0.1847 0.1026 RMSE:1.0509 1.0415 1.0463 Tau:0.3015 0.3156 0.1187\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 23 Step: 1863 Index:0.1631 R2:0.1967 0.1631 0.1091 RMSE:1.0189 1.0169 1.0343 Tau:0.2967 0.2955 0.1287\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 24 Step: 1944 Index:0.1793 R2:0.2092 0.1793 0.1242 RMSE:1.0160 1.0106 1.0439 Tau:0.3071 0.3090 0.1355\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 25 Step: 2025 Index:0.1828 R2:0.2194 0.1828 0.1305 RMSE:1.0222 1.0177 1.0527 Tau:0.3169 0.3122 0.1313\n",
      "Epoch: 26 Step: 2106 Index:0.2071 R2:0.2403 0.2071 0.1502 RMSE:1.0057 1.0043 1.0123 Tau:0.3402 0.3393 0.1304\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 27 Step: 2187 Index:0.1867 R2:0.2343 0.1867 0.1357 RMSE:0.9927 0.9992 1.0243 Tau:0.3274 0.3151 0.1430\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 28 Step: 2268 Index:0.1936 R2:0.2468 0.1936 0.1409 RMSE:1.0082 1.0150 1.0586 Tau:0.3381 0.3251 0.1344\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 29 Step: 2349 Index:0.2067 R2:0.2486 0.2067 0.1495 RMSE:0.9997 1.0032 1.0447 Tau:0.3403 0.3297 0.1495\n",
      "Epoch: 30 Step: 2430 Index:0.2385 R2:0.2786 0.2385 0.1705 RMSE:0.9755 0.9789 1.0148 Tau:0.3677 0.3600 0.1436\n",
      "Epoch: 31 Step: 2511 Index:0.2395 R2:0.2866 0.2395 0.1558 RMSE:1.0597 1.0705 1.0633 Tau:0.3753 0.3618 0.1338\n",
      "Epoch: 32 Step: 2592 Index:0.2434 R2:0.2854 0.2434 0.1686 RMSE:0.9596 0.9671 1.0009 Tau:0.3689 0.3567 0.1588\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 2673 Index:0.2404 R2:0.3062 0.2404 0.1835 RMSE:0.9731 0.9917 0.9951 Tau:0.3897 0.3625 0.1399\n",
      "Epoch: 34 Step: 2754 Index:0.2533 R2:0.3147 0.2533 0.1894 RMSE:0.9626 0.9786 1.0203 Tau:0.3948 0.3645 0.1577\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 35 Step: 2835 Index:0.2428 R2:0.3118 0.2428 0.2004 RMSE:0.9544 0.9757 0.9799 Tau:0.3941 0.3582 0.1689\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 36 Step: 2916 Index:0.2457 R2:0.3122 0.2457 0.1811 RMSE:0.9935 1.0158 1.0774 Tau:0.3876 0.3560 0.1564\n",
      "Epoch: 37 Step: 2997 Index:0.2538 R2:0.3313 0.2538 0.1909 RMSE:0.9668 0.9950 1.0020 Tau:0.4045 0.3618 0.1545\n",
      "Epoch: 38 Step: 3078 Index:0.2706 R2:0.3479 0.2706 0.2137 RMSE:0.9328 0.9607 0.9728 Tau:0.4219 0.3828 0.1559\n",
      "Epoch: 39 Step: 3159 Index:0.2913 R2:0.3440 0.2913 0.2089 RMSE:0.9183 0.9359 0.9821 Tau:0.4154 0.3936 0.1719\n",
      "Epoch: 40 Step: 3240 Index:0.3059 R2:0.3435 0.3059 0.1917 RMSE:1.0384 1.0463 1.0613 Tau:0.4171 0.4037 0.1449\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 41 Step: 3321 Index:0.2904 R2:0.3663 0.2904 0.2087 RMSE:0.9204 0.9496 0.9757 Tau:0.4324 0.3926 0.1622\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 42 Step: 3402 Index:0.2873 R2:0.3648 0.2873 0.2330 RMSE:0.9056 0.9367 0.9601 Tau:0.4337 0.3884 0.1653\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 43 Step: 3483 Index:0.2926 R2:0.3809 0.2926 0.2263 RMSE:0.8991 0.9371 0.9790 Tau:0.4423 0.3926 0.1693\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 44 Step: 3564 Index:0.2789 R2:0.3834 0.2789 0.2139 RMSE:0.9333 0.9772 0.9848 Tau:0.4466 0.3827 0.1559\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 45 Step: 3645 Index:0.2705 R2:0.3835 0.2705 0.2257 RMSE:0.8893 0.9434 0.9670 Tau:0.4467 0.3750 0.1514\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 46 Step: 3726 Index:0.3040 R2:0.3951 0.3040 0.2442 RMSE:0.8884 0.9283 0.9520 Tau:0.4575 0.4023 0.1729\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 47 Step: 3807 Index:0.3011 R2:0.4008 0.3011 0.2306 RMSE:0.8772 0.9248 0.9622 Tau:0.4542 0.3952 0.1573\n",
      "Epoch: 48 Step: 3888 Index:0.3067 R2:0.4108 0.3067 0.2366 RMSE:0.8796 0.9292 0.9572 Tau:0.4630 0.4040 0.1655\n",
      "Epoch: 49 Step: 3969 Index:0.3136 R2:0.4049 0.3136 0.2400 RMSE:0.8769 0.9191 0.9580 Tau:0.4599 0.4030 0.1621\n",
      "Epoch: 50 Step: 4050 Index:0.3214 R2:0.4189 0.3214 0.2464 RMSE:0.8667 0.9142 0.9505 Tau:0.4681 0.4134 0.1760\n",
      "Epoch: 51 Step: 4131 Index:0.3249 R2:0.4227 0.3249 0.2644 RMSE:0.8736 0.9196 0.9403 Tau:0.4729 0.4078 0.1679\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 52 Step: 4212 Index:0.2906 R2:0.4314 0.2906 0.2439 RMSE:0.8757 0.9485 0.9561 Tau:0.4811 0.3894 0.1698\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 53 Step: 4293 Index:0.2951 R2:0.4339 0.2951 0.2578 RMSE:0.8576 0.9334 0.9443 Tau:0.4780 0.3909 0.1642\n",
      "Epoch: 54 Step: 4374 Index:0.3328 R2:0.4300 0.3328 0.2626 RMSE:0.8868 0.9329 0.9515 Tau:0.4789 0.4159 0.1667\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 55 Step: 4455 Index:0.3225 R2:0.4401 0.3225 0.2727 RMSE:0.8442 0.9090 0.9374 Tau:0.4817 0.4091 0.1742\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 56 Step: 4536 Index:0.3318 R2:0.4492 0.3318 0.2766 RMSE:0.8390 0.9032 0.9340 Tau:0.4878 0.4176 0.1824\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 57 Step: 4617 Index:0.2974 R2:0.4607 0.2974 0.2652 RMSE:0.8434 0.9340 0.9402 Tau:0.4956 0.3904 0.1692\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 58 Step: 4698 Index:0.3007 R2:0.4660 0.3007 0.2792 RMSE:0.8302 0.9262 0.9422 Tau:0.4970 0.3934 0.1733\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 59 Step: 4779 Index:0.2862 R2:0.4706 0.2862 0.2677 RMSE:0.8256 0.9340 0.9382 Tau:0.5006 0.3807 0.1510\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 60 Step: 4860 Index:0.3082 R2:0.4783 0.3082 0.2715 RMSE:0.8178 0.9193 0.9359 Tau:0.5041 0.3982 0.1707\n",
      "Epoch: 61 Step: 4941 Index:0.3348 R2:0.4862 0.3348 0.2809 RMSE:0.8299 0.9137 0.9303 Tau:0.5110 0.4186 0.1712\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 62 Step: 5022 Index:0.3070 R2:0.4810 0.3070 0.2633 RMSE:0.8200 0.9262 0.9417 Tau:0.5051 0.3893 0.1533\n",
      "Epoch: 63 Step: 5103 Index:0.3459 R2:0.4858 0.3459 0.2947 RMSE:0.8610 0.9331 0.9413 Tau:0.5109 0.4224 0.1693\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 64 Step: 5184 Index:0.3115 R2:0.4960 0.3115 0.2769 RMSE:0.8030 0.9164 0.9339 Tau:0.5143 0.3916 0.1713\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 65 Step: 5265 Index:0.2751 R2:0.4881 0.2751 0.2930 RMSE:0.8133 0.9477 0.9212 Tau:0.5123 0.3669 0.1596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 66 Step: 5346 Index:0.3382 R2:0.4871 0.3382 0.3216 RMSE:0.8350 0.9165 0.9103 Tau:0.5114 0.4115 0.1732\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 67 Step: 5427 Index:0.3277 R2:0.5059 0.3277 0.3189 RMSE:0.8091 0.9178 0.9280 Tau:0.5202 0.4055 0.1720\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 68 Step: 5508 Index:0.3253 R2:0.5039 0.3253 0.2696 RMSE:0.8027 0.9115 0.9374 Tau:0.5197 0.4076 0.1613\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 69 Step: 5589 Index:0.3233 R2:0.5008 0.3233 0.2933 RMSE:0.8029 0.9110 0.9202 Tau:0.5149 0.3932 0.1738\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 70 Step: 5670 Index:0.3197 R2:0.5223 0.3197 0.3159 RMSE:0.7904 0.9187 0.9076 Tau:0.5311 0.4023 0.1621\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 71 Step: 5751 Index:0.3101 R2:0.5199 0.3101 0.2820 RMSE:0.7858 0.9209 0.9299 Tau:0.5294 0.3949 0.1444\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 72 Step: 5832 Index:0.3153 R2:0.5250 0.3153 0.3214 RMSE:0.7819 0.9152 0.9021 Tau:0.5332 0.4038 0.1481\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 73 Step: 5913 Index:0.3083 R2:0.5295 0.3083 0.3160 RMSE:0.7750 0.9229 0.9074 Tau:0.5344 0.3924 0.1607\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 74 Step: 5994 Index:0.3010 R2:0.5182 0.3010 0.2731 RMSE:0.7857 0.9286 0.9447 Tau:0.5266 0.3913 0.1465\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 75 Step: 6075 Index:0.3286 R2:0.5348 0.3286 0.3405 RMSE:0.7885 0.9136 0.8922 Tau:0.5382 0.4076 0.1747\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 76 Step: 6156 Index:0.2886 R2:0.5464 0.2886 0.2980 RMSE:0.7679 0.9343 0.9224 Tau:0.5479 0.3825 0.1529\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 77 Step: 6237 Index:0.3095 R2:0.5487 0.3095 0.3178 RMSE:0.7632 0.9258 0.9189 Tau:0.5485 0.3969 0.1670\n",
      "Epoch: 78 Step: 6318 Index:0.3491 R2:0.5412 0.3491 0.3441 RMSE:0.8149 0.9252 0.9100 Tau:0.5443 0.4199 0.1614\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 79 Step: 6399 Index:0.3307 R2:0.5522 0.3307 0.3303 RMSE:0.7640 0.9119 0.9117 Tau:0.5476 0.4108 0.1640\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 80 Step: 6480 Index:0.3334 R2:0.5599 0.3334 0.3100 RMSE:0.7626 0.9123 0.9276 Tau:0.5530 0.4131 0.1524\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 81 Step: 6561 Index:0.3451 R2:0.5531 0.3451 0.3367 RMSE:0.7596 0.9009 0.9073 Tau:0.5496 0.4222 0.1533\n",
      "Epoch: 82 Step: 6642 Index:0.3822 R2:0.5413 0.3822 0.3051 RMSE:0.7806 0.8788 0.9192 Tau:0.5461 0.4412 0.1568\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 83 Step: 6723 Index:0.3289 R2:0.5535 0.3289 0.3242 RMSE:0.7928 0.9496 0.9605 Tau:0.5496 0.4086 0.1766\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 84 Step: 6804 Index:0.3469 R2:0.5621 0.3469 0.3482 RMSE:0.7529 0.8964 0.8916 Tau:0.5565 0.4204 0.1712\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 85 Step: 6885 Index:0.3341 R2:0.5753 0.3341 0.3323 RMSE:0.7473 0.9032 0.8947 Tau:0.5638 0.4172 0.1508\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 86 Step: 6966 Index:0.3122 R2:0.5710 0.3122 0.3055 RMSE:0.7451 0.9216 0.9192 Tau:0.5589 0.3967 0.1399\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 87 Step: 7047 Index:0.3237 R2:0.5691 0.3237 0.3273 RMSE:0.7504 0.9099 0.8976 Tau:0.5589 0.4023 0.1480\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 88 Step: 7128 Index:0.3063 R2:0.5579 0.3063 0.3266 RMSE:0.7931 0.9618 0.9481 Tau:0.5463 0.3861 0.1505\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 89 Step: 7209 Index:0.3622 R2:0.5814 0.3622 0.3501 RMSE:0.7389 0.8822 0.8826 Tau:0.5682 0.4368 0.1556\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 90 Step: 7290 Index:0.3581 R2:0.5876 0.3581 0.3336 RMSE:0.7350 0.8872 0.8952 Tau:0.5709 0.4315 0.1554\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 91 Step: 7371 Index:0.3623 R2:0.5788 0.3623 0.3554 RMSE:0.7350 0.8828 0.8805 Tau:0.5654 0.4305 0.1704\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 92 Step: 7452 Index:0.3532 R2:0.5814 0.3532 0.3493 RMSE:0.7334 0.9038 0.9008 Tau:0.5654 0.4265 0.1631\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 93 Step: 7533 Index:0.3662 R2:0.5939 0.3662 0.3338 RMSE:0.7250 0.8802 0.8961 Tau:0.5752 0.4407 0.1449\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 94 Step: 7614 Index:0.3639 R2:0.5984 0.3639 0.3381 RMSE:0.7426 0.9119 0.9314 Tau:0.5800 0.4346 0.1666\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 95 Step: 7695 Index:0.3598 R2:0.6000 0.3598 0.3513 RMSE:0.7164 0.8850 0.8827 Tau:0.5798 0.4301 0.1536\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 96 Step: 7776 Index:0.3717 R2:0.6047 0.3717 0.3403 RMSE:0.7153 0.8756 0.8909 Tau:0.5829 0.4427 0.1569\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 97 Step: 7857 Index:0.3524 R2:0.6047 0.3524 0.3486 RMSE:0.7166 0.8926 0.8873 Tau:0.5840 0.4280 0.1664\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 98 Step: 7938 Index:0.3477 R2:0.6072 0.3477 0.3381 RMSE:0.7269 0.9161 0.9214 Tau:0.5881 0.4249 0.1556\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 99 Step: 8019 Index:0.3703 R2:0.6147 0.3703 0.3536 RMSE:0.7018 0.8789 0.8851 Tau:0.5869 0.4397 0.1633\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 100 Step: 8100 Index:0.3642 R2:0.6200 0.3642 0.3401 RMSE:0.7085 0.8820 0.8898 Tau:0.5956 0.4383 0.1457\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 101 Step: 8181 Index:0.3642 R2:0.6092 0.3642 0.3131 RMSE:0.7085 0.8828 0.9131 Tau:0.5871 0.4338 0.1580\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 102 Step: 8262 Index:0.3573 R2:0.6098 0.3573 0.3373 RMSE:0.7415 0.9075 0.9114 Tau:0.5875 0.4209 0.1630\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 103 Step: 8343 Index:0.3476 R2:0.6090 0.3476 0.3431 RMSE:0.7090 0.9166 0.9128 Tau:0.5832 0.4229 0.1508\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 104 Step: 8424 Index:0.3744 R2:0.6245 0.3744 0.3640 RMSE:0.6944 0.8751 0.8758 Tau:0.5934 0.4460 0.1461\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 105 Step: 8505 Index:0.3695 R2:0.6309 0.3695 0.3444 RMSE:0.7001 0.8813 0.8910 Tau:0.5989 0.4378 0.1405\n",
      "Epoch: 106 Step: 8586 Index:0.3829 R2:0.6366 0.3829 0.3599 RMSE:0.6917 0.8790 0.8875 Tau:0.6023 0.4467 0.1469\n",
      "Epoch: 107 Step: 8667 Index:0.3947 R2:0.6327 0.3947 0.3597 RMSE:0.6865 0.8600 0.8788 Tau:0.5992 0.4492 0.1666\n",
      "Epoch: 108 Step: 8748 Index:0.3997 R2:0.6345 0.3997 0.3523 RMSE:0.6943 0.8589 0.8870 Tau:0.6024 0.4561 0.1532\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 109 Step: 8829 Index:0.3726 R2:0.6183 0.3726 0.3546 RMSE:0.7012 0.8769 0.8827 Tau:0.5890 0.4426 0.1493\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 110 Step: 8910 Index:0.3704 R2:0.6353 0.3704 0.3742 RMSE:0.6942 0.8791 0.8714 Tau:0.5995 0.4379 0.1582\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 111 Step: 8991 Index:0.3977 R2:0.6213 0.3977 0.3261 RMSE:0.7003 0.8607 0.9089 Tau:0.5926 0.4426 0.1469\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 112 Step: 9072 Index:0.3927 R2:0.6441 0.3927 0.3715 RMSE:0.6731 0.8648 0.8740 Tau:0.6053 0.4518 0.1614\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 113 Step: 9153 Index:0.3963 R2:0.6432 0.3963 0.3561 RMSE:0.6957 0.8640 0.8903 Tau:0.6043 0.4482 0.1612\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 114 Step: 9234 Index:0.3761 R2:0.6494 0.3761 0.3622 RMSE:0.6704 0.8790 0.8843 Tau:0.6090 0.4427 0.1499\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 115 Step: 9315 Index:0.3550 R2:0.6520 0.3550 0.3458 RMSE:0.6719 0.8899 0.8874 Tau:0.6149 0.4237 0.1441\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 116 Step: 9396 Index:0.3711 R2:0.6513 0.3711 0.3489 RMSE:0.6753 0.8768 0.8871 Tau:0.6080 0.4293 0.1473\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 117 Step: 9477 Index:0.3980 R2:0.6480 0.3980 0.3642 RMSE:0.7185 0.8826 0.9042 Tau:0.6101 0.4551 0.1480\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 118 Step: 9558 Index:0.3541 R2:0.6559 0.3541 0.3656 RMSE:0.6717 0.8896 0.8762 Tau:0.6128 0.4206 0.1508\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 119 Step: 9639 Index:0.3699 R2:0.6464 0.3699 0.3429 RMSE:0.6903 0.8852 0.9014 Tau:0.6058 0.4351 0.1437\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 120 Step: 9720 Index:0.3647 R2:0.6509 0.3647 0.3251 RMSE:0.6749 0.8979 0.9175 Tau:0.6119 0.4292 0.1704\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 121 Step: 9801 Index:0.3728 R2:0.6664 0.3728 0.3610 RMSE:0.6844 0.8949 0.9024 Tau:0.6253 0.4397 0.1549\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 122 Step: 9882 Index:0.3925 R2:0.6701 0.3925 0.3710 RMSE:0.6493 0.8645 0.8748 Tau:0.6248 0.4507 0.1694\n",
      "Epoch: 123 Step: 9963 Index:0.4058 R2:0.6699 0.4058 0.3799 RMSE:0.6836 0.8661 0.8874 Tau:0.6240 0.4588 0.1554\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 124 Step: 10044 Index:0.3768 R2:0.6783 0.3768 0.3545 RMSE:0.6461 0.8754 0.8870 Tau:0.6304 0.4384 0.1527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 125 Step: 10125 Index:0.3682 R2:0.6606 0.3682 0.3457 RMSE:0.6595 0.8876 0.8977 Tau:0.6136 0.4406 0.1354\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 126 Step: 10206 Index:0.3553 R2:0.6733 0.3553 0.3466 RMSE:0.6516 0.8919 0.8903 Tau:0.6272 0.4300 0.1332\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 127 Step: 10287 Index:0.3791 R2:0.6721 0.3791 0.3734 RMSE:0.6665 0.8755 0.8777 Tau:0.6230 0.4368 0.1501\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 128 Step: 10368 Index:0.3619 R2:0.6736 0.3619 0.3583 RMSE:0.6493 0.8869 0.8874 Tau:0.6235 0.4318 0.1381\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 129 Step: 10449 Index:0.3718 R2:0.6754 0.3718 0.3435 RMSE:0.6713 0.9146 0.9166 Tau:0.6278 0.4306 0.1585\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 130 Step: 10530 Index:0.3918 R2:0.6956 0.3918 0.3732 RMSE:0.6317 0.8636 0.8695 Tau:0.6415 0.4480 0.1512\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 131 Step: 10611 Index:0.3579 R2:0.6772 0.3579 0.3542 RMSE:0.6396 0.8987 0.8913 Tau:0.6274 0.4214 0.1528\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 132 Step: 10692 Index:0.3962 R2:0.6845 0.3962 0.3526 RMSE:0.6358 0.8621 0.8918 Tau:0.6345 0.4517 0.1574\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 133 Step: 10773 Index:0.3922 R2:0.7000 0.3922 0.3634 RMSE:0.6483 0.8744 0.8970 Tau:0.6460 0.4482 0.1561\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 134 Step: 10854 Index:0.3972 R2:0.6891 0.3972 0.3579 RMSE:0.6396 0.8798 0.8940 Tau:0.6335 0.4450 0.1582\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 135 Step: 10935 Index:0.3849 R2:0.6873 0.3849 0.3673 RMSE:0.6969 0.8987 0.9187 Tau:0.6332 0.4331 0.1633\n",
      "Epoch: 136 Step: 11016 Index:0.4176 R2:0.6983 0.4176 0.3747 RMSE:0.6212 0.8491 0.8749 Tau:0.6382 0.4594 0.1717\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 137 Step: 11097 Index:0.3907 R2:0.7004 0.3907 0.3591 RMSE:0.6589 0.9091 0.9171 Tau:0.6429 0.4500 0.1578\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 138 Step: 11178 Index:0.3898 R2:0.6991 0.3898 0.3770 RMSE:0.6730 0.8844 0.9025 Tau:0.6441 0.4402 0.1559\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 139 Step: 11259 Index:0.3747 R2:0.7004 0.3747 0.3423 RMSE:0.6312 0.8849 0.9062 Tau:0.6447 0.4412 0.1618\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 140 Step: 11340 Index:0.3599 R2:0.6944 0.3599 0.3469 RMSE:0.6248 0.8959 0.8958 Tau:0.6379 0.4267 0.1657\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 141 Step: 11421 Index:0.3672 R2:0.7146 0.3672 0.3739 RMSE:0.6065 0.8899 0.8758 Tau:0.6519 0.4263 0.1672\n",
      "Epoch: 142 Step: 11502 Index:0.4192 R2:0.6939 0.4192 0.3426 RMSE:0.6271 0.8640 0.9186 Tau:0.6355 0.4632 0.1822\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 143 Step: 11583 Index:0.3608 R2:0.6973 0.3608 0.3613 RMSE:0.6479 0.9362 0.9189 Tau:0.6381 0.4323 0.1542\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 144 Step: 11664 Index:0.4127 R2:0.7198 0.4127 0.3842 RMSE:0.6202 0.8537 0.8770 Tau:0.6569 0.4596 0.1587\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 145 Step: 11745 Index:0.4081 R2:0.7144 0.4081 0.3862 RMSE:0.6135 0.8557 0.8752 Tau:0.6530 0.4553 0.1625\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 146 Step: 11826 Index:0.4019 R2:0.7241 0.4019 0.3629 RMSE:0.6933 0.9072 0.9476 Tau:0.6605 0.4596 0.1651\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 147 Step: 11907 Index:0.3992 R2:0.7276 0.3992 0.3739 RMSE:0.6160 0.8746 0.8973 Tau:0.6622 0.4518 0.1597\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 148 Step: 11988 Index:0.4027 R2:0.7263 0.4027 0.3741 RMSE:0.6118 0.8637 0.8914 Tau:0.6602 0.4535 0.1744\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 149 Step: 12069 Index:0.3877 R2:0.7340 0.3877 0.3744 RMSE:0.5975 0.8685 0.8769 Tau:0.6656 0.4394 0.1543\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 150 Step: 12150 Index:0.3981 R2:0.7325 0.3981 0.3755 RMSE:0.5867 0.8731 0.8917 Tau:0.6678 0.4493 0.1681\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 151 Step: 12231 Index:0.3956 R2:0.7225 0.3956 0.3535 RMSE:0.6300 0.9116 0.9265 Tau:0.6577 0.4527 0.1652\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 152 Step: 12312 Index:0.3959 R2:0.7385 0.3959 0.3757 RMSE:0.6539 0.8957 0.9226 Tau:0.6668 0.4575 0.1651\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 153 Step: 12393 Index:0.3845 R2:0.7438 0.3845 0.3815 RMSE:0.5861 0.8738 0.8742 Tau:0.6708 0.4462 0.1530\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 154 Step: 12474 Index:0.3828 R2:0.7380 0.3828 0.3726 RMSE:0.5824 0.8791 0.8856 Tau:0.6674 0.4402 0.1746\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 155 Step: 12555 Index:0.3912 R2:0.7374 0.3912 0.3966 RMSE:0.6091 0.9030 0.8825 Tau:0.6662 0.4450 0.1689\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 156 Step: 12636 Index:0.4086 R2:0.7474 0.4086 0.3956 RMSE:0.5723 0.8681 0.8669 Tau:0.6733 0.4571 0.1795\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 157 Step: 12717 Index:0.3911 R2:0.7482 0.3911 0.3989 RMSE:0.5700 0.8709 0.8654 Tau:0.6740 0.4507 0.1711\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 158 Step: 12798 Index:0.3908 R2:0.7502 0.3908 0.3744 RMSE:0.5668 0.8816 0.8858 Tau:0.6740 0.4498 0.1598\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 159 Step: 12879 Index:0.3863 R2:0.7469 0.3863 0.3773 RMSE:0.5980 0.8780 0.8912 Tau:0.6731 0.4464 0.1869\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 160 Step: 12960 Index:0.3956 R2:0.7516 0.3956 0.3902 RMSE:0.5628 0.8712 0.8744 Tau:0.6764 0.4503 0.1779\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 161 Step: 13041 Index:0.4042 R2:0.7568 0.4042 0.3967 RMSE:0.6103 0.8795 0.9023 Tau:0.6789 0.4553 0.1689\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 162 Step: 13122 Index:0.3886 R2:0.7608 0.3886 0.3735 RMSE:0.5641 0.8682 0.8750 Tau:0.6820 0.4416 0.1791\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 163 Step: 13203 Index:0.3995 R2:0.7570 0.3995 0.3805 RMSE:0.5591 0.8667 0.8816 Tau:0.6809 0.4570 0.1721\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 164 Step: 13284 Index:0.4157 R2:0.7571 0.4157 0.3894 RMSE:0.5834 0.8514 0.8836 Tau:0.6817 0.4624 0.1829\n",
      "Epoch: 165 Step: 13365 Index:0.4250 R2:0.7486 0.4250 0.3964 RMSE:0.5858 0.8570 0.8951 Tau:0.6718 0.4709 0.1790\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 166 Step: 13446 Index:0.3688 R2:0.7445 0.3688 0.3717 RMSE:0.5847 0.8970 0.9002 Tau:0.6730 0.4343 0.1830\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 167 Step: 13527 Index:0.4021 R2:0.7616 0.4021 0.3856 RMSE:0.5640 0.8597 0.8718 Tau:0.6825 0.4543 0.1684\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 168 Step: 13608 Index:0.3982 R2:0.7722 0.3982 0.3970 RMSE:0.5746 0.8741 0.8868 Tau:0.6933 0.4513 0.1709\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 169 Step: 13689 Index:0.3807 R2:0.7603 0.3807 0.3967 RMSE:0.6085 0.8928 0.9005 Tau:0.6819 0.4363 0.1659\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 170 Step: 13770 Index:0.3882 R2:0.7624 0.3882 0.3835 RMSE:0.5642 0.8889 0.8800 Tau:0.6835 0.4432 0.1823\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 171 Step: 13851 Index:0.3935 R2:0.7739 0.3935 0.3815 RMSE:0.5520 0.8683 0.8870 Tau:0.6915 0.4512 0.1765\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 172 Step: 13932 Index:0.3919 R2:0.7611 0.3919 0.3737 RMSE:0.5546 0.8732 0.8840 Tau:0.6806 0.4455 0.1723\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 173 Step: 14013 Index:0.4239 R2:0.7727 0.4239 0.3853 RMSE:0.5672 0.8736 0.8823 Tau:0.6915 0.4657 0.1769\n",
      "Epoch: 174 Step: 14094 Index:0.4300 R2:0.7451 0.4300 0.3755 RMSE:0.7921 0.9582 1.0430 Tau:0.6689 0.4705 0.1866\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 175 Step: 14175 Index:0.3915 R2:0.7779 0.3915 0.3839 RMSE:0.5467 0.8729 0.8869 Tau:0.6970 0.4457 0.1690\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 176 Step: 14256 Index:0.3782 R2:0.7608 0.3782 0.3784 RMSE:0.5664 0.8914 0.8971 Tau:0.6823 0.4429 0.1513\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 177 Step: 14337 Index:0.4000 R2:0.7839 0.4000 0.3890 RMSE:0.5371 0.8602 0.8721 Tau:0.6993 0.4493 0.1846\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 178 Step: 14418 Index:0.3863 R2:0.7765 0.3863 0.3853 RMSE:0.5447 0.8730 0.8794 Tau:0.6931 0.4379 0.1893\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 179 Step: 14499 Index:0.4233 R2:0.7863 0.4233 0.3960 RMSE:0.5387 0.8457 0.8735 Tau:0.6999 0.4639 0.1826\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 180 Step: 14580 Index:0.4208 R2:0.7840 0.4208 0.4141 RMSE:0.5661 0.8557 0.8800 Tau:0.6996 0.4626 0.1697\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 181 Step: 14661 Index:0.4109 R2:0.7952 0.4109 0.3836 RMSE:0.5335 0.8570 0.8854 Tau:0.7104 0.4586 0.1893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 182 Step: 14742 Index:0.4237 R2:0.7831 0.4237 0.3894 RMSE:0.5270 0.8556 0.8768 Tau:0.6981 0.4619 0.1957\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 183 Step: 14823 Index:0.4003 R2:0.7679 0.4003 0.3883 RMSE:0.5477 0.8670 0.8685 Tau:0.6853 0.4479 0.1707\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 184 Step: 14904 Index:0.3952 R2:0.7901 0.3952 0.3881 RMSE:0.5436 0.8636 0.8785 Tau:0.7026 0.4487 0.1608\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 185 Step: 14985 Index:0.3878 R2:0.7773 0.3878 0.4091 RMSE:0.5597 0.8730 0.8633 Tau:0.6950 0.4338 0.1800\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 186 Step: 15066 Index:0.4184 R2:0.7929 0.4184 0.3877 RMSE:0.5715 0.8710 0.9167 Tau:0.7071 0.4632 0.1865\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 187 Step: 15147 Index:0.4190 R2:0.7983 0.4190 0.4055 RMSE:0.5093 0.8509 0.8627 Tau:0.7102 0.4613 0.1973\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 188 Step: 15228 Index:0.4022 R2:0.7967 0.4022 0.4044 RMSE:0.5119 0.8650 0.8661 Tau:0.7098 0.4505 0.1679\n",
      "Epoch: 189 Step: 15309 Index:0.4335 R2:0.8063 0.4335 0.4124 RMSE:0.5143 0.8375 0.8675 Tau:0.7165 0.4697 0.1851\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 190 Step: 15390 Index:0.4207 R2:0.8044 0.4207 0.3942 RMSE:0.5215 0.8573 0.8935 Tau:0.7155 0.4687 0.1953\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 191 Step: 15471 Index:0.3812 R2:0.7859 0.3812 0.3624 RMSE:0.5295 0.8831 0.9100 Tau:0.7020 0.4454 0.1961\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 192 Step: 15552 Index:0.4147 R2:0.8052 0.4147 0.4187 RMSE:0.5053 0.8596 0.8474 Tau:0.7142 0.4604 0.1757\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 193 Step: 15633 Index:0.4165 R2:0.8053 0.4165 0.3884 RMSE:0.5021 0.8563 0.8884 Tau:0.7174 0.4616 0.1993\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 194 Step: 15714 Index:0.4051 R2:0.8094 0.4051 0.3837 RMSE:0.4989 0.8641 0.8839 Tau:0.7190 0.4583 0.1982\n",
      "Epoch: 195 Step: 15795 Index:0.4381 R2:0.8102 0.4381 0.4171 RMSE:0.5398 0.8483 0.8985 Tau:0.7198 0.4730 0.1787\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 196 Step: 15876 Index:0.4239 R2:0.8008 0.4239 0.3811 RMSE:0.6325 0.9062 0.9982 Tau:0.7145 0.4690 0.1896\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 197 Step: 15957 Index:0.3989 R2:0.8100 0.3989 0.4014 RMSE:0.4949 0.8805 0.8729 Tau:0.7212 0.4493 0.1903\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 198 Step: 16038 Index:0.4334 R2:0.8184 0.4334 0.3919 RMSE:0.4814 0.8464 0.8812 Tau:0.7291 0.4689 0.1974\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 199 Step: 16119 Index:0.4341 R2:0.8030 0.4341 0.3824 RMSE:0.5047 0.8404 0.8839 Tau:0.7196 0.4750 0.1944\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 200 Step: 16200 Index:0.4284 R2:0.7986 0.4284 0.3994 RMSE:0.5054 0.8513 0.8818 Tau:0.7129 0.4697 0.1896\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 201 Step: 16281 Index:0.4245 R2:0.7966 0.4245 0.3901 RMSE:0.5214 0.8762 0.8909 Tau:0.7033 0.4651 0.1992\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 202 Step: 16362 Index:0.4008 R2:0.7976 0.4008 0.3779 RMSE:0.5319 0.8963 0.8916 Tau:0.7095 0.4452 0.1773\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 203 Step: 16443 Index:0.4230 R2:0.8242 0.4230 0.3966 RMSE:0.5027 0.8735 0.8814 Tau:0.7305 0.4677 0.2004\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 204 Step: 16524 Index:0.4026 R2:0.8092 0.4026 0.4042 RMSE:0.5457 0.8903 0.9260 Tau:0.7219 0.4484 0.1743\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 205 Step: 16605 Index:0.4294 R2:0.8133 0.4294 0.3906 RMSE:0.4901 0.8483 0.8739 Tau:0.7197 0.4642 0.1908\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 206 Step: 16686 Index:0.4035 R2:0.8076 0.4035 0.3964 RMSE:0.5118 0.8793 0.9086 Tau:0.7194 0.4598 0.1883\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 207 Step: 16767 Index:0.3960 R2:0.8181 0.3960 0.3666 RMSE:0.5172 0.8782 0.9200 Tau:0.7239 0.4455 0.1995\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 208 Step: 16848 Index:0.4304 R2:0.8313 0.4304 0.4133 RMSE:0.4940 0.8617 0.9040 Tau:0.7390 0.4700 0.1795\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 209 Step: 16929 Index:0.4074 R2:0.8086 0.4074 0.3911 RMSE:0.5134 0.8756 0.8746 Tau:0.7168 0.4523 0.1672\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 210 Step: 17010 Index:0.4183 R2:0.8207 0.4183 0.4121 RMSE:0.5012 0.8660 0.9037 Tau:0.7246 0.4642 0.1921\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 211 Step: 17091 Index:0.4283 R2:0.8275 0.4283 0.3875 RMSE:0.4681 0.8545 0.9004 Tau:0.7312 0.4700 0.1911\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 212 Step: 17172 Index:0.3985 R2:0.8320 0.3985 0.4083 RMSE:0.5043 0.8996 0.8672 Tau:0.7394 0.4479 0.1921\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 213 Step: 17253 Index:0.4295 R2:0.8383 0.4295 0.4053 RMSE:0.4625 0.8484 0.8843 Tau:0.7426 0.4679 0.1859\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 214 Step: 17334 Index:0.4132 R2:0.8158 0.4132 0.3972 RMSE:0.4872 0.8690 0.8814 Tau:0.7237 0.4707 0.1928\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 215 Step: 17415 Index:0.3769 R2:0.8176 0.3769 0.4012 RMSE:0.5017 0.8882 0.8934 Tau:0.7284 0.4412 0.1844\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 216 Step: 17496 Index:0.4121 R2:0.8136 0.4121 0.3913 RMSE:0.4945 0.8661 0.8821 Tau:0.7282 0.4576 0.1839\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 217 Step: 17577 Index:0.4113 R2:0.8366 0.4113 0.3950 RMSE:0.4632 0.8660 0.8956 Tau:0.7445 0.4618 0.2022\n",
      "Epoch: 218 Step: 17658 Index:0.4441 R2:0.8328 0.4441 0.4039 RMSE:0.4683 0.8509 0.8774 Tau:0.7372 0.4753 0.2107\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 219 Step: 17739 Index:0.4358 R2:0.8423 0.4358 0.4202 RMSE:0.4599 0.8405 0.8698 Tau:0.7500 0.4738 0.1967\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 220 Step: 17820 Index:0.4351 R2:0.7784 0.4351 0.3993 RMSE:0.5623 0.8731 0.8763 Tau:0.6968 0.4601 0.1779\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 221 Step: 17901 Index:0.4220 R2:0.8422 0.4220 0.4168 RMSE:0.4658 0.8546 0.8843 Tau:0.7498 0.4704 0.1977\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 222 Step: 17982 Index:0.4154 R2:0.8467 0.4154 0.3852 RMSE:0.4534 0.8589 0.8999 Tau:0.7487 0.4689 0.2016\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 223 Step: 18063 Index:0.4291 R2:0.8335 0.4291 0.3778 RMSE:0.5293 0.9099 0.9221 Tau:0.7348 0.4659 0.2000\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 224 Step: 18144 Index:0.4307 R2:0.8531 0.4307 0.4129 RMSE:0.4485 0.8524 0.8910 Tau:0.7547 0.4723 0.1954\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 225 Step: 18225 Index:0.4180 R2:0.8450 0.4180 0.4229 RMSE:0.4613 0.8583 0.8859 Tau:0.7527 0.4664 0.1960\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 226 Step: 18306 Index:0.4307 R2:0.8489 0.4307 0.4203 RMSE:0.4376 0.8536 0.8756 Tau:0.7533 0.4805 0.2011\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 227 Step: 18387 Index:0.4182 R2:0.8487 0.4182 0.4052 RMSE:0.4409 0.8614 0.8855 Tau:0.7519 0.4738 0.2057\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 228 Step: 18468 Index:0.4221 R2:0.8481 0.4221 0.4007 RMSE:0.4470 0.8732 0.8874 Tau:0.7522 0.4664 0.1900\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 229 Step: 18549 Index:0.4129 R2:0.8529 0.4129 0.3994 RMSE:0.4979 0.8828 0.9449 Tau:0.7529 0.4685 0.1923\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 230 Step: 18630 Index:0.4145 R2:0.8244 0.4145 0.4015 RMSE:0.4828 0.8656 0.8955 Tau:0.7302 0.4518 0.1897\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 231 Step: 18711 Index:0.4158 R2:0.8443 0.4158 0.4265 RMSE:0.4461 0.8629 0.8652 Tau:0.7467 0.4581 0.1816\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 232 Step: 18792 Index:0.4184 R2:0.8341 0.4184 0.4272 RMSE:0.4774 0.8657 0.8923 Tau:0.7365 0.4680 0.2116\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 233 Step: 18873 Index:0.4024 R2:0.8549 0.4024 0.4287 RMSE:0.4402 0.8765 0.8541 Tau:0.7562 0.4560 0.1879\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 234 Step: 18954 Index:0.4273 R2:0.8639 0.4273 0.4226 RMSE:0.4245 0.8472 0.8611 Tau:0.7668 0.4718 0.1905\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 235 Step: 19035 Index:0.3875 R2:0.7981 0.3875 0.4058 RMSE:0.5290 0.8915 0.8658 Tau:0.7138 0.4525 0.1681\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 236 Step: 19116 Index:0.4101 R2:0.8421 0.4101 0.4156 RMSE:0.4521 0.8666 0.8793 Tau:0.7458 0.4702 0.1946\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 237 Step: 19197 Index:0.4103 R2:0.8463 0.4103 0.3856 RMSE:0.4509 0.8613 0.9031 Tau:0.7540 0.4742 0.1928\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 238 Step: 19278 Index:0.4242 R2:0.8427 0.4242 0.3976 RMSE:0.4741 0.8726 0.8808 Tau:0.7506 0.4755 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 239 Step: 19359 Index:0.4374 R2:0.8477 0.4374 0.3923 RMSE:0.4443 0.8498 0.8920 Tau:0.7509 0.4783 0.1938\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 240 Step: 19440 Index:0.4205 R2:0.8543 0.4205 0.3845 RMSE:0.4358 0.8681 0.8995 Tau:0.7559 0.4629 0.2027\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 241 Step: 19521 Index:0.4239 R2:0.8576 0.4239 0.4012 RMSE:0.4384 0.8736 0.8905 Tau:0.7572 0.4745 0.1954\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 242 Step: 19602 Index:0.4243 R2:0.8629 0.4243 0.3921 RMSE:0.4606 0.8775 0.9561 Tau:0.7635 0.4757 0.2053\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 243 Step: 19683 Index:0.4235 R2:0.8439 0.4235 0.4067 RMSE:0.4510 0.8598 0.8966 Tau:0.7486 0.4674 0.1829\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 244 Step: 19764 Index:0.4048 R2:0.8623 0.4048 0.3984 RMSE:0.4204 0.8793 0.8934 Tau:0.7656 0.4601 0.1919\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 245 Step: 19845 Index:0.4021 R2:0.8498 0.4021 0.4002 RMSE:0.4408 0.8686 0.8920 Tau:0.7576 0.4598 0.2005\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 246 Step: 19926 Index:0.3836 R2:0.8531 0.3836 0.3748 RMSE:0.4320 0.8980 0.9211 Tau:0.7602 0.4452 0.2067\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 247 Step: 20007 Index:0.4331 R2:0.8636 0.4331 0.4166 RMSE:0.4619 0.8704 0.9302 Tau:0.7653 0.4763 0.2009\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 248 Step: 20088 Index:0.4128 R2:0.8728 0.4128 0.4122 RMSE:0.4080 0.8709 0.8857 Tau:0.7711 0.4636 0.2109\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 248 r2:0.4039 RMSE:0.8774 WTI:0.2757 AP:0.3255 Tau:0.2107 \n",
      " \n",
      " Top-1:0.2500 Top-1-fp:0.5000 \n",
      " Top-5:0.4000 Top-5-fp:0.5000 \n",
      " Top-10:0.3951 Top-10-fp:0.5556 \n",
      " Top-15:0.3443 Top-15-fp:0.6393 \n",
      " Top-20:0.3667 Top-20-fp:0.6605 \n",
      " Top-25:0.4467 Top-25-fp:0.6700 \n",
      " Top-30:0.4933 Top-30-fp:0.6967 \n",
      " Top-40:0.6000 Top-40-fp:0.7231 \n",
      " Top-50:0.7333 Top-50-fp:0.7297 \n",
      " \n",
      " Top50:0.4400 Top50-fp:0.5000 \n",
      " Top100:0.3500 Top100-fp:0.6100 \n",
      " Top150:0.3467 Top150-fp:0.6533 \n",
      " Top200:0.4467 Top200-fp:0.6650 \n",
      " Top250:0.5067 Top250-fp:0.6960 \n",
      " Top300:0.5733 Top300-fp:0.7133 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
