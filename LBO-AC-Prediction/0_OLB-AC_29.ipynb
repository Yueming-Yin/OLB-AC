{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P29274_0.3333333333333333_300\n",
      "model_file/0_GAFSE_Ki_P29274_0.3333333333333333_300_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P29274_0.3333333333333333_300_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P29274_0.3333333333333333_300_test.csv\"\n",
    "test_active = 300\n",
    "val_rate = 0.1\n",
    "random_seed = 1\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0              CN1C2C(C3=C1C=NC=C3)N(C(=O)N(C2=O)C)C -3.622214\n",
      "1          C1=COC(=C1)C2=NC3=C(C=C2C4=NC=NC=C4)NC=N3 -3.556905\n",
      "2  CC(C)(C)OC(=O)NCCNC1=NC2=NC(=NN2C(=N1)NCCNC(=O... -3.639486\n",
      "3  CCCN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OC... -2.810115\n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNC1=NC2=NC(=NN2C(=N1)N)... -1.250420\n",
      "number of all smiles:  1017\n",
      "number of successfully processed smiles:  1017\n",
      "                                              smiles     value  \\\n",
      "0              CN1C2C(C3=C1C=NC=C3)N(C(=O)N(C2=O)C)C -3.622214   \n",
      "1          C1=COC(=C1)C2=NC3=C(C=C2C4=NC=NC=C4)NC=N3 -3.556905   \n",
      "2  CC(C)(C)OC(=O)NCCNC1=NC2=NC(=NN2C(=N1)NCCNC(=O... -3.639486   \n",
      "3  CCCN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OC... -2.810115   \n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNC1=NC2=NC(=NN2C(=N1)N)... -1.250420   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0                   CN1C(=O)C2C(c3ccncc3N2C)N(C)C1=O  \n",
      "1                c1coc(-c2nc3nc[nH]c3cc2-c2ccncn2)c1  \n",
      "2  CC(C)(C)OC(=O)NCCNc1nc(NCCNC(=O)OC(C)(C)C)n2nc...  \n",
      "3  CCCn1c(=O)n(C)c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCN...  \n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNc1nc(N)n2nc(-c3ccco3)n...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1353\n",
      "number of successfully processed smiles:  1353\n",
      "(1353, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OCC(... -3.009999   \n",
      "1   C1CCC(CC1)NC2=NC3=C(C=C2)C(=O)C=C(N3)C4=CC=CC=C4 -3.322219   \n",
      "2        C1CN(CC=C1)CC2=CC3=C(N=C(N=C3S2)C4=COC=N4)N -2.380211   \n",
      "3  C1=CC=C(C=C1)CC(=O)NC2=NC3=CC=CC=C3N4C2=NC(=N4... -2.200002   \n",
      "4           CCC1=C(C=C2C(=C1)C(=O)C(=CO2)C3=CSC=N3)O -3.199999   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CN(Cc1ccccc1)C(=O)COc1ccc(-c2cc3c([nH]2)c(=O)n...  \n",
      "1          O=c1cc(-c2ccccc2)[nH]c2nc(NC3CCCCC3)ccc12  \n",
      "2               Nc1nc(-c2cocn2)nc2sc(CN3CC=CCC3)cc12  \n",
      "3       O=C(Cc1ccccc1)Nc1nc2ccccc2n2nc(-c3ccco3)nc12  \n",
      "4                    CCc1cc2c(=O)c(-c3cscn3)coc2cc1O  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P29274_0.3333333333333333_300_train.pickle\n",
      "./data/benchmark/Ki_P29274_0.3333333333333333_300_train\n",
      "2370\n",
      "feature dicts file saved as ./data/benchmark/Ki_P29274_0.3333333333333333_300_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915, 3) (102, 3) (1353, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 8\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P29274_0.3333333333333333_300_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3126998/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 169 Index:0.0109 R2:0.0013 0.0109 0.0016 RMSE:1.1510 1.0885 1.1591 Tau:0.0393 0.0880 0.0629\n",
      "Epoch: 2 Step: 338 Index:0.0801 R2:0.0888 0.0801 0.1024 RMSE:1.0936 1.0391 1.0930 Tau:0.1907 0.2046 0.0246\n",
      "Epoch: 3 Step: 507 Index:0.1066 R2:0.1361 0.1066 0.1615 RMSE:1.0704 1.0215 1.0645 Tau:0.2602 0.2411 0.0784\n",
      "Epoch: 4 Step: 676 Index:0.1140 R2:0.1651 0.1140 0.1902 RMSE:1.0682 1.0312 1.0653 Tau:0.2994 0.2516 0.1131\n",
      "Epoch: 5 Step: 845 Index:0.1326 R2:0.1816 0.1326 0.2028 RMSE:1.0329 0.9991 1.0237 Tau:0.2988 0.2722 0.0983\n",
      "Epoch: 6 Step: 1014 Index:0.1650 R2:0.2415 0.1650 0.2596 RMSE:1.0218 0.9886 1.0092 Tau:0.3588 0.3180 0.1581\n",
      "Epoch: 7 Step: 1183 Index:0.1849 R2:0.2642 0.1849 0.2838 RMSE:1.0070 0.9810 0.9918 Tau:0.3691 0.3285 0.1576\n",
      "Epoch: 8 Step: 1352 Index:0.1960 R2:0.2948 0.1960 0.3017 RMSE:0.9655 0.9624 0.9602 Tau:0.3856 0.3184 0.1652\n",
      "Epoch: 9 Step: 1521 Index:0.2085 R2:0.3160 0.2085 0.3209 RMSE:0.9461 0.9572 0.9422 Tau:0.4019 0.3383 0.1743\n",
      "Epoch: 10 Step: 1690 Index:0.2237 R2:0.3307 0.2237 0.3343 RMSE:0.9756 0.9693 0.9653 Tau:0.4082 0.3491 0.1838\n",
      "Epoch: 11 Step: 1859 Index:0.2250 R2:0.3360 0.2250 0.3352 RMSE:0.9380 0.9453 0.9339 Tau:0.4085 0.3561 0.1879\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 12 Step: 2028 Index:0.2201 R2:0.3421 0.2201 0.3358 RMSE:0.9300 0.9719 0.9358 Tau:0.4145 0.3487 0.1894\n",
      "Epoch: 13 Step: 2197 Index:0.2288 R2:0.3528 0.2288 0.3510 RMSE:0.9136 0.9544 0.9148 Tau:0.4227 0.3507 0.1835\n",
      "Epoch: 14 Step: 2366 Index:0.2367 R2:0.3530 0.2367 0.3467 RMSE:1.0176 1.0041 1.0100 Tau:0.4197 0.3608 0.1959\n",
      "Epoch: 15 Step: 2535 Index:0.2388 R2:0.3664 0.2388 0.3620 RMSE:0.9126 0.9412 0.9117 Tau:0.4308 0.3612 0.1930\n",
      "Epoch: 16 Step: 2704 Index:0.2432 R2:0.3679 0.2432 0.3658 RMSE:0.9505 0.9663 0.9446 Tau:0.4319 0.3635 0.1895\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 17 Step: 2873 Index:0.2386 R2:0.3709 0.2386 0.3544 RMSE:0.9078 0.9403 0.9146 Tau:0.4307 0.3643 0.1985\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 18 Step: 3042 Index:0.2352 R2:0.3694 0.2352 0.3619 RMSE:0.9005 0.9682 0.9084 Tau:0.4323 0.3464 0.1908\n",
      "Epoch: 19 Step: 3211 Index:0.2447 R2:0.3797 0.2447 0.3705 RMSE:0.8987 0.9401 0.9022 Tau:0.4404 0.3588 0.1955\n",
      "Epoch: 20 Step: 3380 Index:0.2516 R2:0.3874 0.2516 0.3753 RMSE:0.8983 0.9506 0.9061 Tau:0.4421 0.3682 0.2052\n",
      "Epoch: 21 Step: 3549 Index:0.2520 R2:0.3966 0.2520 0.3794 RMSE:0.8826 0.9456 0.8942 Tau:0.4486 0.3596 0.2044\n",
      "Epoch: 22 Step: 3718 Index:0.2527 R2:0.3984 0.2527 0.3822 RMSE:0.8809 0.9633 0.8958 Tau:0.4496 0.3534 0.2040\n",
      "Epoch: 23 Step: 3887 Index:0.2570 R2:0.4042 0.2570 0.3861 RMSE:0.8786 0.9665 0.8959 Tau:0.4531 0.3643 0.2072\n",
      "Epoch: 24 Step: 4056 Index:0.2641 R2:0.4084 0.2641 0.3879 RMSE:0.8809 0.9414 0.8945 Tau:0.4547 0.3744 0.2126\n",
      "Epoch: 25 Step: 4225 Index:0.2702 R2:0.4180 0.2702 0.3940 RMSE:0.8801 0.9290 0.8929 Tau:0.4613 0.3775 0.2176\n",
      "Epoch: 26 Step: 4394 Index:0.2716 R2:0.4215 0.2716 0.4027 RMSE:0.8651 0.9433 0.8788 Tau:0.4641 0.3701 0.2105\n",
      "Epoch: 27 Step: 4563 Index:0.2764 R2:0.4301 0.2764 0.3985 RMSE:0.8800 0.9197 0.8949 Tau:0.4708 0.3717 0.2188\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 28 Step: 4732 Index:0.2700 R2:0.4274 0.2700 0.3854 RMSE:0.8763 0.9450 0.9004 Tau:0.4653 0.3709 0.2258\n",
      "Epoch: 29 Step: 4901 Index:0.2854 R2:0.4445 0.2854 0.4118 RMSE:0.8477 0.9291 0.8708 Tau:0.4780 0.3923 0.2262\n",
      "Epoch: 30 Step: 5070 Index:0.2888 R2:0.4512 0.2888 0.4152 RMSE:0.8414 0.9308 0.8682 Tau:0.4830 0.3888 0.2256\n",
      "Epoch: 31 Step: 5239 Index:0.3006 R2:0.4566 0.3006 0.4127 RMSE:0.8436 0.9074 0.8710 Tau:0.4855 0.3981 0.2336\n",
      "Epoch: 32 Step: 5408 Index:0.3088 R2:0.4648 0.3088 0.4206 RMSE:0.8444 0.9107 0.8750 Tau:0.4914 0.4047 0.2354\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 5577 Index:0.3041 R2:0.4702 0.3041 0.4247 RMSE:0.8347 0.9448 0.8709 Tau:0.4943 0.3930 0.2370\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 34 Step: 5746 Index:0.3053 R2:0.4787 0.3053 0.4205 RMSE:0.8995 0.9345 0.9315 Tau:0.4999 0.3997 0.2421\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 35 Step: 5915 Index:0.3024 R2:0.4783 0.3024 0.4290 RMSE:0.8241 0.9199 0.8577 Tau:0.4974 0.3973 0.2394\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 36 Step: 6084 Index:0.2958 R2:0.4833 0.2958 0.4358 RMSE:0.8458 1.0107 0.8891 Tau:0.5017 0.3965 0.2324\n",
      "Epoch: 37 Step: 6253 Index:0.3253 R2:0.5043 0.3253 0.4343 RMSE:0.8352 0.8933 0.8792 Tau:0.5160 0.4043 0.2449\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 38 Step: 6422 Index:0.3065 R2:0.4972 0.3065 0.4389 RMSE:0.8075 0.9224 0.8502 Tau:0.5121 0.3989 0.2356\n",
      "Epoch: 39 Step: 6591 Index:0.3283 R2:0.5095 0.3283 0.4544 RMSE:0.8266 0.9147 0.8677 Tau:0.5181 0.4140 0.2476\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 40 Step: 6760 Index:0.3216 R2:0.5158 0.3216 0.4525 RMSE:0.7904 0.9232 0.8409 Tau:0.5245 0.3989 0.2476\n",
      "Epoch: 41 Step: 6929 Index:0.3427 R2:0.5198 0.3427 0.4331 RMSE:0.8861 0.9040 0.9371 Tau:0.5231 0.4066 0.2576\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 42 Step: 7098 Index:0.3258 R2:0.5208 0.3258 0.4546 RMSE:0.7866 0.9220 0.8390 Tau:0.5234 0.4066 0.2549\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 43 Step: 7267 Index:0.3226 R2:0.5190 0.3226 0.4454 RMSE:0.8384 1.0063 0.8970 Tau:0.5238 0.4101 0.2517\n",
      "Epoch: 44 Step: 7436 Index:0.3455 R2:0.5365 0.3455 0.4530 RMSE:0.7903 0.8827 0.8519 Tau:0.5337 0.4148 0.2666\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 45 Step: 7605 Index:0.3352 R2:0.5488 0.3352 0.4675 RMSE:0.7911 0.8983 0.8513 Tau:0.5440 0.4074 0.2573\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 46 Step: 7774 Index:0.3375 R2:0.5485 0.3375 0.4769 RMSE:0.7623 0.9246 0.8223 Tau:0.5433 0.4098 0.2548\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 47 Step: 7943 Index:0.2759 R2:0.4834 0.2759 0.4422 RMSE:0.8300 0.9392 0.8558 Tau:0.5029 0.3763 0.2422\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 48 Step: 8112 Index:0.3002 R2:0.5121 0.3002 0.4543 RMSE:0.8135 0.9212 0.8529 Tau:0.5209 0.3981 0.2492\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 49 Step: 8281 Index:0.3128 R2:0.5241 0.3128 0.4690 RMSE:0.7849 0.9264 0.8275 Tau:0.5276 0.3907 0.2499\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 50 Step: 8450 Index:0.3091 R2:0.5355 0.3091 0.4775 RMSE:0.7811 0.9671 0.8279 Tau:0.5377 0.3993 0.2486\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 51 Step: 8619 Index:0.3372 R2:0.5486 0.3372 0.4762 RMSE:0.7740 0.8928 0.8285 Tau:0.5435 0.4140 0.2638\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 52 Step: 8788 Index:0.3351 R2:0.5474 0.3351 0.4734 RMSE:0.7737 0.9099 0.8350 Tau:0.5419 0.4144 0.2602\n",
      "Epoch: 53 Step: 8957 Index:0.3476 R2:0.5642 0.3476 0.4819 RMSE:0.7816 0.8891 0.8468 Tau:0.5525 0.4171 0.2649\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 54 Step: 9126 Index:0.3374 R2:0.5657 0.3374 0.4889 RMSE:0.7646 0.9636 0.8271 Tau:0.5546 0.4109 0.2565\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 55 Step: 9295 Index:0.3404 R2:0.5686 0.3404 0.4926 RMSE:0.8189 0.9188 0.8765 Tau:0.5579 0.4101 0.2473\n",
      "Epoch: 56 Step: 9464 Index:0.3503 R2:0.5783 0.3503 0.4993 RMSE:0.7641 0.9652 0.8260 Tau:0.5614 0.4175 0.2646\n",
      "Epoch: 57 Step: 9633 Index:0.3629 R2:0.5867 0.3629 0.5024 RMSE:0.7334 0.8861 0.8026 Tau:0.5664 0.4272 0.2693\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 58 Step: 9802 Index:0.3507 R2:0.5833 0.3507 0.4937 RMSE:0.7362 0.9091 0.8153 Tau:0.5649 0.4179 0.2578\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 59 Step: 9971 Index:0.3485 R2:0.5903 0.3485 0.5170 RMSE:0.7351 0.9065 0.7960 Tau:0.5719 0.4218 0.2544\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 60 Step: 10140 Index:0.3614 R2:0.5925 0.3614 0.5100 RMSE:0.7429 0.8922 0.8129 Tau:0.5709 0.4241 0.2699\n",
      "Epoch: 61 Step: 10309 Index:0.3699 R2:0.5990 0.3699 0.5055 RMSE:0.7276 0.8737 0.8041 Tau:0.5736 0.4315 0.2695\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 62 Step: 10478 Index:0.3606 R2:0.5937 0.3606 0.4919 RMSE:0.7287 0.8878 0.8095 Tau:0.5705 0.4307 0.2690\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 63 Step: 10647 Index:0.3662 R2:0.6056 0.3662 0.5124 RMSE:0.7186 0.8861 0.7932 Tau:0.5789 0.4284 0.2678\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 64 Step: 10816 Index:0.3658 R2:0.6119 0.3658 0.5189 RMSE:0.7085 0.9106 0.7877 Tau:0.5863 0.4307 0.2600\n",
      "Epoch: 65 Step: 10985 Index:0.3870 R2:0.6186 0.3870 0.5017 RMSE:0.7599 0.8499 0.8460 Tau:0.5885 0.4436 0.2786\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 66 Step: 11154 Index:0.3656 R2:0.6124 0.3656 0.5192 RMSE:0.7076 0.9147 0.7881 Tau:0.5835 0.4296 0.2665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 Step: 11323 Index:0.4002 R2:0.6238 0.4002 0.5249 RMSE:0.7085 0.8475 0.7860 Tau:0.5940 0.4591 0.2752\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 68 Step: 11492 Index:0.3697 R2:0.6239 0.3697 0.5178 RMSE:0.7059 0.9101 0.7912 Tau:0.5919 0.4358 0.2686\n",
      "Epoch: 69 Step: 11661 Index:0.4071 R2:0.6226 0.4071 0.5081 RMSE:0.7187 0.8374 0.8148 Tau:0.5909 0.4490 0.2820\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 70 Step: 11830 Index:0.3866 R2:0.6301 0.3866 0.5193 RMSE:0.6999 0.8714 0.7881 Tau:0.5982 0.4517 0.2668\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 71 Step: 11999 Index:0.3915 R2:0.6249 0.3915 0.5185 RMSE:0.7133 0.8547 0.8060 Tau:0.5958 0.4568 0.2656\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 72 Step: 12168 Index:0.3966 R2:0.6429 0.3966 0.5385 RMSE:0.7036 0.8558 0.7928 Tau:0.6061 0.4603 0.2774\n",
      "Epoch: 73 Step: 12337 Index:0.4153 R2:0.6412 0.4153 0.5340 RMSE:0.6843 0.8494 0.7751 Tau:0.6054 0.4649 0.2821\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 74 Step: 12506 Index:0.4010 R2:0.6502 0.4010 0.5462 RMSE:0.6970 0.8574 0.7894 Tau:0.6110 0.4595 0.2790\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 75 Step: 12675 Index:0.3972 R2:0.6475 0.3972 0.5480 RMSE:0.6736 0.8980 0.7644 Tau:0.6084 0.4537 0.2830\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 76 Step: 12844 Index:0.4056 R2:0.6445 0.4056 0.5400 RMSE:0.6801 0.8911 0.7733 Tau:0.6045 0.4537 0.2830\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 77 Step: 13013 Index:0.4115 R2:0.6552 0.4115 0.5519 RMSE:0.6890 0.8602 0.7873 Tau:0.6133 0.4630 0.2818\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 78 Step: 13182 Index:0.4077 R2:0.6482 0.4077 0.5435 RMSE:0.6905 0.9228 0.7833 Tau:0.6059 0.4529 0.2880\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 79 Step: 13351 Index:0.4021 R2:0.6486 0.4021 0.5564 RMSE:0.6804 0.9128 0.7625 Tau:0.6073 0.4642 0.2776\n",
      "Epoch: 80 Step: 13520 Index:0.4168 R2:0.6603 0.4168 0.5637 RMSE:0.6702 0.8689 0.7613 Tau:0.6180 0.4851 0.2866\n",
      "Epoch: 81 Step: 13689 Index:0.4206 R2:0.6640 0.4206 0.5579 RMSE:0.6598 0.8662 0.7603 Tau:0.6207 0.4785 0.2811\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 82 Step: 13858 Index:0.4125 R2:0.6523 0.4125 0.5497 RMSE:0.6681 0.8763 0.7630 Tau:0.6126 0.4731 0.2737\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 83 Step: 14027 Index:0.4134 R2:0.6642 0.4134 0.5529 RMSE:0.6965 0.8717 0.7991 Tau:0.6224 0.4879 0.2795\n",
      "Epoch: 84 Step: 14196 Index:0.4269 R2:0.6668 0.4269 0.5585 RMSE:0.7426 0.8627 0.8397 Tau:0.6250 0.4937 0.2881\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 85 Step: 14365 Index:0.4111 R2:0.6649 0.4111 0.5406 RMSE:0.6951 0.9319 0.7957 Tau:0.6170 0.4719 0.2841\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 86 Step: 14534 Index:0.4034 R2:0.6674 0.4034 0.5438 RMSE:0.6776 0.8576 0.7875 Tau:0.6207 0.4886 0.2793\n",
      "Epoch: 87 Step: 14703 Index:0.4379 R2:0.6784 0.4379 0.5217 RMSE:0.7126 0.8210 0.8437 Tau:0.6290 0.4882 0.2975\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 88 Step: 14872 Index:0.4320 R2:0.6753 0.4320 0.5537 RMSE:0.6454 0.8686 0.7647 Tau:0.6266 0.4906 0.2909\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 89 Step: 15041 Index:0.4204 R2:0.6788 0.4204 0.5489 RMSE:0.6664 0.9100 0.7762 Tau:0.6312 0.4836 0.2815\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 90 Step: 15210 Index:0.4338 R2:0.6818 0.4338 0.5515 RMSE:0.6428 0.8419 0.7636 Tau:0.6301 0.4906 0.2956\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 91 Step: 15379 Index:0.4336 R2:0.6875 0.4336 0.5600 RMSE:0.6377 0.8781 0.7584 Tau:0.6353 0.4921 0.2958\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 92 Step: 15548 Index:0.4191 R2:0.6705 0.4191 0.5404 RMSE:0.7390 0.8645 0.8539 Tau:0.6201 0.4801 0.2864\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 93 Step: 15717 Index:0.4275 R2:0.6863 0.4275 0.5654 RMSE:0.6777 0.9505 0.7801 Tau:0.6361 0.4836 0.2849\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 94 Step: 15886 Index:0.4338 R2:0.6838 0.4338 0.5529 RMSE:0.6408 0.8604 0.7598 Tau:0.6323 0.4937 0.3010\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 95 Step: 16055 Index:0.4201 R2:0.6911 0.4201 0.5480 RMSE:0.6650 0.9191 0.7856 Tau:0.6375 0.4758 0.2995\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 96 Step: 16224 Index:0.4163 R2:0.6872 0.4163 0.5572 RMSE:0.6337 0.8926 0.7572 Tau:0.6372 0.4813 0.3017\n",
      "Epoch: 97 Step: 16393 Index:0.4438 R2:0.6930 0.4438 0.5719 RMSE:0.6347 0.8606 0.7600 Tau:0.6398 0.5069 0.3037\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 98 Step: 16562 Index:0.4384 R2:0.7047 0.4384 0.5554 RMSE:0.6225 0.8409 0.7570 Tau:0.6505 0.5007 0.3035\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 99 Step: 16731 Index:0.4367 R2:0.6844 0.4367 0.5413 RMSE:0.6370 0.8602 0.7725 Tau:0.6341 0.4945 0.3002\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 100 Step: 16900 Index:0.4414 R2:0.7073 0.4414 0.5560 RMSE:0.6235 0.8306 0.7637 Tau:0.6512 0.5011 0.2992\n",
      "Epoch: 101 Step: 17069 Index:0.4535 R2:0.6951 0.4535 0.5407 RMSE:0.6671 0.8099 0.8084 Tau:0.6439 0.5104 0.2941\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 102 Step: 17238 Index:0.4331 R2:0.7053 0.4331 0.5734 RMSE:0.6148 0.8824 0.7472 Tau:0.6468 0.4906 0.2990\n",
      "Epoch: 103 Step: 17407 Index:0.4551 R2:0.6985 0.4551 0.5637 RMSE:0.6248 0.8272 0.7501 Tau:0.6440 0.5061 0.3007\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 104 Step: 17576 Index:0.4285 R2:0.6848 0.4285 0.5198 RMSE:0.6449 0.8592 0.7869 Tau:0.6320 0.4599 0.2963\n",
      "Epoch: 105 Step: 17745 Index:0.4553 R2:0.7176 0.4553 0.5771 RMSE:0.6033 0.8545 0.7408 Tau:0.6582 0.5073 0.3061\n",
      "Epoch: 106 Step: 17914 Index:0.4598 R2:0.7062 0.4598 0.5669 RMSE:0.6170 0.8310 0.7555 Tau:0.6496 0.4991 0.3037\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 107 Step: 18083 Index:0.4338 R2:0.7091 0.4338 0.5591 RMSE:0.6231 0.8836 0.7608 Tau:0.6488 0.4941 0.2984\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 108 Step: 18252 Index:0.4490 R2:0.7203 0.4490 0.5749 RMSE:0.6186 0.9111 0.7566 Tau:0.6600 0.4929 0.3109\n",
      "Epoch: 109 Step: 18421 Index:0.4608 R2:0.7070 0.4608 0.5679 RMSE:0.6399 0.8709 0.7948 Tau:0.6500 0.5186 0.3080\n",
      "Epoch: 110 Step: 18590 Index:0.4615 R2:0.7290 0.4615 0.5640 RMSE:0.5960 0.8562 0.7536 Tau:0.6676 0.5096 0.3057\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 111 Step: 18759 Index:0.4609 R2:0.7269 0.4609 0.5362 RMSE:0.7120 0.8284 0.8739 Tau:0.6652 0.5007 0.3135\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 112 Step: 18928 Index:0.4590 R2:0.7363 0.4590 0.5583 RMSE:0.5884 0.8241 0.7673 Tau:0.6716 0.5085 0.3075\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 113 Step: 19097 Index:0.4609 R2:0.7329 0.4609 0.5659 RMSE:0.6505 0.8303 0.8217 Tau:0.6681 0.4956 0.3040\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 114 Step: 19266 Index:0.4613 R2:0.7238 0.4613 0.5718 RMSE:0.6504 0.8386 0.8048 Tau:0.6640 0.5108 0.3058\n",
      "Epoch: 115 Step: 19435 Index:0.4620 R2:0.7323 0.4620 0.5778 RMSE:0.5899 0.8508 0.7390 Tau:0.6700 0.5034 0.3074\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 116 Step: 19604 Index:0.4588 R2:0.7319 0.4588 0.5520 RMSE:0.5924 0.8111 0.7622 Tau:0.6683 0.4910 0.3050\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 117 Step: 19773 Index:0.4507 R2:0.7243 0.4507 0.5481 RMSE:0.5990 0.8293 0.7724 Tau:0.6602 0.4778 0.3183\n",
      "Epoch: 118 Step: 19942 Index:0.4702 R2:0.7461 0.4702 0.5666 RMSE:0.5844 0.8488 0.7539 Tau:0.6770 0.5081 0.3090\n",
      "Epoch: 119 Step: 20111 Index:0.4867 R2:0.7519 0.4867 0.5547 RMSE:0.5789 0.7845 0.7606 Tau:0.6832 0.5182 0.3150\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 120 Step: 20280 Index:0.4713 R2:0.7461 0.4713 0.5653 RMSE:0.5930 0.8085 0.7756 Tau:0.6792 0.5224 0.3095\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 121 Step: 20449 Index:0.4688 R2:0.7514 0.4688 0.5523 RMSE:0.5861 0.7993 0.7825 Tau:0.6816 0.5053 0.3107\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 122 Step: 20618 Index:0.3108 R2:0.6127 0.3108 0.3970 RMSE:0.7549 0.9065 0.9277 Tau:0.6012 0.3950 0.2748\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 123 Step: 20787 Index:0.4698 R2:0.7246 0.4698 0.5641 RMSE:0.6164 0.8091 0.7867 Tau:0.6630 0.5092 0.3034\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 124 Step: 20956 Index:0.4712 R2:0.7492 0.4712 0.5453 RMSE:0.6215 0.7976 0.8140 Tau:0.6811 0.5259 0.3018\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 125 Step: 21125 Index:0.4544 R2:0.7446 0.4544 0.5501 RMSE:0.5835 0.8305 0.7617 Tau:0.6771 0.5081 0.2942\n",
      "Epoch: 126 Step: 21294 Index:0.4965 R2:0.7637 0.4965 0.5710 RMSE:0.5642 0.7804 0.7583 Tau:0.6913 0.5306 0.3089\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 127 Step: 21463 Index:0.3951 R2:0.5033 0.3951 0.4369 RMSE:0.8291 0.8526 0.8861 Tau:0.5202 0.4486 0.2805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 128 Step: 21632 Index:0.4601 R2:0.6813 0.4601 0.5632 RMSE:0.6429 0.8331 0.7519 Tau:0.6396 0.5085 0.3067\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 129 Step: 21801 Index:0.4548 R2:0.7232 0.4548 0.5622 RMSE:0.5994 0.8287 0.7554 Tau:0.6666 0.4956 0.3126\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 130 Step: 21970 Index:0.4658 R2:0.7431 0.4658 0.5572 RMSE:0.5818 0.8202 0.7573 Tau:0.6787 0.4980 0.3166\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 131 Step: 22139 Index:0.4617 R2:0.7454 0.4617 0.5857 RMSE:0.5752 0.8478 0.7443 Tau:0.6805 0.5135 0.3133\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 132 Step: 22308 Index:0.4615 R2:0.7624 0.4615 0.5749 RMSE:0.5615 0.8570 0.7424 Tau:0.6951 0.5131 0.3104\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 133 Step: 22477 Index:0.4817 R2:0.7693 0.4817 0.5659 RMSE:0.5635 0.7938 0.7610 Tau:0.6987 0.5228 0.3078\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 134 Step: 22646 Index:0.4713 R2:0.7669 0.4713 0.5690 RMSE:0.5571 0.8360 0.7470 Tau:0.6949 0.5046 0.3147\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 135 Step: 22815 Index:0.4656 R2:0.7660 0.4656 0.5768 RMSE:0.5490 0.8364 0.7460 Tau:0.6944 0.5123 0.3080\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 136 Step: 22984 Index:0.4644 R2:0.7709 0.4644 0.5828 RMSE:0.5480 0.8480 0.7372 Tau:0.6980 0.5073 0.3158\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 137 Step: 23153 Index:0.4613 R2:0.7598 0.4613 0.5640 RMSE:0.5649 0.8250 0.7800 Tau:0.6896 0.4991 0.3257\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 138 Step: 23322 Index:0.4806 R2:0.7782 0.4806 0.5680 RMSE:0.5385 0.8077 0.7512 Tau:0.7037 0.5174 0.3144\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 139 Step: 23491 Index:0.4719 R2:0.7783 0.4719 0.5819 RMSE:0.5622 0.8171 0.7716 Tau:0.7032 0.5015 0.3254\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 140 Step: 23660 Index:0.4806 R2:0.7784 0.4806 0.5752 RMSE:0.5787 0.8049 0.7835 Tau:0.7038 0.5236 0.3095\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 141 Step: 23829 Index:0.4735 R2:0.7819 0.4735 0.5578 RMSE:0.5606 0.7873 0.7743 Tau:0.7042 0.5096 0.3121\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 142 Step: 23998 Index:0.4784 R2:0.7786 0.4784 0.5733 RMSE:0.5363 0.8126 0.7509 Tau:0.7045 0.5259 0.3118\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 143 Step: 24167 Index:0.4693 R2:0.7832 0.4693 0.5653 RMSE:0.6239 0.8294 0.8285 Tau:0.7075 0.5139 0.3039\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 144 Step: 24336 Index:0.4961 R2:0.7687 0.4961 0.5784 RMSE:0.5963 0.7979 0.7891 Tau:0.6943 0.5259 0.3124\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 145 Step: 24505 Index:0.4667 R2:0.7812 0.4667 0.5737 RMSE:0.5450 0.8373 0.7452 Tau:0.7044 0.5081 0.3199\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 146 Step: 24674 Index:0.4723 R2:0.7885 0.4723 0.5737 RMSE:0.5468 0.8029 0.7722 Tau:0.7096 0.5147 0.3248\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 147 Step: 24843 Index:0.4619 R2:0.7799 0.4619 0.5652 RMSE:0.5583 0.8253 0.7946 Tau:0.7037 0.4987 0.3244\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 148 Step: 25012 Index:0.4673 R2:0.7719 0.4673 0.5554 RMSE:0.5418 0.8122 0.7643 Tau:0.6983 0.4972 0.3132\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 149 Step: 25181 Index:0.4512 R2:0.7621 0.4512 0.5545 RMSE:0.5575 0.8414 0.7756 Tau:0.6870 0.4750 0.3189\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 150 Step: 25350 Index:0.4626 R2:0.7755 0.4626 0.5682 RMSE:0.5367 0.8476 0.7615 Tau:0.6992 0.4933 0.3247\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 151 Step: 25519 Index:0.4798 R2:0.7908 0.4798 0.5845 RMSE:0.5271 0.8050 0.7380 Tau:0.7144 0.5255 0.3187\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 152 Step: 25688 Index:0.4848 R2:0.7392 0.4848 0.5510 RMSE:0.5892 0.8764 0.7868 Tau:0.6775 0.5154 0.3148\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 153 Step: 25857 Index:0.4852 R2:0.7849 0.4852 0.5427 RMSE:0.6288 0.8128 0.8650 Tau:0.7101 0.5213 0.3104\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 154 Step: 26026 Index:0.4941 R2:0.7838 0.4941 0.5814 RMSE:0.5304 0.8287 0.7417 Tau:0.7103 0.5298 0.3174\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 155 Step: 26195 Index:0.4889 R2:0.7929 0.4889 0.5725 RMSE:0.5271 0.8021 0.7422 Tau:0.7151 0.5267 0.3175\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 156 Step: 26364 Index:0.4743 R2:0.7755 0.4743 0.5594 RMSE:0.5445 0.8130 0.7594 Tau:0.7006 0.5011 0.3158\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 156 r2:0.5710 RMSE:0.7583 WTI:0.3245 AP:0.4879 Tau:0.3089 \n",
      " \n",
      " Top-1:0.3077 Top-1-fp:0.1538 \n",
      " Top-5:0.6119 Top-5-fp:0.1940 \n",
      " Top-10:0.6296 Top-10-fp:0.3037 \n",
      " Top-15:0.5248 Top-15-fp:0.4406 \n",
      " Top-20:0.4926 Top-20-fp:0.5037 \n",
      " Top-25:0.5100 Top-25-fp:0.5473 \n",
      " Top-30:0.5633 Top-30-fp:0.5827 \n",
      " Top-40:0.6800 Top-40-fp:0.6229 \n",
      " Top-50:0.7667 Top-50-fp:0.6598 \n",
      " \n",
      " Top50:0.5800 Top50-fp:0.2200 \n",
      " Top100:0.6700 Top100-fp:0.2300 \n",
      " Top150:0.5933 Top150-fp:0.3467 \n",
      " Top200:0.5300 Top200-fp:0.4350 \n",
      " Top250:0.5160 Top250-fp:0.4800 \n",
      " Top300:0.4700 Top300-fp:0.5300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
