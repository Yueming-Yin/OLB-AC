{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC50_P41143_1_200\n",
      "model_file/0_GAFSE_EC50_P41143_1_200_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/EC50_P41143_1_200_train.csv\"\n",
    "test_filename = \"./data/benchmark/EC50_P41143_1_200_test.csv\"\n",
    "test_active = 200\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC1=CC(=CC(=C1CC(C(=O)NC(C)C(=O)NCC(=O)NC(CC2=... -2.230449\n",
      "1  COC1=CC(=C(C=C1)C(=O)NC2(CCN(CC2)C3=NC=CC(=C3)... -1.146128\n",
      "2  CCN(CC)C(=O)C1=CC=C(C=C1)C2CC3(CCNCC3)OC4=CC=C... -2.623249\n",
      "3  CCCCC(C(=O)N1CCCC1C(=O)NC(CC(C)C)C(=O)NC(CC2=C... -0.361728\n",
      "4  CCN(CC)C(=O)C1=CC=C(C=C1)N(C2CCN(CC2)CCC3=CC(=... -0.417472\n",
      "number of all smiles:  523\n",
      "number of successfully processed smiles:  523\n",
      "                                              smiles     value  \\\n",
      "0  CC1=CC(=CC(=C1CC(C(=O)NC(C)C(=O)NCC(=O)NC(CC2=... -2.230449   \n",
      "1  COC1=CC(=C(C=C1)C(=O)NC2(CCN(CC2)C3=NC=CC(=C3)... -1.146128   \n",
      "2  CCN(CC)C(=O)C1=CC=C(C=C1)C2CC3(CCNCC3)OC4=CC=C... -2.623249   \n",
      "3  CCCCC(C(=O)N1CCCC1C(=O)NC(CC(C)C)C(=O)NC(CC2=C... -0.361728   \n",
      "4  CCN(CC)C(=O)C1=CC=C(C=C1)N(C2CCN(CC2)CCC3=CC(=... -0.417472   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  Cc1cc(O)cc(C)c1CC(N)C(=O)NC(C)C(=O)NCC(=O)NC(C...  \n",
      "1  COc1ccc(C(=O)NC2(c3ccccc3)CCN(c3cc(N)ccn3)CC2)...  \n",
      "2      CCN(CC)C(=O)c1ccc(C2CC3(CCNCC3)Oc3ccccc32)cc1  \n",
      "3  CCCCC(NC(=O)C(Cc1ccccc1)NC(=O)CNC(=O)C(C)NC(=O...  \n",
      "4  CCN(CC)C(=O)c1ccc(N(c2cccc(O)c2)C2CCN(CCc3cccc...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  893\n",
      "number of successfully processed smiles:  893\n",
      "(893, 3)\n",
      "                                              smiles     value  \\\n",
      "0            C1CN(CCC12C3=CC=CC=C3CO2)C4=NC=CC(=C4)N -2.488551   \n",
      "1  COC1=C2C3=C(CC4C56C3(CCN4CC7CC7)C(O2)C(CC5)(C(... -2.216773   \n",
      "2  CC(C(=O)NC(CC1=CC=CC=C1)C(=O)N(C)CC(=O)N)N(C)C... -2.982271   \n",
      "3  COC1=C2C3=C(CC4C5(C3(CCN4CC6CC6)C(O2)C(=O)CC5)... -1.243038   \n",
      "4  CCCCC(C(=O)NC(CC(=O)O)C(=O)NC(CC1=CC=CC=C1)C(=... -0.397940   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0                  Nc1ccnc(N2CCC3(CC2)OCc2ccccc23)c1  \n",
      "1  COc1ccc2c3c1OC1C4(OC)CCC5(CC4COCc4ccc(C(O)CO)c...  \n",
      "2  CC(C(=O)NC(Cc1ccccc1)C(=O)N(C)CC(N)=O)N(C)C(=O...  \n",
      "3  COc1ccc2c3c1OC1C(=O)CCC4(NC(=O)C=Cc5ccccc5Cl)C...  \n",
      "4  CCCCC(C(=O)NC(CC(=O)O)C(=O)NC(Cc1ccccc1)C(N)=O...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/EC50_P41143_1_200_train.pickle\n",
      "./data/benchmark/EC50_P41143_1_200_train\n",
      "1416\n",
      "feature dicts file saved as ./data/benchmark/EC50_P41143_1_200_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 3) (105, 3) (893, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_EC50_P41143_1_200_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2753547/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 89 Index:-1.0470 R2:0.0141 0.0235 0.0332 RMSE:1.1411 1.1345 1.0171 Tau:0.0393 0.0875 -0.0451\n",
      "Epoch: 2 Step: 178 Index:-0.9765 R2:0.0343 0.0706 0.0287 RMSE:1.1483 1.1240 1.0522 Tau:0.1178 0.1476 0.1252\n",
      "Epoch: 3 Step: 267 Index:-0.9058 R2:0.0613 0.1112 0.0219 RMSE:1.1170 1.1033 1.0237 Tau:0.2023 0.1975 0.3082\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 4 Step: 356 Index:-0.9156 R2:0.0561 0.0988 0.0202 RMSE:1.1453 1.1247 1.0587 Tau:0.2067 0.2092 0.3575\n",
      "Epoch: 5 Step: 445 Index:-0.8686 R2:0.0805 0.1298 0.0196 RMSE:1.1089 1.0939 1.0294 Tau:0.2278 0.2253 0.3845\n",
      "Epoch: 6 Step: 534 Index:-0.8554 R2:0.0854 0.1291 0.0196 RMSE:1.0993 1.0777 1.0371 Tau:0.2249 0.2224 0.3970\n",
      "Epoch: 7 Step: 623 Index:-0.8070 R2:0.1168 0.1695 0.0271 RMSE:1.0967 1.0814 1.0294 Tau:0.2604 0.2745 0.4134\n",
      "Epoch: 8 Step: 712 Index:-0.7538 R2:0.1562 0.1968 0.0387 RMSE:1.0719 1.0550 1.0200 Tau:0.2840 0.3012 0.4348\n",
      "Epoch: 9 Step: 801 Index:-0.7260 R2:0.2060 0.2372 0.0475 RMSE:1.0842 1.0679 1.0283 Tau:0.3081 0.3419 0.4420\n",
      "Epoch: 10 Step: 890 Index:-0.6866 R2:0.2077 0.2387 0.0451 RMSE:1.0511 1.0369 1.0123 Tau:0.3015 0.3504 0.4400\n",
      "Epoch: 11 Step: 979 Index:-0.6832 R2:0.2053 0.2297 0.0554 RMSE:1.0429 1.0288 1.0306 Tau:0.3029 0.3456 0.4537\n",
      "Epoch: 12 Step: 1068 Index:-0.6463 R2:0.2381 0.2556 0.0646 RMSE:1.0211 1.0084 1.0082 Tau:0.3202 0.3621 0.4575\n",
      "Epoch: 13 Step: 1157 Index:-0.6241 R2:0.2567 0.2687 0.0731 RMSE:1.0085 0.9986 1.0099 Tau:0.3321 0.3746 0.4568\n",
      "Epoch: 14 Step: 1246 Index:-0.6059 R2:0.2846 0.2835 0.0819 RMSE:1.0089 1.0035 0.9966 Tau:0.3706 0.3977 0.4635\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 15 Step: 1335 Index:-0.6299 R2:0.2684 0.2753 0.0747 RMSE:1.0022 1.0000 1.0255 Tau:0.3400 0.3702 0.4656\n",
      "Epoch: 16 Step: 1424 Index:-0.5875 R2:0.2962 0.2953 0.0795 RMSE:0.9993 0.9936 1.0093 Tau:0.3752 0.4061 0.4631\n",
      "Epoch: 17 Step: 1513 Index:-0.5722 R2:0.3053 0.2963 0.0945 RMSE:0.9671 0.9684 0.9935 Tau:0.3741 0.3962 0.4693\n",
      "Epoch: 18 Step: 1602 Index:-0.5661 R2:0.3159 0.3054 0.0996 RMSE:0.9706 0.9747 0.9989 Tau:0.3821 0.4087 0.4692\n",
      "Epoch: 19 Step: 1691 Index:-0.5529 R2:0.3260 0.3055 0.0915 RMSE:0.9655 0.9667 0.9895 Tau:0.3965 0.4138 0.4695\n",
      "Epoch: 20 Step: 1780 Index:-0.5311 R2:0.3365 0.3221 0.1044 RMSE:0.9469 0.9493 0.9861 Tau:0.3996 0.4182 0.4706\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 21 Step: 1869 Index:-0.5347 R2:0.3325 0.3203 0.1030 RMSE:0.9471 0.9474 0.9983 Tau:0.3880 0.4127 0.4758\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 22 Step: 1958 Index:-0.5471 R2:0.3421 0.3162 0.0894 RMSE:0.9530 0.9664 1.0054 Tau:0.4054 0.4193 0.4763\n",
      "Epoch: 23 Step: 2047 Index:-0.5160 R2:0.3575 0.3262 0.1066 RMSE:0.9273 0.9412 0.9915 Tau:0.4148 0.4252 0.4813\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 24 Step: 2136 Index:-0.5213 R2:0.3705 0.3281 0.1216 RMSE:0.9360 0.9505 0.9740 Tau:0.4279 0.4292 0.4828\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 25 Step: 2225 Index:-0.5447 R2:0.3814 0.3294 0.1228 RMSE:0.9442 0.9731 0.9939 Tau:0.4344 0.4285 0.4846\n",
      "Epoch: 26 Step: 2314 Index:-0.4860 R2:0.3883 0.3461 0.1294 RMSE:0.9054 0.9269 0.9768 Tau:0.4396 0.4409 0.4836\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 27 Step: 2403 Index:-0.4906 R2:0.3981 0.3440 0.1280 RMSE:0.9008 0.9308 0.9749 Tau:0.4440 0.4402 0.4863\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 28 Step: 2492 Index:-0.5428 R2:0.4036 0.3379 0.1345 RMSE:0.9348 0.9741 0.9909 Tau:0.4506 0.4314 0.4858\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 29 Step: 2581 Index:-0.5088 R2:0.3943 0.3460 0.1345 RMSE:0.9069 0.9486 0.9897 Tau:0.4302 0.4398 0.4895\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 30 Step: 2670 Index:-0.4896 R2:0.4295 0.3578 0.1479 RMSE:0.8837 0.9394 0.9725 Tau:0.4659 0.4497 0.4897\n",
      "Epoch: 31 Step: 2759 Index:-0.4670 R2:0.4246 0.3652 0.1538 RMSE:0.8854 0.9185 0.9821 Tau:0.4582 0.4516 0.4929\n",
      "Epoch: 32 Step: 2848 Index:-0.4572 R2:0.4431 0.3610 0.1567 RMSE:0.8739 0.9165 0.9549 Tau:0.4812 0.4593 0.4916\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 2937 Index:-0.4723 R2:0.4423 0.3596 0.1499 RMSE:0.8726 0.9213 0.9773 Tau:0.4848 0.4490 0.4952\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 34 Step: 3026 Index:-0.4646 R2:0.4456 0.3691 0.1628 RMSE:0.8572 0.9187 0.9640 Tau:0.4723 0.4541 0.4902\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 35 Step: 3115 Index:-0.4674 R2:0.4539 0.3698 0.1631 RMSE:0.8518 0.9189 0.9629 Tau:0.4847 0.4516 0.4973\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 36 Step: 3204 Index:-0.4789 R2:0.4634 0.3747 0.1663 RMSE:0.8579 0.9444 0.9732 Tau:0.4929 0.4655 0.4946\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 37 Step: 3293 Index:-0.4575 R2:0.4731 0.3645 0.1600 RMSE:0.8460 0.9153 0.9642 Tau:0.5007 0.4578 0.4943\n",
      "Epoch: 38 Step: 3382 Index:-0.4481 R2:0.4748 0.3800 0.1799 RMSE:0.8373 0.9070 0.9570 Tau:0.5083 0.4589 0.5013\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 39 Step: 3471 Index:-0.4597 R2:0.4870 0.3753 0.1857 RMSE:0.8340 0.9167 0.9438 Tau:0.5116 0.4571 0.5011\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 40 Step: 3560 Index:-0.5628 R2:0.4772 0.3799 0.1991 RMSE:0.9222 1.0268 1.0160 Tau:0.4952 0.4640 0.5007\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 41 Step: 3649 Index:-0.4523 R2:0.4785 0.3779 0.1708 RMSE:0.8632 0.9200 0.9844 Tau:0.5142 0.4677 0.5009\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 42 Step: 3738 Index:-0.5601 R2:0.4956 0.3676 0.1640 RMSE:0.9189 1.0223 1.0320 Tau:0.5132 0.4622 0.4988\n",
      "Epoch: 43 Step: 3827 Index:-0.4358 R2:0.5143 0.4000 0.1942 RMSE:0.8240 0.9149 0.9445 Tau:0.5319 0.4791 0.5019\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 44 Step: 3916 Index:-0.4989 R2:0.5189 0.3881 0.1914 RMSE:0.8548 0.9648 0.9751 Tau:0.5298 0.4659 0.4971\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 45 Step: 4005 Index:-0.4435 R2:0.4982 0.3958 0.1889 RMSE:0.8614 0.9218 0.9981 Tau:0.5232 0.4783 0.5046\n",
      "Epoch: 46 Step: 4094 Index:-0.4299 R2:0.5286 0.3951 0.2039 RMSE:0.8008 0.9031 0.9335 Tau:0.5408 0.4732 0.5013\n",
      "Epoch: 47 Step: 4183 Index:-0.4126 R2:0.5217 0.4050 0.2085 RMSE:0.8343 0.8983 0.9561 Tau:0.5456 0.4857 0.5044\n",
      "Epoch: 48 Step: 4272 Index:-0.4041 R2:0.5376 0.4148 0.2098 RMSE:0.7894 0.8930 0.9343 Tau:0.5463 0.4890 0.5024\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 49 Step: 4361 Index:-0.4102 R2:0.5388 0.4064 0.2039 RMSE:0.7862 0.8904 0.9381 Tau:0.5472 0.4802 0.5003\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 50 Step: 4450 Index:-0.4305 R2:0.5235 0.3942 0.2037 RMSE:0.8098 0.8985 0.9499 Tau:0.5375 0.4681 0.4925\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 51 Step: 4539 Index:-0.4275 R2:0.5509 0.4055 0.2247 RMSE:0.7891 0.9084 0.9272 Tau:0.5526 0.4809 0.5035\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 52 Step: 4628 Index:-0.4070 R2:0.5511 0.4166 0.2107 RMSE:0.7931 0.8857 0.9553 Tau:0.5536 0.4787 0.4990\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 53 Step: 4717 Index:-0.4344 R2:0.5478 0.4067 0.2283 RMSE:0.7950 0.8992 0.9196 Tau:0.5454 0.4648 0.5007\n",
      "Epoch: 54 Step: 4806 Index:-0.3801 R2:0.5544 0.4290 0.2180 RMSE:0.7796 0.8738 0.9441 Tau:0.5579 0.4937 0.5005\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 55 Step: 4895 Index:-0.4060 R2:0.5564 0.4212 0.2063 RMSE:0.7767 0.9030 0.9463 Tau:0.5640 0.4970 0.5092\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 56 Step: 4984 Index:-0.3964 R2:0.5670 0.4151 0.2225 RMSE:0.7692 0.8847 0.9205 Tau:0.5658 0.4882 0.5030\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 57 Step: 5073 Index:-0.4713 R2:0.5694 0.4156 0.2202 RMSE:0.8004 0.9460 0.9642 Tau:0.5592 0.4747 0.5028\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 58 Step: 5162 Index:-0.3939 R2:0.5595 0.4127 0.2223 RMSE:0.7777 0.8865 0.9369 Tau:0.5590 0.4926 0.5030\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 59 Step: 5251 Index:-0.4172 R2:0.5599 0.4091 0.2256 RMSE:0.8138 0.9102 0.9668 Tau:0.5683 0.4930 0.5091\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 60 Step: 5340 Index:-0.4017 R2:0.5787 0.4238 0.2258 RMSE:0.7565 0.8918 0.9290 Tau:0.5713 0.4901 0.5026\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 61 Step: 5429 Index:-0.3944 R2:0.5697 0.4290 0.2392 RMSE:0.7555 0.8856 0.9224 Tau:0.5664 0.4912 0.5104\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 62 Step: 5518 Index:-0.4194 R2:0.5843 0.4333 0.2456 RMSE:0.7631 0.8999 0.9233 Tau:0.5768 0.4805 0.5088\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 63 Step: 5607 Index:-0.5021 R2:0.5922 0.4261 0.2277 RMSE:0.8231 0.9833 0.9935 Tau:0.5800 0.4813 0.5070\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 64 Step: 5696 Index:-0.3953 R2:0.5845 0.4240 0.2350 RMSE:0.7793 0.8846 0.9416 Tau:0.5759 0.4893 0.5070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 Step: 5785 Index:-0.3792 R2:0.5948 0.4380 0.2431 RMSE:0.7390 0.8780 0.9155 Tau:0.5850 0.4989 0.5073\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 66 Step: 5874 Index:-0.3885 R2:0.5941 0.4404 0.2432 RMSE:0.7433 0.8896 0.9238 Tau:0.5835 0.5011 0.5082\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 67 Step: 5963 Index:-0.4337 R2:0.6088 0.4433 0.2423 RMSE:0.7639 0.9300 0.9511 Tau:0.5904 0.4963 0.5116\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 68 Step: 6052 Index:-0.4709 R2:0.6086 0.4417 0.2413 RMSE:0.7940 0.9639 0.9794 Tau:0.5859 0.4930 0.5041\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 69 Step: 6141 Index:-0.4259 R2:0.5976 0.4378 0.2248 RMSE:0.7506 0.9262 0.9635 Tau:0.5840 0.5003 0.5085\n",
      "Epoch: 70 Step: 6230 Index:-0.3787 R2:0.6068 0.4326 0.2409 RMSE:0.7298 0.8750 0.9288 Tau:0.5881 0.4963 0.5087\n",
      "Epoch: 71 Step: 6319 Index:-0.3684 R2:0.6139 0.4432 0.2571 RMSE:0.7177 0.8727 0.9103 Tau:0.5951 0.5044 0.5134\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 72 Step: 6408 Index:-0.4449 R2:0.6176 0.4360 0.2420 RMSE:0.7700 0.9397 0.9569 Tau:0.5926 0.4948 0.5055\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 73 Step: 6497 Index:-0.4205 R2:0.6009 0.4269 0.2453 RMSE:0.8049 0.9149 0.9789 Tau:0.5877 0.4945 0.5050\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 74 Step: 6586 Index:-0.3812 R2:0.6234 0.4347 0.2437 RMSE:0.7151 0.8804 0.9167 Tau:0.6004 0.4992 0.5100\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 75 Step: 6675 Index:-0.4214 R2:0.6192 0.4283 0.2453 RMSE:0.7376 0.9103 0.9300 Tau:0.5904 0.4890 0.5034\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 76 Step: 6764 Index:-0.3729 R2:0.6246 0.4500 0.2526 RMSE:0.7496 0.8791 0.9573 Tau:0.5984 0.5062 0.5112\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 77 Step: 6853 Index:-0.3805 R2:0.6212 0.4367 0.2524 RMSE:0.7116 0.8761 0.9163 Tau:0.5952 0.4956 0.5081\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 78 Step: 6942 Index:-0.3831 R2:0.6321 0.4438 0.2684 RMSE:0.7179 0.8860 0.9062 Tau:0.6001 0.5029 0.5118\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 79 Step: 7031 Index:-0.3858 R2:0.6354 0.4294 0.2587 RMSE:0.7044 0.8747 0.9052 Tau:0.6045 0.4890 0.5133\n",
      "Epoch: 80 Step: 7120 Index:-0.3631 R2:0.6366 0.4461 0.2803 RMSE:0.7102 0.8664 0.8886 Tau:0.6021 0.5033 0.5143\n",
      "Epoch: 81 Step: 7209 Index:-0.3480 R2:0.6471 0.4570 0.2710 RMSE:0.6894 0.8549 0.8983 Tau:0.6110 0.5069 0.5121\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 82 Step: 7298 Index:-0.3489 R2:0.6460 0.4556 0.2692 RMSE:0.7088 0.8617 0.9161 Tau:0.6130 0.5128 0.5147\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 83 Step: 7387 Index:-0.3978 R2:0.6528 0.4470 0.2693 RMSE:0.7086 0.8967 0.9162 Tau:0.6144 0.4989 0.5140\n",
      "Epoch: 84 Step: 7476 Index:-0.3376 R2:0.6434 0.4627 0.2608 RMSE:0.6891 0.8562 0.9126 Tau:0.6103 0.5187 0.5132\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 85 Step: 7565 Index:-0.4008 R2:0.6565 0.4646 0.2759 RMSE:0.7131 0.9070 0.9269 Tau:0.6139 0.5062 0.5120\n",
      "Epoch: 86 Step: 7654 Index:-0.3317 R2:0.6511 0.4668 0.2689 RMSE:0.7008 0.8532 0.9196 Tau:0.6168 0.5216 0.5134\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 87 Step: 7743 Index:-0.3426 R2:0.6408 0.4652 0.2577 RMSE:0.6887 0.8617 0.9215 Tau:0.6111 0.5190 0.5156\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 88 Step: 7832 Index:-0.3756 R2:0.6365 0.4543 0.2484 RMSE:0.6966 0.8788 0.9445 Tau:0.6110 0.5033 0.5136\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 89 Step: 7921 Index:-0.4414 R2:0.6542 0.4396 0.2597 RMSE:0.7128 0.9253 0.9386 Tau:0.6142 0.4838 0.5143\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 90 Step: 8010 Index:-0.3321 R2:0.6674 0.4668 0.2874 RMSE:0.6847 0.8485 0.8997 Tau:0.6244 0.5165 0.5167\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 91 Step: 8099 Index:-0.4013 R2:0.6685 0.4481 0.2675 RMSE:0.6914 0.9097 0.9309 Tau:0.6264 0.5084 0.5171\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 92 Step: 8188 Index:-0.3902 R2:0.6575 0.4484 0.2746 RMSE:0.6844 0.8825 0.9079 Tau:0.6146 0.4923 0.5102\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 93 Step: 8277 Index:-0.3560 R2:0.6723 0.4705 0.2802 RMSE:0.6753 0.8750 0.9079 Tau:0.6296 0.5190 0.5174\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 94 Step: 8366 Index:-0.3445 R2:0.6750 0.4608 0.2761 RMSE:0.6595 0.8599 0.9001 Tau:0.6319 0.5154 0.5175\n",
      "Epoch: 95 Step: 8455 Index:-0.3208 R2:0.6748 0.4670 0.2858 RMSE:0.6754 0.8406 0.8857 Tau:0.6327 0.5198 0.5180\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 96 Step: 8544 Index:-0.3288 R2:0.6573 0.4754 0.2802 RMSE:0.6985 0.8526 0.9267 Tau:0.6155 0.5238 0.5148\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 97 Step: 8633 Index:-0.3425 R2:0.6789 0.4723 0.2961 RMSE:0.6732 0.8634 0.8897 Tau:0.6337 0.5209 0.5184\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 98 Step: 8722 Index:-0.3576 R2:0.6810 0.4558 0.2820 RMSE:0.6633 0.8693 0.8952 Tau:0.6360 0.5117 0.5187\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 99 Step: 8811 Index:-0.3769 R2:0.6909 0.4600 0.2857 RMSE:0.6596 0.8827 0.9021 Tau:0.6399 0.5058 0.5164\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 100 Step: 8900 Index:-0.3706 R2:0.6309 0.4468 0.2626 RMSE:0.7209 0.8878 0.9127 Tau:0.6085 0.5172 0.5202\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 101 Step: 8989 Index:-0.3316 R2:0.6851 0.4629 0.2888 RMSE:0.6591 0.8484 0.8849 Tau:0.6426 0.5168 0.5178\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 102 Step: 9078 Index:-0.3665 R2:0.6688 0.4774 0.2563 RMSE:0.6699 0.8664 0.9299 Tau:0.6260 0.5000 0.5109\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 103 Step: 9167 Index:-0.3339 R2:0.6864 0.4943 0.2971 RMSE:0.6685 0.8562 0.8983 Tau:0.6402 0.5223 0.5196\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 104 Step: 9256 Index:-0.3237 R2:0.6924 0.4807 0.2990 RMSE:0.6776 0.8464 0.9026 Tau:0.6430 0.5227 0.5180\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 105 Step: 9345 Index:-0.3257 R2:0.6956 0.4738 0.2941 RMSE:0.6407 0.8465 0.8867 Tau:0.6471 0.5209 0.5181\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 106 Step: 9434 Index:-0.3263 R2:0.6942 0.4754 0.2910 RMSE:0.6431 0.8431 0.8875 Tau:0.6457 0.5168 0.5164\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 107 Step: 9523 Index:-0.3329 R2:0.6781 0.4855 0.2828 RMSE:0.6521 0.8585 0.9152 Tau:0.6328 0.5256 0.5171\n",
      "Epoch: 108 Step: 9612 Index:-0.2939 R2:0.6924 0.4904 0.3082 RMSE:0.6541 0.8294 0.8806 Tau:0.6436 0.5355 0.5210\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 109 Step: 9701 Index:-0.3151 R2:0.6709 0.4887 0.2857 RMSE:0.6609 0.8448 0.9001 Tau:0.6314 0.5297 0.5145\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 110 Step: 9790 Index:-0.3474 R2:0.6928 0.4618 0.2772 RMSE:0.6571 0.8661 0.9146 Tau:0.6461 0.5187 0.5184\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 111 Step: 9879 Index:-0.4017 R2:0.6929 0.4742 0.3099 RMSE:0.6762 0.9035 0.9067 Tau:0.6459 0.5018 0.5241\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 112 Step: 9968 Index:-0.3906 R2:0.6998 0.4758 0.2894 RMSE:0.6519 0.8923 0.9137 Tau:0.6490 0.5018 0.5192\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 113 Step: 10057 Index:-0.3862 R2:0.6971 0.4853 0.2882 RMSE:0.6772 0.9126 0.9406 Tau:0.6499 0.5264 0.5239\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 114 Step: 10146 Index:-0.3425 R2:0.7025 0.4688 0.2914 RMSE:0.6375 0.8530 0.8883 Tau:0.6446 0.5106 0.5130\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 115 Step: 10235 Index:-0.4044 R2:0.7072 0.4676 0.2825 RMSE:0.6602 0.9150 0.9388 Tau:0.6556 0.5106 0.5175\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 116 Step: 10324 Index:-0.3693 R2:0.6860 0.4472 0.3011 RMSE:0.6614 0.8634 0.8769 Tau:0.6353 0.4941 0.5092\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 117 Step: 10413 Index:-0.3197 R2:0.7193 0.4810 0.3127 RMSE:0.6231 0.8384 0.8755 Tau:0.6605 0.5187 0.5200\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 118 Step: 10502 Index:-0.3443 R2:0.7134 0.4989 0.3049 RMSE:0.6330 0.8787 0.9071 Tau:0.6590 0.5344 0.5210\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 119 Step: 10591 Index:-0.3618 R2:0.7047 0.4731 0.3120 RMSE:0.6325 0.8654 0.8826 Tau:0.6531 0.5036 0.5156\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 120 Step: 10680 Index:-0.3445 R2:0.7248 0.4739 0.3073 RMSE:0.6128 0.8495 0.8774 Tau:0.6674 0.5051 0.5226\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 121 Step: 10769 Index:-0.4896 R2:0.7250 0.4690 0.2990 RMSE:0.7365 0.9906 0.9875 Tau:0.6671 0.5011 0.5242\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 122 Step: 10858 Index:-0.3873 R2:0.7204 0.4613 0.2997 RMSE:0.6388 0.8971 0.9062 Tau:0.6625 0.5099 0.5247\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 123 Step: 10947 Index:-0.3303 R2:0.7315 0.4886 0.3036 RMSE:0.6004 0.8551 0.8918 Tau:0.6723 0.5249 0.5227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 124 Step: 11036 Index:-0.3568 R2:0.7279 0.4726 0.3081 RMSE:0.6166 0.8615 0.8838 Tau:0.6674 0.5047 0.5211\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 125 Step: 11125 Index:-0.3748 R2:0.7334 0.4684 0.3007 RMSE:0.6093 0.8791 0.8951 Tau:0.6721 0.5044 0.5228\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 126 Step: 11214 Index:-0.3267 R2:0.7340 0.4739 0.3207 RMSE:0.6131 0.8439 0.8699 Tau:0.6724 0.5172 0.5259\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 127 Step: 11303 Index:-0.3822 R2:0.7307 0.4623 0.3157 RMSE:0.6075 0.8770 0.8826 Tau:0.6664 0.4948 0.5267\n",
      "Epoch: 128 Step: 11392 Index:-0.2921 R2:0.7127 0.4864 0.3090 RMSE:0.6381 0.8272 0.8705 Tau:0.6605 0.5352 0.5240\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 129 Step: 11481 Index:-0.3801 R2:0.7232 0.4909 0.3170 RMSE:0.6289 0.8947 0.9124 Tau:0.6673 0.5146 0.5273\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 130 Step: 11570 Index:-0.3797 R2:0.7384 0.4732 0.3227 RMSE:0.5939 0.8889 0.8903 Tau:0.6743 0.5091 0.5257\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 131 Step: 11659 Index:-0.3356 R2:0.7429 0.4934 0.3210 RMSE:0.5965 0.8642 0.8905 Tau:0.6787 0.5286 0.5250\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 132 Step: 11748 Index:-0.3540 R2:0.7298 0.4681 0.3154 RMSE:0.6217 0.8601 0.8851 Tau:0.6725 0.5062 0.5256\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 133 Step: 11837 Index:-0.3305 R2:0.7484 0.5013 0.3191 RMSE:0.6011 0.8668 0.8979 Tau:0.6859 0.5363 0.5283\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 134 Step: 11926 Index:-0.3285 R2:0.7430 0.4919 0.3223 RMSE:0.5900 0.8420 0.8741 Tau:0.6790 0.5135 0.5216\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 135 Step: 12015 Index:-0.3003 R2:0.7423 0.4984 0.3097 RMSE:0.5930 0.8307 0.8807 Tau:0.6748 0.5304 0.5236\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 136 Step: 12104 Index:-0.3623 R2:0.7470 0.4813 0.3173 RMSE:0.6067 0.8776 0.8944 Tau:0.6806 0.5154 0.5244\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 137 Step: 12193 Index:-0.3680 R2:0.7396 0.4788 0.3024 RMSE:0.6074 0.8936 0.9195 Tau:0.6806 0.5256 0.5266\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 138 Step: 12282 Index:-0.3825 R2:0.7513 0.4812 0.3158 RMSE:0.5982 0.8971 0.9061 Tau:0.6882 0.5146 0.5285\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 139 Step: 12371 Index:-0.3397 R2:0.7406 0.4881 0.3109 RMSE:0.6077 0.8535 0.8882 Tau:0.6750 0.5139 0.5256\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 140 Step: 12460 Index:-0.3043 R2:0.7593 0.4943 0.3207 RMSE:0.5715 0.8372 0.8770 Tau:0.6885 0.5330 0.5247\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 141 Step: 12549 Index:-0.5681 R2:0.6747 0.4436 0.3162 RMSE:0.7388 1.0380 1.0018 Tau:0.6307 0.4699 0.5222\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 142 Step: 12638 Index:-0.3583 R2:0.7413 0.4771 0.3278 RMSE:0.5907 0.8616 0.8772 Tau:0.6769 0.5033 0.5264\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 143 Step: 12727 Index:-0.3808 R2:0.7421 0.4815 0.2982 RMSE:0.6004 0.9031 0.9266 Tau:0.6788 0.5223 0.5313\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 144 Step: 12816 Index:-0.4146 R2:0.7526 0.4851 0.3152 RMSE:0.6173 0.9438 0.9412 Tau:0.6860 0.5293 0.5334\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 145 Step: 12905 Index:-0.3996 R2:0.6761 0.4366 0.2975 RMSE:0.6911 0.8632 0.8763 Tau:0.6313 0.4637 0.5191\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 146 Step: 12994 Index:-0.3626 R2:0.7503 0.4730 0.3111 RMSE:0.5910 0.8626 0.8981 Tau:0.6832 0.5000 0.5266\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 147 Step: 13083 Index:-0.3526 R2:0.7689 0.4859 0.3127 RMSE:0.5668 0.8672 0.8953 Tau:0.6968 0.5146 0.5300\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 148 Step: 13172 Index:-0.3677 R2:0.7600 0.4594 0.3018 RMSE:0.5816 0.8702 0.9005 Tau:0.6905 0.5025 0.5266\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 149 Step: 13261 Index:-0.3360 R2:0.7670 0.4796 0.3033 RMSE:0.5653 0.8561 0.9036 Tau:0.6971 0.5201 0.5295\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 150 Step: 13350 Index:-0.3617 R2:0.7691 0.4716 0.3017 RMSE:0.5599 0.8807 0.9121 Tau:0.6990 0.5190 0.5303\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 151 Step: 13439 Index:-0.5007 R2:0.7486 0.4595 0.3060 RMSE:0.6915 1.0010 0.9944 Tau:0.6807 0.5003 0.5260\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 152 Step: 13528 Index:-0.3483 R2:0.7427 0.4747 0.3014 RMSE:0.5936 0.8593 0.9014 Tau:0.6778 0.5110 0.5298\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 153 Step: 13617 Index:-0.3589 R2:0.7629 0.4911 0.3106 RMSE:0.5708 0.8911 0.9255 Tau:0.6920 0.5322 0.5303\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 154 Step: 13706 Index:-0.4387 R2:0.7602 0.4774 0.3178 RMSE:0.6497 0.9489 0.9590 Tau:0.6906 0.5102 0.5278\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 155 Step: 13795 Index:-0.3684 R2:0.7347 0.4568 0.3083 RMSE:0.5969 0.8772 0.8954 Tau:0.6732 0.5088 0.5273\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 156 Step: 13884 Index:-0.3797 R2:0.7689 0.4705 0.2992 RMSE:0.5634 0.8943 0.9213 Tau:0.6956 0.5146 0.5300\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 157 Step: 13973 Index:-0.3794 R2:0.7361 0.4731 0.2925 RMSE:0.6187 0.8790 0.9383 Tau:0.6717 0.4996 0.5219\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 158 Step: 14062 Index:-0.3626 R2:0.7754 0.4692 0.3139 RMSE:0.5575 0.8662 0.8948 Tau:0.6957 0.5036 0.5269\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 158 r2:0.3090 RMSE:0.8705 WTI:0.4028 AP:0.7401 Tau:0.5240 \n",
      " \n",
      " Top-1:0.2500 Top-1-fp:0.0000 \n",
      " Top-5:0.4773 Top-5-fp:0.1818 \n",
      " Top-10:0.6629 Top-10-fp:0.1685 \n",
      " Top-15:0.6767 Top-15-fp:0.1880 \n",
      " Top-20:0.7416 Top-20-fp:0.2135 \n",
      " Top-25:0.8300 Top-25-fp:0.2556 \n",
      " Top-30:0.9000 Top-30-fp:0.3258 \n",
      " Top-40:0.9750 Top-40-fp:0.4538 \n",
      " Top-50:0.9900 Top-50-fp:0.5561 \n",
      " \n",
      " Top50:0.5400 Top50-fp:0.1600 \n",
      " Top100:0.6000 Top100-fp:0.1900 \n",
      " Top150:0.7267 Top150-fp:0.2000 \n",
      " Top200:0.7600 Top200-fp:0.2400 \n",
      " Top250:0.8850 Top250-fp:0.2920 \n",
      " Top300:0.9300 Top300-fp:0.3800 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
