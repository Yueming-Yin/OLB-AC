{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC50_P21453_0.5_210\n",
      "model_file/0_GAFSE_EC50_P21453_0.5_210_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/EC50_P21453_0.5_210_train.csv\"\n",
    "test_filename = \"./data/benchmark/EC50_P21453_0.5_210_test.csv\"\n",
    "test_active = 210\n",
    "val_rate = 0.1\n",
    "random_seed = 0\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CCCOC1=C(C=C(C=C1)C2=NC(=NO2)C3=CC4=C(C=C3)N(C... -0.462398\n",
      "1  CC1=CC=C(C=C1)CCCC(=O)C2=CC=C(C=C2)COCC(C)(COP... -0.845098\n",
      "2  CCOC1=C(C=C(C=N1)C2=NN=C(S2)C3=CC(=C(C=C3Cl)OC... -2.773786\n",
      "3  CC(=O)OC1=CC=C(C=C1)N2C(=C3C(=C2O)C4(C=CC3(O4)... -3.509203\n",
      "4  CCCCCCN1CCC2=C(C1=O)C=CC(=C2)C3CCC(C3)(COP(=O)... -2.740363\n",
      "number of all smiles:  938\n",
      "number of successfully processed smiles:  938\n",
      "                                              smiles     value  \\\n",
      "0  CCCOC1=C(C=C(C=C1)C2=NC(=NO2)C3=CC4=C(C=C3)N(C... -0.462398   \n",
      "1  CC1=CC=C(C=C1)CCCC(=O)C2=CC=C(C=C2)COCC(C)(COP... -0.845098   \n",
      "2  CCOC1=C(C=C(C=N1)C2=NN=C(S2)C3=CC(=C(C=C3Cl)OC... -2.773786   \n",
      "3  CC(=O)OC1=CC=C(C=C1)N2C(=C3C(=C2O)C4(C=CC3(O4)... -3.509203   \n",
      "4  CCCCCCN1CCC2=C(C1=O)C=CC(=C2)C3CCC(C3)(COP(=O)... -2.740363   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CCCOc1ccc(-c2nc(-c3ccc4c(c3)CCN4CC(N)(CO)CO)no...  \n",
      "1  Cc1ccc(CCCC(=O)c2ccc(COCC(C)(N)COP(=O)(O)O)cc2...  \n",
      "2  CCOc1ncc(-c2nnc(-c3cc(F)c(OCC(N)CO)cc3Cl)s2)cc1Cl  \n",
      "3    CC(=O)Oc1ccc(-n2c(O)c3c(c2O)C2(C)C=CC3(C)O2)cc1  \n",
      "4    CCCCCCN1CCc2cc(C3CCC(N)(COP(=O)(O)O)C3)ccc2C1=O  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  871\n",
      "number of successfully processed smiles:  871\n",
      "(871, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CC1(C2=CC=CC=C2)C3=NC4=C(C=C3)N=C(S4)C5=C(C=... -1.623249   \n",
      "1      CC1=CC(=C(C=C1)C(C)C)OCC2=CC=C(C=C2)C3=NN=CO3 -3.316495   \n",
      "2         CC1=C(N2C=CSC2=N1)C3=CSC(=N3)NCC4=CC=CC=C4 -3.503382   \n",
      "3  CC1=NC(=CO1)C2=CC=C(C=C2)OC3=CC=C(C=C3)CCC(CO)... -1.611723   \n",
      "4  CC1=CC(=CC(=C1OCC(CN)O)C)CCC(=O)C2=C3CCC(CC3=C... -0.845098   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  O=C(O)CCNc1ccc(-c2nc3ccc(C4(c5ccccc5)CC4)nc3s2...  \n",
      "1             Cc1ccc(C(C)C)c(OCc2ccc(-c3nnco3)cc2)c1  \n",
      "2                  Cc1nc2sccn2c1-c1csc(NCc2ccccc2)n1  \n",
      "3  Cc1nc(-c2ccc(Oc3ccc(CCC(N)(CO)COP(=O)(O)O)cc3)...  \n",
      "4  Cc1cc(CCC(=O)c2sc(C)c3c2CCC(C)(C)C3)cc(C)c1OCC...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/EC50_P21453_0.5_210_train.pickle\n",
      "./data/benchmark/EC50_P21453_0.5_210_train\n",
      "1809\n",
      "feature dicts file saved as ./data/benchmark/EC50_P21453_0.5_210_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(844, 3) (94, 3) (871, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_EC50_P21453_0.5_210_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3037830/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 87 Index:0.1722 R2:0.2340 0.1722 0.1113 RMSE:1.5925 1.5396 1.4955 Tau:-0.3226 -0.2976 -0.3091\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 2 Step: 174 Index:0.0016 R2:0.0001 0.0016 0.0089 RMSE:1.5574 1.5153 1.4447 Tau:0.0092 -0.0332 -0.0990\n",
      "Epoch: 3 Step: 261 Index:0.2588 R2:0.4101 0.2588 0.3157 RMSE:1.3241 1.3298 1.3077 Tau:0.4630 0.3667 0.3449\n",
      "Epoch: 4 Step: 348 Index:0.2802 R2:0.4409 0.2802 0.3379 RMSE:1.2503 1.3144 1.2101 Tau:0.4824 0.3851 0.3629\n",
      "Epoch: 5 Step: 435 Index:0.2997 R2:0.4618 0.2997 0.3506 RMSE:1.1682 1.2594 1.2525 Tau:0.4978 0.3943 0.3720\n",
      "Epoch: 6 Step: 522 Index:0.3078 R2:0.4744 0.3078 0.3604 RMSE:1.1199 1.2433 1.2101 Tau:0.5054 0.4021 0.3790\n",
      "Epoch: 7 Step: 609 Index:0.3080 R2:0.4778 0.3080 0.3557 RMSE:1.1437 1.2736 1.2624 Tau:0.5079 0.3989 0.3817\n",
      "Epoch: 8 Step: 696 Index:0.3243 R2:0.4966 0.3243 0.3732 RMSE:1.1089 1.2413 1.1357 Tau:0.5191 0.4109 0.3846\n",
      "Epoch: 9 Step: 783 Index:0.3363 R2:0.5064 0.3363 0.3880 RMSE:1.0661 1.2036 1.1336 Tau:0.5260 0.4261 0.3912\n",
      "Epoch: 10 Step: 870 Index:0.3459 R2:0.5158 0.3459 0.3995 RMSE:1.0569 1.1932 1.1309 Tau:0.5322 0.4325 0.3927\n",
      "Epoch: 11 Step: 957 Index:0.3549 R2:0.5239 0.3549 0.4106 RMSE:1.0501 1.1829 1.1229 Tau:0.5380 0.4385 0.3957\n",
      "Epoch: 12 Step: 1044 Index:0.3634 R2:0.5334 0.3634 0.4238 RMSE:1.1519 1.2831 1.1324 Tau:0.5413 0.4496 0.3995\n",
      "Epoch: 13 Step: 1131 Index:0.3775 R2:0.5424 0.3775 0.4336 RMSE:1.0946 1.2282 1.0963 Tau:0.5476 0.4523 0.3998\n",
      "Epoch: 14 Step: 1218 Index:0.3793 R2:0.5419 0.3793 0.4319 RMSE:1.0314 1.1652 1.1175 Tau:0.5482 0.4510 0.4006\n",
      "Epoch: 15 Step: 1305 Index:0.3895 R2:0.5490 0.3895 0.4466 RMSE:1.0296 1.1687 1.0653 Tau:0.5521 0.4546 0.4024\n",
      "Epoch: 16 Step: 1392 Index:0.4011 R2:0.5551 0.4011 0.4586 RMSE:1.1502 1.2609 1.1190 Tau:0.5551 0.4542 0.4034\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 17 Step: 1479 Index:0.3989 R2:0.5538 0.3989 0.4524 RMSE:1.0149 1.1466 1.0654 Tau:0.5557 0.4546 0.4071\n",
      "Epoch: 18 Step: 1566 Index:0.4024 R2:0.5547 0.4024 0.4518 RMSE:1.0308 1.1580 1.1307 Tau:0.5564 0.4528 0.4076\n",
      "Epoch: 19 Step: 1653 Index:0.4045 R2:0.5567 0.4045 0.4509 RMSE:1.0285 1.1573 1.1309 Tau:0.5563 0.4533 0.4061\n",
      "Epoch: 20 Step: 1740 Index:0.4077 R2:0.5592 0.4077 0.4564 RMSE:1.0376 1.1707 1.1471 Tau:0.5583 0.4588 0.4066\n",
      "Epoch: 21 Step: 1827 Index:0.4154 R2:0.5618 0.4154 0.4656 RMSE:1.0041 1.1251 1.0607 Tau:0.5595 0.4606 0.4080\n",
      "Epoch: 22 Step: 1914 Index:0.4184 R2:0.5639 0.4184 0.4644 RMSE:1.0157 1.1491 1.1116 Tau:0.5608 0.4602 0.4092\n",
      "Epoch: 23 Step: 2001 Index:0.4241 R2:0.5664 0.4241 0.4659 RMSE:1.0331 1.1472 1.1400 Tau:0.5622 0.4629 0.4083\n",
      "Epoch: 24 Step: 2088 Index:0.4319 R2:0.5690 0.4319 0.4724 RMSE:0.9995 1.1137 1.0450 Tau:0.5637 0.4666 0.4098\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 25 Step: 2175 Index:0.4298 R2:0.5701 0.4298 0.4724 RMSE:0.9916 1.1130 1.0588 Tau:0.5646 0.4634 0.4144\n",
      "Epoch: 26 Step: 2262 Index:0.4348 R2:0.5731 0.4348 0.4763 RMSE:0.9959 1.1146 1.0380 Tau:0.5658 0.4717 0.4127\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 27 Step: 2349 Index:0.4322 R2:0.5721 0.4322 0.4750 RMSE:0.9900 1.1145 1.0441 Tau:0.5643 0.4657 0.4145\n",
      "Epoch: 28 Step: 2436 Index:0.4421 R2:0.5744 0.4421 0.4785 RMSE:1.0005 1.0966 1.0670 Tau:0.5677 0.4804 0.4104\n",
      "Epoch: 29 Step: 2523 Index:0.4485 R2:0.5762 0.4485 0.4851 RMSE:0.9888 1.0926 1.0361 Tau:0.5667 0.4809 0.4079\n",
      "Epoch: 30 Step: 2610 Index:0.4488 R2:0.5799 0.4488 0.4847 RMSE:0.9844 1.1020 1.0624 Tau:0.5703 0.4804 0.4114\n",
      "Epoch: 31 Step: 2697 Index:0.4489 R2:0.5812 0.4489 0.4842 RMSE:0.9997 1.1140 1.0957 Tau:0.5714 0.4818 0.4112\n",
      "Epoch: 32 Step: 2784 Index:0.4595 R2:0.5838 0.4595 0.4862 RMSE:0.9772 1.0797 1.0412 Tau:0.5731 0.4809 0.4127\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 2871 Index:0.4551 R2:0.5842 0.4551 0.4868 RMSE:1.0336 1.1400 1.1487 Tau:0.5741 0.4800 0.4147\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 34 Step: 2958 Index:0.4538 R2:0.5837 0.4538 0.4879 RMSE:0.9837 1.1028 1.0686 Tau:0.5721 0.4804 0.4148\n",
      "Epoch: 35 Step: 3045 Index:0.4607 R2:0.5898 0.4607 0.4939 RMSE:0.9687 1.0818 1.0372 Tau:0.5765 0.4869 0.4144\n",
      "Epoch: 36 Step: 3132 Index:0.4716 R2:0.5925 0.4716 0.4981 RMSE:0.9668 1.0721 1.0190 Tau:0.5776 0.4919 0.4144\n",
      "Epoch: 37 Step: 3219 Index:0.4754 R2:0.5945 0.4754 0.5006 RMSE:0.9647 1.0655 1.0191 Tau:0.5794 0.4966 0.4151\n",
      "Epoch: 38 Step: 3306 Index:0.4766 R2:0.5938 0.4766 0.4999 RMSE:0.9670 1.0676 1.0438 Tau:0.5803 0.4887 0.4180\n",
      "Epoch: 39 Step: 3393 Index:0.4823 R2:0.5977 0.4823 0.5057 RMSE:0.9841 1.0826 1.0090 Tau:0.5812 0.4952 0.4147\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 40 Step: 3480 Index:0.4620 R2:0.5935 0.4620 0.5008 RMSE:0.9880 1.0852 1.0211 Tau:0.5785 0.4938 0.4164\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 41 Step: 3567 Index:0.4651 R2:0.5966 0.4651 0.4977 RMSE:0.9688 1.0843 1.0154 Tau:0.5802 0.4850 0.4180\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 42 Step: 3654 Index:0.4745 R2:0.5988 0.4745 0.4992 RMSE:0.9587 1.0663 1.0338 Tau:0.5825 0.4933 0.4200\n",
      "Epoch: 43 Step: 3741 Index:0.4838 R2:0.6023 0.4838 0.5040 RMSE:0.9716 1.0743 1.0086 Tau:0.5847 0.4993 0.4191\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 44 Step: 3828 Index:0.4799 R2:0.6017 0.4799 0.4975 RMSE:0.9551 1.0604 1.0245 Tau:0.5833 0.5012 0.4183\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 45 Step: 3915 Index:0.4834 R2:0.6053 0.4834 0.5033 RMSE:0.9503 1.0621 1.0152 Tau:0.5859 0.4984 0.4188\n",
      "Epoch: 46 Step: 4002 Index:0.4879 R2:0.6055 0.4879 0.5116 RMSE:1.0297 1.1236 1.0290 Tau:0.5869 0.4933 0.4175\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 47 Step: 4089 Index:0.4801 R2:0.6058 0.4801 0.5041 RMSE:0.9637 1.0786 1.0088 Tau:0.5874 0.4892 0.4199\n",
      "Epoch: 48 Step: 4176 Index:0.4892 R2:0.6067 0.4892 0.5038 RMSE:1.0004 1.0917 1.0207 Tau:0.5883 0.4855 0.4218\n",
      "Epoch: 49 Step: 4263 Index:0.4937 R2:0.6133 0.4937 0.5137 RMSE:0.9432 1.0478 1.0023 Tau:0.5926 0.5021 0.4205\n",
      "Epoch: 50 Step: 4350 Index:0.4982 R2:0.6143 0.4982 0.5190 RMSE:0.9618 1.0585 0.9950 Tau:0.5901 0.5076 0.4149\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 51 Step: 4437 Index:0.4855 R2:0.6139 0.4855 0.5110 RMSE:0.9518 1.0641 1.0012 Tau:0.5924 0.4998 0.4216\n",
      "Epoch: 52 Step: 4524 Index:0.5002 R2:0.6181 0.5002 0.5168 RMSE:0.9522 1.0548 0.9958 Tau:0.5941 0.5035 0.4183\n",
      "Epoch: 53 Step: 4611 Index:0.5019 R2:0.6205 0.5019 0.5162 RMSE:0.9466 1.0484 0.9964 Tau:0.5946 0.5081 0.4168\n",
      "Epoch: 54 Step: 4698 Index:0.5059 R2:0.6229 0.5059 0.5219 RMSE:0.9505 1.0506 0.9917 Tau:0.5966 0.5122 0.4216\n",
      "Epoch: 55 Step: 4785 Index:0.5100 R2:0.6228 0.5100 0.5252 RMSE:0.9349 1.0300 0.9890 Tau:0.5967 0.5118 0.4190\n",
      "Epoch: 56 Step: 4872 Index:0.5188 R2:0.6259 0.5188 0.5269 RMSE:0.9737 1.0604 0.9986 Tau:0.5989 0.5200 0.4209\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 57 Step: 4959 Index:0.5117 R2:0.6257 0.5117 0.5256 RMSE:0.9280 1.0338 1.0120 Tau:0.5979 0.5131 0.4156\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 58 Step: 5046 Index:0.5162 R2:0.6280 0.5162 0.5291 RMSE:0.9244 1.0295 0.9856 Tau:0.5985 0.5205 0.4162\n",
      "Epoch: 59 Step: 5133 Index:0.5288 R2:0.6235 0.5288 0.5245 RMSE:1.0731 1.1257 1.0660 Tau:0.5985 0.5214 0.4182\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 60 Step: 5220 Index:0.5223 R2:0.6286 0.5223 0.5308 RMSE:0.9492 1.0386 0.9857 Tau:0.5998 0.5200 0.4178\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 61 Step: 5307 Index:0.5163 R2:0.6310 0.5163 0.5273 RMSE:0.9180 1.0231 0.9982 Tau:0.6033 0.5177 0.4238\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 62 Step: 5394 Index:0.5209 R2:0.6347 0.5209 0.5310 RMSE:0.9132 1.0185 0.9910 Tau:0.6046 0.5210 0.4216\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 63 Step: 5481 Index:0.5171 R2:0.6350 0.5171 0.5287 RMSE:0.9131 1.0235 0.9899 Tau:0.6052 0.5150 0.4231\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 64 Step: 5568 Index:0.5252 R2:0.6337 0.5252 0.5333 RMSE:0.9474 1.0415 0.9860 Tau:0.6010 0.5196 0.4126\n",
      "Epoch: 65 Step: 5655 Index:0.5464 R2:0.6339 0.5464 0.5327 RMSE:0.9489 1.0089 0.9910 Tau:0.6005 0.5265 0.4144\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 66 Step: 5742 Index:0.5349 R2:0.6431 0.5349 0.5422 RMSE:0.9191 1.0099 0.9706 Tau:0.6119 0.5228 0.4258\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 67 Step: 5829 Index:0.5308 R2:0.6444 0.5308 0.5421 RMSE:0.9061 1.0048 0.9792 Tau:0.6108 0.5159 0.4227\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 68 Step: 5916 Index:0.5251 R2:0.6455 0.5251 0.5409 RMSE:0.9059 1.0203 0.9988 Tau:0.6116 0.5210 0.4246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 69 Step: 6003 Index:0.5368 R2:0.6495 0.5368 0.5447 RMSE:0.8963 0.9996 0.9771 Tau:0.6130 0.5247 0.4225\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 70 Step: 6090 Index:0.5164 R2:0.6423 0.5164 0.5410 RMSE:0.9397 1.0514 0.9777 Tau:0.6055 0.5219 0.4218\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 71 Step: 6177 Index:0.5273 R2:0.6400 0.5273 0.5318 RMSE:0.9073 1.0170 0.9945 Tau:0.6079 0.5177 0.4265\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 72 Step: 6264 Index:0.5355 R2:0.6483 0.5355 0.5460 RMSE:1.0329 1.1135 1.0369 Tau:0.6152 0.5224 0.4286\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 73 Step: 6351 Index:0.5249 R2:0.6512 0.5249 0.5391 RMSE:0.9643 1.0918 1.0938 Tau:0.6143 0.5168 0.4231\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 74 Step: 6438 Index:0.5333 R2:0.6550 0.5333 0.5463 RMSE:0.8875 1.0055 0.9730 Tau:0.6188 0.5210 0.4278\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 75 Step: 6525 Index:0.5298 R2:0.6578 0.5298 0.5466 RMSE:0.9124 1.0457 1.0262 Tau:0.6171 0.5237 0.4218\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 76 Step: 6612 Index:0.5349 R2:0.6617 0.5349 0.5547 RMSE:0.8791 1.0101 0.9658 Tau:0.6200 0.5228 0.4226\n",
      "Epoch: 77 Step: 6699 Index:0.5521 R2:0.6670 0.5521 0.5564 RMSE:0.9484 1.0434 0.9890 Tau:0.6269 0.5366 0.4270\n",
      "Epoch: 78 Step: 6786 Index:0.5535 R2:0.6703 0.5535 0.5572 RMSE:0.8724 0.9807 0.9633 Tau:0.6295 0.5366 0.4293\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 79 Step: 6873 Index:0.5400 R2:0.6662 0.5400 0.5595 RMSE:0.8739 1.0047 0.9566 Tau:0.6243 0.5279 0.4233\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 80 Step: 6960 Index:0.5442 R2:0.6707 0.5442 0.5545 RMSE:0.8979 1.0144 0.9629 Tau:0.6304 0.5348 0.4296\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 81 Step: 7047 Index:0.5461 R2:0.6736 0.5461 0.5635 RMSE:0.8842 1.0016 0.9489 Tau:0.6298 0.5362 0.4278\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 82 Step: 7134 Index:0.5489 R2:0.6783 0.5489 0.5622 RMSE:0.8572 0.9903 0.9555 Tau:0.6329 0.5417 0.4264\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 83 Step: 7221 Index:0.5451 R2:0.6793 0.5451 0.5559 RMSE:0.8591 0.9996 0.9741 Tau:0.6345 0.5399 0.4304\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 84 Step: 7308 Index:0.5516 R2:0.6712 0.5516 0.5455 RMSE:0.9117 1.0260 1.0378 Tau:0.6254 0.5316 0.4204\n",
      "Epoch: 85 Step: 7395 Index:0.5546 R2:0.6813 0.5546 0.5627 RMSE:0.8744 0.9972 0.9494 Tau:0.6365 0.5422 0.4339\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 86 Step: 7482 Index:0.5536 R2:0.6850 0.5536 0.5647 RMSE:0.8509 0.9947 0.9614 Tau:0.6388 0.5491 0.4307\n",
      "Epoch: 87 Step: 7569 Index:0.5595 R2:0.6861 0.5595 0.5737 RMSE:0.8471 0.9840 0.9410 Tau:0.6359 0.5454 0.4223\n",
      "Epoch: 88 Step: 7656 Index:0.5606 R2:0.6907 0.5606 0.5668 RMSE:0.8809 1.0173 1.0065 Tau:0.6396 0.5518 0.4215\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 89 Step: 7743 Index:0.5375 R2:0.6856 0.5375 0.5565 RMSE:0.8471 1.0070 0.9628 Tau:0.6402 0.5348 0.4355\n",
      "Epoch: 90 Step: 7830 Index:0.5774 R2:0.6982 0.5774 0.5778 RMSE:0.8613 0.9711 0.9371 Tau:0.6491 0.5601 0.4326\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 91 Step: 7917 Index:0.5606 R2:0.6958 0.5606 0.5703 RMSE:0.8926 1.0283 1.0313 Tau:0.6471 0.5491 0.4321\n",
      "Epoch: 92 Step: 8004 Index:0.5806 R2:0.7003 0.5806 0.5733 RMSE:0.8411 0.9722 0.9665 Tau:0.6503 0.5647 0.4277\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 93 Step: 8091 Index:0.5702 R2:0.7009 0.5702 0.5782 RMSE:0.8286 0.9727 0.9459 Tau:0.6490 0.5500 0.4321\n",
      "Epoch: 94 Step: 8178 Index:0.5823 R2:0.7020 0.5823 0.5726 RMSE:0.8258 0.9547 0.9455 Tau:0.6490 0.5606 0.4235\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 95 Step: 8265 Index:0.5662 R2:0.7049 0.5662 0.5839 RMSE:0.8261 0.9810 0.9295 Tau:0.6503 0.5541 0.4258\n",
      "Epoch: 96 Step: 8352 Index:0.5901 R2:0.7090 0.5901 0.5760 RMSE:0.8197 0.9402 0.9325 Tau:0.6568 0.5703 0.4263\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 97 Step: 8439 Index:0.5740 R2:0.7075 0.5740 0.5821 RMSE:0.8179 0.9681 0.9326 Tau:0.6532 0.5578 0.4265\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 98 Step: 8526 Index:0.5856 R2:0.7046 0.5856 0.5837 RMSE:0.9042 1.0009 0.9745 Tau:0.6507 0.5564 0.4172\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 99 Step: 8613 Index:0.5666 R2:0.7026 0.5666 0.5724 RMSE:0.8464 0.9881 0.9471 Tau:0.6529 0.5509 0.4293\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 100 Step: 8700 Index:0.5609 R2:0.7042 0.5609 0.5793 RMSE:0.8318 0.9988 0.9459 Tau:0.6526 0.5546 0.4296\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 101 Step: 8787 Index:0.5751 R2:0.7123 0.5751 0.5830 RMSE:0.8131 0.9624 0.9352 Tau:0.6568 0.5624 0.4253\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 102 Step: 8874 Index:0.5744 R2:0.7149 0.5744 0.5804 RMSE:0.8100 0.9718 0.9466 Tau:0.6604 0.5638 0.4317\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 103 Step: 8961 Index:0.5804 R2:0.7196 0.5804 0.5840 RMSE:0.8093 0.9647 0.9478 Tau:0.6636 0.5647 0.4301\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 104 Step: 9048 Index:0.5857 R2:0.7185 0.5857 0.5776 RMSE:0.8093 0.9462 0.9305 Tau:0.6653 0.5716 0.4287\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 105 Step: 9135 Index:0.5797 R2:0.7210 0.5797 0.5773 RMSE:0.8146 0.9748 0.9635 Tau:0.6647 0.5693 0.4266\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 106 Step: 9222 Index:0.5886 R2:0.7254 0.5886 0.5803 RMSE:0.7988 0.9528 0.9471 Tau:0.6668 0.5781 0.4258\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 107 Step: 9309 Index:0.5789 R2:0.7090 0.5789 0.5745 RMSE:0.8678 0.9854 0.9534 Tau:0.6594 0.5629 0.4361\n",
      "Epoch: 108 Step: 9396 Index:0.5968 R2:0.7295 0.5968 0.5840 RMSE:0.8019 0.9406 0.9283 Tau:0.6691 0.5726 0.4258\n",
      "Epoch: 109 Step: 9483 Index:0.6059 R2:0.7244 0.6059 0.5798 RMSE:0.8048 0.9219 0.9283 Tau:0.6646 0.5698 0.4143\n",
      "Epoch: 110 Step: 9570 Index:0.6081 R2:0.7319 0.6081 0.5813 RMSE:0.8083 0.9254 0.9295 Tau:0.6727 0.5744 0.4271\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 111 Step: 9657 Index:0.5920 R2:0.7329 0.5920 0.5787 RMSE:0.7808 0.9437 0.9370 Tau:0.6721 0.5809 0.4294\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 112 Step: 9744 Index:0.5942 R2:0.7231 0.5942 0.5757 RMSE:0.8168 0.9562 0.9555 Tau:0.6622 0.5753 0.4097\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 113 Step: 9831 Index:0.6032 R2:0.7349 0.6032 0.5861 RMSE:0.7781 0.9347 0.9316 Tau:0.6727 0.5799 0.4227\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 114 Step: 9918 Index:0.5979 R2:0.7255 0.5979 0.5831 RMSE:0.8390 0.9860 0.9846 Tau:0.6644 0.5762 0.4178\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 115 Step: 10005 Index:0.6012 R2:0.7404 0.6012 0.5900 RMSE:0.7735 0.9341 0.9305 Tau:0.6776 0.5785 0.4281\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 116 Step: 10092 Index:0.6024 R2:0.7253 0.6024 0.5851 RMSE:0.9343 1.0302 1.0213 Tau:0.6703 0.5781 0.4249\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 117 Step: 10179 Index:0.6080 R2:0.7374 0.6080 0.5798 RMSE:0.7869 0.9217 0.9287 Tau:0.6785 0.5795 0.4296\n",
      "Epoch: 118 Step: 10266 Index:0.6127 R2:0.7395 0.6127 0.5807 RMSE:0.8150 0.9331 0.9414 Tau:0.6796 0.5795 0.4276\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 119 Step: 10353 Index:0.6011 R2:0.7279 0.6011 0.5726 RMSE:0.7943 0.9268 0.9378 Tau:0.6711 0.5841 0.4351\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 120 Step: 10440 Index:0.6115 R2:0.7425 0.6115 0.5846 RMSE:0.8230 0.9475 0.9538 Tau:0.6795 0.5868 0.4242\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 121 Step: 10527 Index:0.6061 R2:0.7366 0.6061 0.5800 RMSE:0.8091 0.9375 0.9429 Tau:0.6736 0.5795 0.4220\n",
      "Epoch: 122 Step: 10614 Index:0.6186 R2:0.7472 0.6186 0.5880 RMSE:0.7979 0.9202 0.9264 Tau:0.6850 0.5919 0.4296\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 123 Step: 10701 Index:0.6171 R2:0.7493 0.6171 0.5899 RMSE:0.7694 0.9256 0.9408 Tau:0.6847 0.5910 0.4307\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 124 Step: 10788 Index:0.6120 R2:0.7380 0.6120 0.5736 RMSE:0.7819 0.9138 0.9371 Tau:0.6797 0.5905 0.4335\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 125 Step: 10875 Index:0.6099 R2:0.7506 0.6099 0.5884 RMSE:0.7554 0.9197 0.9253 Tau:0.6857 0.5882 0.4321\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 126 Step: 10962 Index:0.6038 R2:0.7469 0.6038 0.5882 RMSE:0.7622 0.9333 0.9337 Tau:0.6840 0.5735 0.4320\n",
      "Epoch: 127 Step: 11049 Index:0.6188 R2:0.7478 0.6188 0.5795 RMSE:0.7675 0.9296 0.9556 Tau:0.6821 0.5942 0.4231\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 128 Step: 11136 Index:0.6084 R2:0.7501 0.6084 0.5712 RMSE:0.7734 0.9480 0.9777 Tau:0.6862 0.5901 0.4324\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 129 Step: 11223 Index:0.6135 R2:0.7441 0.6135 0.5728 RMSE:0.7670 0.9165 0.9439 Tau:0.6816 0.5868 0.4343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 130 Step: 11310 Index:0.6081 R2:0.7539 0.6081 0.5766 RMSE:0.7542 0.9411 0.9563 Tau:0.6857 0.5855 0.4252\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 131 Step: 11397 Index:0.6155 R2:0.7572 0.6155 0.5810 RMSE:0.7660 0.9384 0.9673 Tau:0.6926 0.5942 0.4348\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 132 Step: 11484 Index:0.6187 R2:0.7560 0.6187 0.5733 RMSE:0.8213 0.9928 1.0302 Tau:0.6895 0.5947 0.4264\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 133 Step: 11571 Index:0.5997 R2:0.7570 0.5997 0.5916 RMSE:0.7474 0.9425 0.9301 Tau:0.6892 0.5822 0.4314\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 134 Step: 11658 Index:0.6150 R2:0.7604 0.6150 0.5791 RMSE:0.7472 0.9108 0.9311 Tau:0.6960 0.5942 0.4356\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 135 Step: 11745 Index:0.6127 R2:0.7603 0.6127 0.5840 RMSE:0.7749 0.9671 0.9872 Tau:0.6911 0.5882 0.4279\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 136 Step: 11832 Index:0.6070 R2:0.7508 0.6070 0.5712 RMSE:0.7570 0.9273 0.9583 Tau:0.6878 0.5813 0.4332\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 137 Step: 11919 Index:0.6133 R2:0.7600 0.6133 0.5786 RMSE:0.7473 0.9176 0.9428 Tau:0.6951 0.5887 0.4338\n",
      "Epoch: 138 Step: 12006 Index:0.6259 R2:0.7636 0.6259 0.5702 RMSE:0.7684 0.9172 0.9582 Tau:0.6951 0.5947 0.4285\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 139 Step: 12093 Index:0.6111 R2:0.7598 0.6111 0.5907 RMSE:0.7447 0.9312 0.9349 Tau:0.6904 0.5878 0.4267\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 140 Step: 12180 Index:0.6241 R2:0.7634 0.6241 0.5882 RMSE:0.7525 0.9057 0.9292 Tau:0.6931 0.5928 0.4243\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 141 Step: 12267 Index:0.6080 R2:0.7622 0.6080 0.5734 RMSE:0.7539 0.9257 0.9420 Tau:0.6988 0.5850 0.4329\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 142 Step: 12354 Index:0.6178 R2:0.7649 0.6178 0.5840 RMSE:0.7340 0.9104 0.9297 Tau:0.6967 0.5951 0.4298\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 143 Step: 12441 Index:0.6195 R2:0.7653 0.6195 0.5828 RMSE:0.7861 0.9365 0.9559 Tau:0.6988 0.5873 0.4325\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 144 Step: 12528 Index:0.6088 R2:0.7594 0.6088 0.5668 RMSE:0.7632 0.9647 0.9937 Tau:0.6900 0.5878 0.4203\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 145 Step: 12615 Index:0.6133 R2:0.7692 0.6133 0.5800 RMSE:0.7284 0.9271 0.9483 Tau:0.6997 0.5887 0.4315\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 146 Step: 12702 Index:0.6146 R2:0.7700 0.6146 0.5772 RMSE:0.7293 0.9304 0.9580 Tau:0.7011 0.5933 0.4351\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 147 Step: 12789 Index:0.6062 R2:0.7610 0.6062 0.5838 RMSE:0.7827 0.9476 0.9621 Tau:0.6882 0.5901 0.4151\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 148 Step: 12876 Index:0.6180 R2:0.7729 0.6180 0.5933 RMSE:0.7618 0.9281 0.9394 Tau:0.7010 0.5891 0.4283\n",
      "Epoch: 149 Step: 12963 Index:0.6296 R2:0.7767 0.6296 0.5787 RMSE:0.7142 0.9028 0.9454 Tau:0.7033 0.5993 0.4257\n",
      "Epoch: 150 Step: 13050 Index:0.6342 R2:0.7757 0.6342 0.5773 RMSE:0.7211 0.8875 0.9364 Tau:0.7059 0.5988 0.4335\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 151 Step: 13137 Index:0.6244 R2:0.7741 0.6244 0.5947 RMSE:0.7279 0.9070 0.9247 Tau:0.7025 0.5924 0.4263\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 152 Step: 13224 Index:0.6272 R2:0.7750 0.6272 0.5773 RMSE:0.7272 0.9070 0.9511 Tau:0.7029 0.5919 0.4277\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 153 Step: 13311 Index:0.6315 R2:0.7803 0.6315 0.5863 RMSE:0.7194 0.8978 0.9373 Tau:0.7075 0.5956 0.4291\n",
      "Epoch: 154 Step: 13398 Index:0.6409 R2:0.7840 0.6409 0.5901 RMSE:0.7340 0.8880 0.9301 Tau:0.7122 0.6025 0.4327\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 155 Step: 13485 Index:0.6255 R2:0.7757 0.6255 0.5657 RMSE:0.7204 0.9182 0.9792 Tau:0.7052 0.5988 0.4351\n",
      "Epoch: 156 Step: 13572 Index:0.6469 R2:0.7828 0.6469 0.5698 RMSE:0.7053 0.8842 0.9610 Tau:0.7095 0.6094 0.4312\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 157 Step: 13659 Index:0.6436 R2:0.7791 0.6436 0.5665 RMSE:0.7156 0.8807 0.9598 Tau:0.7060 0.5984 0.4280\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 158 Step: 13746 Index:0.6457 R2:0.7732 0.6457 0.5632 RMSE:0.7400 0.9056 0.9994 Tau:0.7040 0.5988 0.4376\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 159 Step: 13833 Index:0.6358 R2:0.7774 0.6358 0.5937 RMSE:0.7454 0.9045 0.9401 Tau:0.7028 0.5974 0.4277\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 160 Step: 13920 Index:0.6461 R2:0.7795 0.6461 0.5739 RMSE:0.7193 0.8921 0.9626 Tau:0.7068 0.6062 0.4309\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 161 Step: 14007 Index:0.6370 R2:0.7789 0.6370 0.5756 RMSE:0.7715 0.9136 0.9602 Tau:0.7116 0.5988 0.4378\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 162 Step: 14094 Index:0.6418 R2:0.7804 0.6418 0.5854 RMSE:0.7191 0.8838 0.9341 Tau:0.7086 0.6030 0.4348\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 163 Step: 14181 Index:0.6396 R2:0.7812 0.6396 0.5838 RMSE:0.7089 0.8963 0.9505 Tau:0.7087 0.6034 0.4311\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 164 Step: 14268 Index:0.6314 R2:0.7806 0.6314 0.5817 RMSE:0.7546 0.9129 0.9494 Tau:0.7108 0.5928 0.4369\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 165 Step: 14355 Index:0.6410 R2:0.7851 0.6410 0.5824 RMSE:0.7478 0.9024 0.9580 Tau:0.7113 0.6043 0.4285\n",
      "Epoch: 166 Step: 14442 Index:0.6491 R2:0.7899 0.6491 0.5951 RMSE:0.7594 0.9032 0.9575 Tau:0.7165 0.6057 0.4335\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 167 Step: 14529 Index:0.6325 R2:0.7824 0.6325 0.6020 RMSE:0.7123 0.8954 0.9103 Tau:0.7106 0.5961 0.4339\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 168 Step: 14616 Index:0.6398 R2:0.7907 0.6398 0.5858 RMSE:0.7172 0.8947 0.9457 Tau:0.7172 0.6016 0.4320\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 169 Step: 14703 Index:0.6328 R2:0.7742 0.6328 0.5620 RMSE:0.7324 0.8915 0.9552 Tau:0.7037 0.5878 0.4402\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 170 Step: 14790 Index:0.6279 R2:0.7860 0.6279 0.5874 RMSE:0.7661 0.9929 1.0169 Tau:0.7111 0.5937 0.4323\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 171 Step: 14877 Index:0.3969 R2:0.5516 0.3969 0.4540 RMSE:1.1080 1.1867 1.1315 Tau:0.5523 0.4652 0.3666\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 172 Step: 14964 Index:0.4480 R2:0.6276 0.4480 0.5016 RMSE:0.9673 1.1160 1.0691 Tau:0.6025 0.4915 0.4111\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 173 Step: 15051 Index:0.4720 R2:0.6502 0.4720 0.5261 RMSE:0.9094 1.0744 1.0167 Tau:0.6167 0.5007 0.4231\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 174 Step: 15138 Index:0.4956 R2:0.6765 0.4956 0.5503 RMSE:0.9897 1.1234 1.0160 Tau:0.6332 0.5127 0.4254\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 175 Step: 15225 Index:0.5156 R2:0.6873 0.5156 0.5553 RMSE:0.8479 1.0228 0.9631 Tau:0.6408 0.5205 0.4326\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 176 Step: 15312 Index:0.5334 R2:0.7034 0.5334 0.5569 RMSE:0.8395 1.0027 0.9576 Tau:0.6538 0.5334 0.4413\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 177 Step: 15399 Index:0.5457 R2:0.7116 0.5457 0.5701 RMSE:0.8290 0.9942 0.9386 Tau:0.6587 0.5362 0.4402\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 178 Step: 15486 Index:0.5641 R2:0.7186 0.5641 0.5729 RMSE:0.8181 0.9750 0.9362 Tau:0.6619 0.5509 0.4366\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 179 Step: 15573 Index:0.5707 R2:0.7223 0.5707 0.5813 RMSE:0.8611 1.0029 0.9586 Tau:0.6646 0.5551 0.4306\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 180 Step: 15660 Index:0.5936 R2:0.7354 0.5936 0.5766 RMSE:0.7872 0.9359 0.9320 Tau:0.6739 0.5656 0.4343\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 181 Step: 15747 Index:0.5903 R2:0.7385 0.5903 0.5845 RMSE:0.7960 0.9496 0.9297 Tau:0.6786 0.5643 0.4372\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 182 Step: 15834 Index:0.6076 R2:0.7454 0.6076 0.5849 RMSE:0.7699 0.9288 0.9451 Tau:0.6837 0.5726 0.4383\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 183 Step: 15921 Index:0.6097 R2:0.7450 0.6097 0.5829 RMSE:0.7641 0.9174 0.9324 Tau:0.6828 0.5735 0.4420\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 184 Step: 16008 Index:0.6127 R2:0.7543 0.6127 0.5711 RMSE:0.7507 0.9156 0.9474 Tau:0.6860 0.5827 0.4310\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 185 Step: 16095 Index:0.6259 R2:0.7585 0.6259 0.5789 RMSE:0.7444 0.8980 0.9360 Tau:0.6920 0.5868 0.4351\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 186 Step: 16182 Index:0.6172 R2:0.7570 0.6172 0.5935 RMSE:0.7473 0.9119 0.9234 Tau:0.6900 0.5822 0.4361\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 187 Step: 16269 Index:0.6163 R2:0.7610 0.6163 0.5845 RMSE:0.7454 0.9278 0.9510 Tau:0.6934 0.5873 0.4333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 188 Step: 16356 Index:0.6196 R2:0.7587 0.6196 0.5641 RMSE:0.7799 0.9249 0.9714 Tau:0.6888 0.5762 0.4258\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 189 Step: 16443 Index:0.6301 R2:0.7633 0.6301 0.5688 RMSE:0.7674 0.9083 0.9644 Tau:0.6930 0.5822 0.4289\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 190 Step: 16530 Index:0.6395 R2:0.7682 0.6395 0.5588 RMSE:0.7511 0.9149 0.9936 Tau:0.6989 0.5974 0.4335\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 191 Step: 16617 Index:0.6206 R2:0.7672 0.6206 0.5673 RMSE:0.7390 0.9175 0.9693 Tau:0.6961 0.5891 0.4264\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 192 Step: 16704 Index:0.6319 R2:0.7638 0.6319 0.5661 RMSE:0.7347 0.8944 0.9573 Tau:0.6977 0.5887 0.4407\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 193 Step: 16791 Index:0.6424 R2:0.7754 0.6424 0.5614 RMSE:0.7212 0.8772 0.9575 Tau:0.7037 0.6011 0.4340\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 194 Step: 16878 Index:0.6377 R2:0.7705 0.6377 0.5555 RMSE:0.7780 0.9095 0.9951 Tau:0.6982 0.5901 0.4325\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 195 Step: 16965 Index:0.6387 R2:0.7773 0.6387 0.5868 RMSE:0.7162 0.8846 0.9309 Tau:0.7068 0.5961 0.4350\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 196 Step: 17052 Index:0.6453 R2:0.7811 0.6453 0.5896 RMSE:0.7181 0.8786 0.9345 Tau:0.7078 0.5997 0.4332\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 196 r2:0.5951 RMSE:0.9575 WTI:0.3568 AP:0.6288 Tau:0.4335 \n",
      " \n",
      " Top-1:0.3750 Top-1-fp:0.3750 \n",
      " Top-5:0.3488 Top-5-fp:0.5116 \n",
      " Top-10:0.5402 Top-10-fp:0.4023 \n",
      " Top-15:0.5692 Top-15-fp:0.3538 \n",
      " Top-20:0.6839 Top-20-fp:0.3046 \n",
      " Top-25:0.6952 Top-25-fp:0.3272 \n",
      " Top-30:0.7714 Top-30-fp:0.3793 \n",
      " Top-40:0.8429 Top-40-fp:0.4914 \n",
      " Top-50:0.8905 Top-50-fp:0.5701 \n",
      " \n",
      " Top50:0.3200 Top50-fp:0.5200 \n",
      " Top100:0.5600 Top100-fp:0.3800 \n",
      " Top150:0.6200 Top150-fp:0.3200 \n",
      " Top200:0.6900 Top200-fp:0.3100 \n",
      " Top250:0.7619 Top250-fp:0.3600 \n",
      " Top300:0.8143 Top300-fp:0.4300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
