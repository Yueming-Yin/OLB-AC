{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P08908_1_300\n",
      "model_file/0_GAFSE_Ki_P08908_1_300_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P08908_1_300_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P08908_1_300_test.csv\"\n",
    "test_active = 300\n",
    "val_rate = 0.03\n",
    "random_seed = 1\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0   C1CC(N(C1)C(=O)CCCCN2CCN(CC2)C3=CC=CC=C3O)C(=O)N -1.716003\n",
      "1  C1CN(CCC1C2=CSC3=CC=CC=C32)CC(COC4=CC=CC5=C4C=... -0.260071\n",
      "2                COC1=CC(=CC=C1)SC2=CC=CC=C2N3CCNCC3 -2.146128\n",
      "3        C1CN(CCN1CC2=CN3C=CC=CC3=N2)C4=CC=C(C=C4)Cl -3.230449\n",
      "4       CCCOC1=CC=CC=C1CN2CCN(CC2)CC3=CC4=CC=CC=C4O3 -3.390582\n",
      "number of all smiles:  3207\n",
      "number of successfully processed smiles:  3207\n",
      "                                              smiles     value  \\\n",
      "0   C1CC(N(C1)C(=O)CCCCN2CCN(CC2)C3=CC=CC=C3O)C(=O)N -1.716003   \n",
      "1  C1CN(CCC1C2=CSC3=CC=CC=C32)CC(COC4=CC=CC5=C4C=... -0.260071   \n",
      "2                COC1=CC(=CC=C1)SC2=CC=CC=C2N3CCNCC3 -2.146128   \n",
      "3        C1CN(CCN1CC2=CN3C=CC=CC3=N2)C4=CC=C(C=C4)Cl -3.230449   \n",
      "4       CCCOC1=CC=CC=C1CN2CCN(CC2)CC3=CC4=CC=CC=C4O3 -3.390582   \n",
      "\n",
      "                                      cano_smiles  \n",
      "0       NC(=O)C1CCCN1C(=O)CCCCN1CCN(c2ccccc2O)CC1  \n",
      "1  OC(COc1cccc2[nH]ccc12)CN1CCC(c2csc3ccccc23)CC1  \n",
      "2                   COc1cccc(Sc2ccccc2N2CCNCC2)c1  \n",
      "3            Clc1ccc(N2CCN(Cc3cn4ccccc4n3)CC2)cc1  \n",
      "4           CCCOc1ccccc1CN1CCN(Cc2cc3ccccc3o2)CC1  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  717\n",
      "number of successfully processed smiles:  717\n",
      "(717, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CN(CCC1CC2=CC(=C(C=C2)Br)OCC(=O)O)CCC3=CC4=C... -1.959041   \n",
      "1  CN(C)S(=O)(=O)NC1=CC=C(C=C1)N2CCN(CC2)C(=O)COC... -1.605305   \n",
      "2  CCOC(=O)C1CCN(CC1)CC2COC3=C(O2)C4=C(C=C3)NC(=C... -0.079181   \n",
      "3  C1=CC=C(C(=C1)COC2=CC3=C(C=C2)NC=C3CCN)COC4=CC... -0.633468   \n",
      "4              COC1=CC=CC=C1OCCNCCCC2=CNC3=CC=CC=C32 -0.301030   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  Cl.O=C(O)COc1cc(CC2CCN(CCc3ccc4c(c3)C(=O)C(O)C...  \n",
      "1  CN(C)S(=O)(=O)Nc1ccc(N2CCN(C(=O)COc3ccc4[nH]cc...  \n",
      "2  CCOC(=O)C1CCN(CC2COc3ccc4[nH]c(C(=O)OC)cc4c3O2...  \n",
      "3  NCCc1c[nH]c2ccc(OCc3ccccc3COc3ccc4[nH]cc(CCN)c...  \n",
      "4                  COc1ccccc1OCCNCCCc1c[nH]c2ccccc12  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P08908_1_300_train.pickle\n",
      "./data/benchmark/Ki_P08908_1_300_train\n",
      "3924\n",
      "feature dicts file saved as ./data/benchmark/Ki_P08908_1_300_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3111, 3) (96, 3) (717, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P08908_1_300_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3127451/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 311 Index:0.1855 R2:0.0604 0.1855 0.1308 RMSE:1.0643 0.9970 1.0042 Tau:0.1659 0.2713 0.2240\n",
      "Epoch: 2 Step: 622 Index:0.2065 R2:0.0725 0.2065 0.1619 RMSE:1.0595 0.9678 0.9832 Tau:0.1860 0.3025 0.2588\n",
      "Epoch: 3 Step: 933 Index:0.2319 R2:0.1061 0.2319 0.2045 RMSE:1.0353 0.9564 0.9557 Tau:0.2216 0.3174 0.2565\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 4 Step: 1244 Index:0.2266 R2:0.1327 0.2266 0.2393 RMSE:1.0219 0.9535 0.9373 Tau:0.2490 0.3227 0.2544\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 5 Step: 1555 Index:0.2279 R2:0.1422 0.2279 0.2375 RMSE:1.0569 0.9935 0.9774 Tau:0.2578 0.3214 0.2471\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 6 Step: 1866 Index:0.2167 R2:0.1325 0.2167 0.2332 RMSE:1.0582 0.9840 0.9728 Tau:0.2562 0.3231 0.2780\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 7 Step: 2177 Index:0.2273 R2:0.1608 0.2273 0.2570 RMSE:1.0106 0.9454 0.9248 Tau:0.2759 0.3376 0.2633\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 8 Step: 2488 Index:0.2100 R2:0.1737 0.2100 0.2651 RMSE:1.0154 0.9659 0.9371 Tau:0.2886 0.3275 0.2566\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 9 Step: 2799 Index:0.2212 R2:0.1824 0.2212 0.2745 RMSE:0.9924 0.9401 0.9079 Tau:0.2943 0.3394 0.2635\n",
      "Epoch: 10 Step: 3110 Index:0.2372 R2:0.1962 0.2372 0.2770 RMSE:0.9856 0.9345 0.9079 Tau:0.3048 0.3477 0.2764\n",
      "Epoch: 11 Step: 3421 Index:0.2425 R2:0.2023 0.2425 0.2826 RMSE:0.9803 0.9286 0.9018 Tau:0.3108 0.3512 0.2866\n",
      "Epoch: 12 Step: 3732 Index:0.2534 R2:0.2101 0.2534 0.2795 RMSE:0.9736 0.9194 0.8994 Tau:0.3160 0.3539 0.2885\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 13 Step: 4043 Index:0.2393 R2:0.2195 0.2393 0.2919 RMSE:0.9996 0.9614 0.9253 Tau:0.3256 0.3530 0.2956\n",
      "Epoch: 14 Step: 4354 Index:0.2739 R2:0.2294 0.2739 0.2972 RMSE:0.9615 0.9053 0.8876 Tau:0.3303 0.3640 0.3017\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 15 Step: 4665 Index:0.2477 R2:0.2325 0.2477 0.2825 RMSE:0.9774 0.9346 0.9125 Tau:0.3298 0.3482 0.2898\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 16 Step: 4976 Index:0.2565 R2:0.2596 0.2565 0.3071 RMSE:0.9658 0.9313 0.9061 Tau:0.3546 0.3556 0.2912\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 17 Step: 5287 Index:0.2705 R2:0.2589 0.2705 0.3070 RMSE:0.9470 0.9077 0.8853 Tau:0.3531 0.3714 0.3011\n",
      "Epoch: 18 Step: 5598 Index:0.3031 R2:0.2729 0.3031 0.3247 RMSE:0.9407 0.8949 0.8752 Tau:0.3622 0.3771 0.3217\n",
      "Epoch: 19 Step: 5909 Index:0.3183 R2:0.2829 0.3183 0.3193 RMSE:0.9318 0.8786 0.8744 Tau:0.3686 0.3863 0.3164\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 20 Step: 6220 Index:0.2848 R2:0.2859 0.2848 0.3260 RMSE:0.9309 0.9001 0.8702 Tau:0.3747 0.3749 0.3053\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 21 Step: 6531 Index:0.2907 R2:0.3030 0.2907 0.3211 RMSE:0.9363 0.9073 0.8931 Tau:0.3859 0.3789 0.2921\n",
      "Epoch: 22 Step: 6842 Index:0.3223 R2:0.3213 0.3223 0.3353 RMSE:0.9126 0.8779 0.8667 Tau:0.3983 0.3960 0.3118\n",
      "Epoch: 23 Step: 7153 Index:0.3420 R2:0.3207 0.3420 0.3209 RMSE:0.9174 0.8716 0.8813 Tau:0.3954 0.4114 0.3050\n",
      "Epoch: 24 Step: 7464 Index:0.3514 R2:0.3355 0.3514 0.3575 RMSE:0.8943 0.8555 0.8450 Tau:0.4061 0.4166 0.3122\n",
      "Epoch: 25 Step: 7775 Index:0.3628 R2:0.3485 0.3628 0.3474 RMSE:0.8910 0.8489 0.8580 Tau:0.4144 0.4245 0.3011\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 26 Step: 8086 Index:0.3479 R2:0.3566 0.3479 0.3597 RMSE:0.8874 0.8608 0.8471 Tau:0.4200 0.4254 0.2987\n",
      "Epoch: 27 Step: 8397 Index:0.3916 R2:0.3602 0.3916 0.3409 RMSE:0.9166 0.8619 0.8915 Tau:0.4218 0.4355 0.2871\n",
      "Epoch: 28 Step: 8708 Index:0.4156 R2:0.3713 0.4156 0.3834 RMSE:0.9114 0.8492 0.8749 Tau:0.4297 0.4553 0.3016\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 29 Step: 9019 Index:0.3949 R2:0.3676 0.3949 0.3828 RMSE:0.8764 0.8281 0.8324 Tau:0.4282 0.4430 0.3071\n",
      "Epoch: 30 Step: 9330 Index:0.4311 R2:0.3897 0.4311 0.4000 RMSE:0.8547 0.8001 0.8151 Tau:0.4399 0.4649 0.3122\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 31 Step: 9641 Index:0.4077 R2:0.4003 0.4077 0.3965 RMSE:0.8522 0.8168 0.8209 Tau:0.4482 0.4579 0.3114\n",
      "Epoch: 32 Step: 9952 Index:0.4350 R2:0.4057 0.4350 0.3908 RMSE:0.8906 0.8391 0.8667 Tau:0.4546 0.4763 0.3004\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 10263 Index:0.4120 R2:0.4065 0.4120 0.3898 RMSE:0.8858 0.8487 0.8700 Tau:0.4507 0.4724 0.2993\n",
      "Epoch: 34 Step: 10574 Index:0.4480 R2:0.4246 0.4480 0.4128 RMSE:0.8633 0.8134 0.8400 Tau:0.4640 0.4873 0.3184\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 35 Step: 10885 Index:0.4391 R2:0.4168 0.4391 0.4203 RMSE:0.8637 0.8168 0.8376 Tau:0.4584 0.4829 0.3157\n",
      "Epoch: 36 Step: 11196 Index:0.4702 R2:0.4369 0.4702 0.4269 RMSE:0.8605 0.8034 0.8425 Tau:0.4722 0.5005 0.3195\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 37 Step: 11507 Index:0.4697 R2:0.4343 0.4697 0.4103 RMSE:0.8731 0.8124 0.8600 Tau:0.4711 0.5058 0.3140\n",
      "Epoch: 38 Step: 11818 Index:0.5129 R2:0.4397 0.5129 0.4454 RMSE:0.8295 0.7547 0.7880 Tau:0.4736 0.5290 0.3508\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 39 Step: 12129 Index:0.4982 R2:0.4521 0.4982 0.4457 RMSE:0.8685 0.8076 0.8412 Tau:0.4807 0.5198 0.3413\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 40 Step: 12440 Index:0.4698 R2:0.4470 0.4698 0.4510 RMSE:0.8371 0.7918 0.8136 Tau:0.4790 0.5167 0.3382\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 41 Step: 12751 Index:0.5037 R2:0.4526 0.5037 0.4535 RMSE:0.8144 0.7514 0.7873 Tau:0.4806 0.5159 0.3454\n",
      "Epoch: 42 Step: 13062 Index:0.5237 R2:0.4770 0.5237 0.4510 RMSE:0.8065 0.7471 0.7935 Tau:0.4967 0.5321 0.3529\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 43 Step: 13373 Index:0.5063 R2:0.4761 0.5063 0.4461 RMSE:0.8140 0.7641 0.8118 Tau:0.4969 0.5216 0.3304\n",
      "Epoch: 44 Step: 13684 Index:0.5290 R2:0.4735 0.5290 0.4695 RMSE:0.8226 0.7632 0.7927 Tau:0.4955 0.5330 0.3545\n",
      "Epoch: 45 Step: 13995 Index:0.5302 R2:0.4953 0.5302 0.4804 RMSE:0.8255 0.7755 0.8118 Tau:0.5095 0.5409 0.3556\n",
      "Epoch: 46 Step: 14306 Index:0.5414 R2:0.4971 0.5414 0.4511 RMSE:0.7829 0.7230 0.7848 Tau:0.5106 0.5422 0.3410\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 47 Step: 14617 Index:0.5385 R2:0.4889 0.5385 0.4825 RMSE:0.8055 0.7421 0.7895 Tau:0.5013 0.5374 0.3718\n",
      "Epoch: 48 Step: 14928 Index:0.5486 R2:0.5040 0.5486 0.4465 RMSE:0.7960 0.7354 0.8079 Tau:0.5159 0.5519 0.3561\n",
      "Epoch: 49 Step: 15239 Index:0.5608 R2:0.5010 0.5608 0.4691 RMSE:0.8222 0.7601 0.8056 Tau:0.5120 0.5646 0.3551\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 50 Step: 15550 Index:0.5360 R2:0.5042 0.5360 0.4902 RMSE:0.7704 0.7212 0.7539 Tau:0.5139 0.5418 0.3738\n",
      "Epoch: 51 Step: 15861 Index:0.5717 R2:0.5164 0.5717 0.4795 RMSE:0.7670 0.7042 0.7632 Tau:0.5211 0.5611 0.3784\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 52 Step: 16172 Index:0.5296 R2:0.5185 0.5296 0.4577 RMSE:0.7766 0.7390 0.7896 Tau:0.5244 0.5413 0.3649\n",
      "Epoch: 53 Step: 16483 Index:0.5899 R2:0.5370 0.5899 0.4954 RMSE:0.7508 0.6883 0.7486 Tau:0.5352 0.5764 0.3785\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 54 Step: 16794 Index:0.5699 R2:0.5212 0.5699 0.4853 RMSE:0.7588 0.6967 0.7589 Tau:0.5243 0.5734 0.3708\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 55 Step: 17105 Index:0.5898 R2:0.5502 0.5898 0.5065 RMSE:0.7488 0.6925 0.7551 Tau:0.5427 0.5778 0.3851\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 56 Step: 17416 Index:0.5628 R2:0.5336 0.5628 0.4757 RMSE:0.7822 0.7230 0.7984 Tau:0.5335 0.5663 0.3637\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 57 Step: 17727 Index:0.5718 R2:0.5454 0.5718 0.4769 RMSE:0.8049 0.7549 0.8336 Tau:0.5405 0.5611 0.3647\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 58 Step: 18038 Index:0.5772 R2:0.5310 0.5772 0.4703 RMSE:0.7627 0.7024 0.7791 Tau:0.5313 0.5729 0.3907\n",
      "Epoch: 59 Step: 18349 Index:0.6223 R2:0.5616 0.6223 0.4928 RMSE:0.7315 0.6587 0.7501 Tau:0.5504 0.5958 0.3813\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 60 Step: 18660 Index:0.5580 R2:0.5501 0.5580 0.4586 RMSE:0.7441 0.7120 0.7807 Tau:0.5470 0.5685 0.3650\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 61 Step: 18971 Index:0.6033 R2:0.5634 0.6033 0.4923 RMSE:0.7821 0.7328 0.7947 Tau:0.5535 0.5782 0.3892\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 62 Step: 19282 Index:0.6072 R2:0.5870 0.6072 0.5059 RMSE:0.7205 0.6778 0.7533 Tau:0.5700 0.5923 0.3792\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 63 Step: 19593 Index:0.6163 R2:0.5596 0.6163 0.5015 RMSE:0.7304 0.6663 0.7428 Tau:0.5488 0.6054 0.4014\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 64 Step: 19904 Index:0.6173 R2:0.5720 0.6173 0.5159 RMSE:0.7260 0.6642 0.7478 Tau:0.5590 0.5980 0.4112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 Step: 20215 Index:0.6301 R2:0.5774 0.6301 0.5332 RMSE:0.7175 0.6573 0.7235 Tau:0.5605 0.5997 0.4069\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 66 Step: 20526 Index:0.5934 R2:0.5664 0.5934 0.5089 RMSE:0.7211 0.6822 0.7388 Tau:0.5546 0.5773 0.3926\n",
      "Epoch: 67 Step: 20837 Index:0.6453 R2:0.6022 0.6453 0.5129 RMSE:0.7099 0.6489 0.7530 Tau:0.5800 0.6107 0.3914\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 68 Step: 21148 Index:0.6118 R2:0.6074 0.6118 0.5265 RMSE:0.7057 0.6723 0.7452 Tau:0.5809 0.5848 0.3920\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 69 Step: 21459 Index:0.5817 R2:0.5881 0.5817 0.5003 RMSE:0.7321 0.7060 0.7827 Tau:0.5692 0.5773 0.3797\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 70 Step: 21770 Index:0.6403 R2:0.6184 0.6403 0.5117 RMSE:0.7397 0.6962 0.8011 Tau:0.5909 0.6089 0.3981\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 71 Step: 22081 Index:0.6234 R2:0.5932 0.6234 0.5001 RMSE:0.7191 0.6682 0.7617 Tau:0.5744 0.5927 0.3828\n",
      "Epoch: 72 Step: 22392 Index:0.6507 R2:0.6181 0.6507 0.5337 RMSE:0.6791 0.6277 0.7213 Tau:0.5875 0.6234 0.4152\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 73 Step: 22703 Index:0.6100 R2:0.6224 0.6100 0.5175 RMSE:0.7212 0.6982 0.7852 Tau:0.5907 0.5830 0.3932\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 74 Step: 23014 Index:0.6399 R2:0.6206 0.6399 0.5302 RMSE:0.6760 0.6379 0.7241 Tau:0.5897 0.6151 0.4029\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 75 Step: 23325 Index:0.6390 R2:0.5860 0.6390 0.5272 RMSE:0.7022 0.6359 0.7313 Tau:0.5690 0.6182 0.4208\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 76 Step: 23636 Index:0.6227 R2:0.6274 0.6227 0.5447 RMSE:0.7184 0.6911 0.7671 Tau:0.5946 0.5997 0.4240\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 77 Step: 23947 Index:0.6408 R2:0.6321 0.6408 0.5407 RMSE:0.6746 0.6490 0.7148 Tau:0.5983 0.6015 0.4212\n",
      "Epoch: 78 Step: 24258 Index:0.6692 R2:0.6282 0.6692 0.5306 RMSE:0.7200 0.6571 0.7820 Tau:0.5967 0.6287 0.4244\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 79 Step: 24569 Index:0.5964 R2:0.6213 0.5964 0.5229 RMSE:0.6875 0.6781 0.7562 Tau:0.5927 0.5773 0.4158\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 80 Step: 24880 Index:0.6493 R2:0.6235 0.6493 0.5500 RMSE:0.6716 0.6357 0.7121 Tau:0.5905 0.6234 0.4256\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 81 Step: 25191 Index:0.6582 R2:0.6420 0.6582 0.5416 RMSE:0.6748 0.6342 0.7409 Tau:0.6067 0.6243 0.4384\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 82 Step: 25502 Index:0.6482 R2:0.6463 0.6482 0.5238 RMSE:0.6842 0.6506 0.7617 Tau:0.6080 0.6160 0.4127\n",
      "Epoch: 83 Step: 25813 Index:0.6742 R2:0.6499 0.6742 0.5391 RMSE:0.6653 0.6192 0.7357 Tau:0.6103 0.6348 0.4294\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 84 Step: 26124 Index:0.6437 R2:0.6480 0.6437 0.5467 RMSE:0.6915 0.6621 0.7622 Tau:0.6091 0.6151 0.4210\n",
      "Epoch: 85 Step: 26435 Index:0.6762 R2:0.6385 0.6762 0.5682 RMSE:0.6569 0.6057 0.6959 Tau:0.6024 0.6322 0.4452\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 86 Step: 26746 Index:0.6412 R2:0.6487 0.6412 0.5278 RMSE:0.6553 0.6372 0.7237 Tau:0.6087 0.6133 0.4245\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 87 Step: 27057 Index:0.6462 R2:0.6542 0.6462 0.5497 RMSE:0.6450 0.6369 0.7089 Tau:0.6124 0.6081 0.4215\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 88 Step: 27368 Index:0.6616 R2:0.6621 0.6616 0.5563 RMSE:0.6407 0.6171 0.7083 Tau:0.6186 0.6247 0.4214\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 89 Step: 27679 Index:0.6640 R2:0.6663 0.6640 0.5540 RMSE:0.6668 0.6373 0.7455 Tau:0.6229 0.6318 0.4351\n",
      "Epoch: 90 Step: 27990 Index:0.6842 R2:0.6447 0.6842 0.5346 RMSE:0.6965 0.6336 0.7709 Tau:0.6084 0.6401 0.4380\n",
      "Epoch: 91 Step: 28301 Index:0.6956 R2:0.6621 0.6956 0.5384 RMSE:0.6402 0.5868 0.7267 Tau:0.6208 0.6463 0.4265\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 92 Step: 28612 Index:0.6560 R2:0.6656 0.6560 0.5184 RMSE:0.7496 0.7166 0.8620 Tau:0.6229 0.6199 0.4225\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 93 Step: 28923 Index:0.6859 R2:0.6619 0.6859 0.5365 RMSE:0.6483 0.5997 0.7390 Tau:0.6198 0.6375 0.4379\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 94 Step: 29234 Index:0.6717 R2:0.6650 0.6717 0.5604 RMSE:0.6343 0.6098 0.7046 Tau:0.6197 0.6261 0.4432\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 95 Step: 29545 Index:0.6830 R2:0.6854 0.6830 0.5613 RMSE:0.6219 0.5986 0.7099 Tau:0.6345 0.6405 0.4420\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 96 Step: 29856 Index:0.6796 R2:0.6791 0.6796 0.5616 RMSE:0.6197 0.6045 0.7000 Tau:0.6297 0.6392 0.4384\n",
      "Epoch: 97 Step: 30167 Index:0.7095 R2:0.6827 0.7095 0.5904 RMSE:0.6325 0.6026 0.6880 Tau:0.6316 0.6577 0.4513\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 98 Step: 30478 Index:0.6891 R2:0.6943 0.6891 0.5763 RMSE:0.6339 0.6113 0.7205 Tau:0.6404 0.6493 0.4518\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 99 Step: 30789 Index:0.6874 R2:0.7003 0.6874 0.5660 RMSE:0.6107 0.5981 0.7067 Tau:0.6453 0.6405 0.4416\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 100 Step: 31100 Index:0.6658 R2:0.6948 0.6658 0.5478 RMSE:0.6163 0.6149 0.7350 Tau:0.6404 0.6410 0.4311\n",
      "Epoch: 101 Step: 31411 Index:0.7120 R2:0.6872 0.7120 0.5673 RMSE:0.6477 0.5887 0.7446 Tau:0.6350 0.6616 0.4361\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 102 Step: 31722 Index:0.6687 R2:0.6809 0.6687 0.5357 RMSE:0.6238 0.6100 0.7235 Tau:0.6315 0.6344 0.4330\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 103 Step: 32033 Index:0.6773 R2:0.6765 0.6773 0.5221 RMSE:0.6236 0.6013 0.7425 Tau:0.6281 0.6388 0.4521\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 104 Step: 32344 Index:0.6745 R2:0.6723 0.6745 0.5286 RMSE:0.6668 0.6221 0.7823 Tau:0.6262 0.6212 0.4449\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 105 Step: 32655 Index:0.6926 R2:0.6914 0.6926 0.5340 RMSE:0.6335 0.5987 0.7492 Tau:0.6394 0.6484 0.4438\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 106 Step: 32966 Index:0.6948 R2:0.7044 0.6948 0.5610 RMSE:0.6544 0.6210 0.7796 Tau:0.6493 0.6414 0.4567\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 107 Step: 33277 Index:0.6750 R2:0.7022 0.6750 0.5679 RMSE:0.5988 0.6057 0.7000 Tau:0.6458 0.6212 0.4523\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 108 Step: 33588 Index:0.6781 R2:0.6978 0.6781 0.5535 RMSE:0.6055 0.6175 0.7167 Tau:0.6431 0.6296 0.4288\n",
      "Epoch: 109 Step: 33899 Index:0.7132 R2:0.7071 0.7132 0.5580 RMSE:0.5948 0.5680 0.7150 Tau:0.6518 0.6713 0.4620\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 110 Step: 34210 Index:0.6941 R2:0.7022 0.6941 0.5691 RMSE:0.6151 0.5937 0.7192 Tau:0.6468 0.6414 0.4606\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 111 Step: 34521 Index:0.6651 R2:0.7005 0.6651 0.5384 RMSE:0.6007 0.6192 0.7169 Tau:0.6463 0.6340 0.4491\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 112 Step: 34832 Index:0.6653 R2:0.6590 0.6653 0.5099 RMSE:0.6494 0.6278 0.7382 Tau:0.6193 0.6370 0.4356\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 113 Step: 35143 Index:0.6966 R2:0.7223 0.6966 0.5784 RMSE:0.5857 0.5873 0.7049 Tau:0.6610 0.6546 0.4575\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 114 Step: 35454 Index:0.6939 R2:0.7064 0.6939 0.5862 RMSE:0.6107 0.5944 0.7301 Tau:0.6484 0.6669 0.4524\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 115 Step: 35765 Index:0.6829 R2:0.7182 0.6829 0.5640 RMSE:0.5807 0.6004 0.7105 Tau:0.6589 0.6370 0.4549\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 116 Step: 36076 Index:0.7034 R2:0.6950 0.7034 0.5709 RMSE:0.6033 0.5899 0.6990 Tau:0.6405 0.6533 0.4633\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 117 Step: 36387 Index:0.7080 R2:0.7285 0.7080 0.5582 RMSE:0.5820 0.5719 0.7346 Tau:0.6664 0.6686 0.4580\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 118 Step: 36698 Index:0.6730 R2:0.6997 0.6730 0.5268 RMSE:0.6586 0.6286 0.8236 Tau:0.6463 0.6309 0.4435\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 119 Step: 37009 Index:0.6874 R2:0.7274 0.6874 0.5668 RMSE:0.5709 0.5979 0.7015 Tau:0.6642 0.6388 0.4663\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 120 Step: 37320 Index:0.6761 R2:0.7057 0.6761 0.5431 RMSE:0.6955 0.6764 0.8443 Tau:0.6515 0.6357 0.4653\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 121 Step: 37631 Index:0.6877 R2:0.7170 0.6877 0.5772 RMSE:0.5812 0.5976 0.7083 Tau:0.6567 0.6432 0.4592\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 122 Step: 37942 Index:0.6472 R2:0.7168 0.6472 0.5337 RMSE:0.5991 0.6308 0.7484 Tau:0.6551 0.6142 0.4636\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 123 Step: 38253 Index:0.6903 R2:0.7309 0.6903 0.5828 RMSE:0.5665 0.5970 0.6929 Tau:0.6687 0.6313 0.4739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 124 Step: 38564 Index:0.6887 R2:0.7144 0.6887 0.5351 RMSE:0.5965 0.5943 0.7453 Tau:0.6544 0.6414 0.4609\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 125 Step: 38875 Index:0.6807 R2:0.7218 0.6807 0.5493 RMSE:0.6055 0.6124 0.7539 Tau:0.6630 0.6388 0.4828\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 126 Step: 39186 Index:0.7088 R2:0.7429 0.7088 0.5636 RMSE:0.5579 0.5776 0.7015 Tau:0.6759 0.6599 0.4766\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 127 Step: 39497 Index:0.6711 R2:0.7333 0.6711 0.5440 RMSE:0.5710 0.6319 0.7200 Tau:0.6686 0.6379 0.4531\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 128 Step: 39808 Index:0.6982 R2:0.7444 0.6982 0.5613 RMSE:0.5583 0.5815 0.7230 Tau:0.6775 0.6577 0.4516\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 129 Step: 40119 Index:0.6804 R2:0.7403 0.6804 0.5764 RMSE:0.5647 0.5985 0.7182 Tau:0.6747 0.6375 0.4706\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 130 Step: 40430 Index:0.6816 R2:0.7439 0.6816 0.5720 RMSE:0.5605 0.5973 0.7180 Tau:0.6766 0.6353 0.4752\n",
      "Epoch: 131 Step: 40741 Index:0.7217 R2:0.7524 0.7217 0.5895 RMSE:0.5918 0.5761 0.7492 Tau:0.6830 0.6651 0.4850\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 132 Step: 41052 Index:0.7159 R2:0.7418 0.7159 0.5806 RMSE:0.5725 0.5675 0.7131 Tau:0.6770 0.6612 0.4933\n",
      "Epoch: 133 Step: 41363 Index:0.7365 R2:0.7485 0.7365 0.5833 RMSE:0.5851 0.5492 0.7551 Tau:0.6814 0.6836 0.4931\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 134 Step: 41674 Index:0.6947 R2:0.7539 0.6947 0.5837 RMSE:0.5428 0.5869 0.6990 Tau:0.6842 0.6493 0.4862\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 135 Step: 41985 Index:0.6903 R2:0.7577 0.6903 0.5454 RMSE:0.5408 0.6041 0.7246 Tau:0.6873 0.6405 0.4780\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 136 Step: 42296 Index:0.7062 R2:0.7456 0.7062 0.5463 RMSE:0.5600 0.5747 0.7420 Tau:0.6802 0.6489 0.4806\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 137 Step: 42607 Index:0.6756 R2:0.7223 0.6756 0.5410 RMSE:0.5914 0.6064 0.7899 Tau:0.6636 0.6445 0.4723\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 138 Step: 42918 Index:0.6717 R2:0.7547 0.6717 0.5365 RMSE:0.5557 0.6099 0.7231 Tau:0.6846 0.6414 0.4874\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 139 Step: 43229 Index:0.7112 R2:0.7441 0.7112 0.5553 RMSE:0.5701 0.5709 0.7580 Tau:0.6786 0.6524 0.4845\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 140 Step: 43540 Index:0.6563 R2:0.7370 0.6563 0.5496 RMSE:0.5795 0.6209 0.7599 Tau:0.6718 0.6138 0.4764\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 141 Step: 43851 Index:0.7183 R2:0.7519 0.7183 0.5567 RMSE:0.5458 0.5801 0.7193 Tau:0.6840 0.6695 0.4615\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 142 Step: 44162 Index:0.7091 R2:0.7541 0.7091 0.5529 RMSE:0.5519 0.5715 0.7340 Tau:0.6844 0.6528 0.4841\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 143 Step: 44473 Index:0.7190 R2:0.7539 0.7190 0.5544 RMSE:0.5563 0.5613 0.7628 Tau:0.6868 0.6656 0.4767\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 144 Step: 44784 Index:0.7123 R2:0.7519 0.7123 0.5598 RMSE:0.5867 0.5758 0.7737 Tau:0.6832 0.6695 0.4753\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 145 Step: 45095 Index:0.6991 R2:0.7514 0.6991 0.5499 RMSE:0.5445 0.5976 0.7264 Tau:0.6827 0.6463 0.4717\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 146 Step: 45406 Index:0.6992 R2:0.7589 0.6992 0.5523 RMSE:0.5721 0.5853 0.7775 Tau:0.6879 0.6550 0.4719\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 147 Step: 45717 Index:0.6912 R2:0.7627 0.6912 0.5647 RMSE:0.5514 0.6260 0.7003 Tau:0.6918 0.6388 0.4794\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 148 Step: 46028 Index:0.6569 R2:0.7277 0.6569 0.5740 RMSE:0.5792 0.6391 0.6907 Tau:0.6664 0.6208 0.4674\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 149 Step: 46339 Index:0.6952 R2:0.7656 0.6952 0.5456 RMSE:0.5375 0.5843 0.7433 Tau:0.6925 0.6594 0.4845\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 150 Step: 46650 Index:0.6699 R2:0.7662 0.6699 0.5348 RMSE:0.5510 0.6108 0.7591 Tau:0.6915 0.6362 0.4768\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 151 Step: 46961 Index:0.7097 R2:0.7717 0.7097 0.5498 RMSE:0.5728 0.5855 0.7954 Tau:0.6973 0.6669 0.4875\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 152 Step: 47272 Index:0.6799 R2:0.7725 0.6799 0.5543 RMSE:0.5407 0.6053 0.7566 Tau:0.7003 0.6401 0.4876\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 153 Step: 47583 Index:0.7000 R2:0.7706 0.7000 0.5683 RMSE:0.5229 0.5917 0.7128 Tau:0.6967 0.6410 0.4998\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 154 Step: 47894 Index:0.6931 R2:0.7581 0.6931 0.5538 RMSE:0.5370 0.5972 0.7341 Tau:0.6899 0.6480 0.4815\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 155 Step: 48205 Index:0.6640 R2:0.7614 0.6640 0.5402 RMSE:0.5424 0.6144 0.7639 Tau:0.6893 0.6111 0.4662\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 156 Step: 48516 Index:0.6677 R2:0.7106 0.6677 0.5201 RMSE:0.5960 0.6115 0.7532 Tau:0.6506 0.6313 0.4796\n",
      "Epoch: 157 Step: 48827 Index:0.7502 R2:0.7610 0.7502 0.5603 RMSE:0.5448 0.5323 0.7284 Tau:0.6884 0.6994 0.4951\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 158 Step: 49138 Index:0.7421 R2:0.7743 0.7421 0.5705 RMSE:0.5212 0.5531 0.6994 Tau:0.6976 0.6796 0.4993\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 159 Step: 49449 Index:0.7414 R2:0.7577 0.7414 0.5530 RMSE:0.5400 0.5468 0.7281 Tau:0.6881 0.6823 0.5084\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 160 Step: 49760 Index:0.7258 R2:0.7660 0.7258 0.5651 RMSE:0.5634 0.5630 0.7777 Tau:0.6928 0.6682 0.5004\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 161 Step: 50071 Index:0.7061 R2:0.7704 0.7061 0.5443 RMSE:0.5460 0.5786 0.7519 Tau:0.6967 0.6463 0.5059\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 162 Step: 50382 Index:0.7228 R2:0.7774 0.7228 0.5581 RMSE:0.5198 0.5615 0.7119 Tau:0.7051 0.6651 0.5000\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 163 Step: 50693 Index:0.7166 R2:0.7818 0.7166 0.5544 RMSE:0.5114 0.5691 0.7233 Tau:0.7060 0.6638 0.5077\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 164 Step: 51004 Index:0.6899 R2:0.7882 0.6899 0.5478 RMSE:0.5202 0.5893 0.7457 Tau:0.7105 0.6388 0.4966\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 165 Step: 51315 Index:0.7117 R2:0.7751 0.7117 0.5764 RMSE:0.5198 0.5739 0.7230 Tau:0.6980 0.6634 0.5145\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 166 Step: 51626 Index:0.6833 R2:0.7595 0.6833 0.5507 RMSE:0.5510 0.5955 0.7379 Tau:0.6906 0.6410 0.4989\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 167 Step: 51937 Index:0.7124 R2:0.7797 0.7124 0.5684 RMSE:0.5163 0.5683 0.7212 Tau:0.7045 0.6651 0.5150\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 168 Step: 52248 Index:0.6967 R2:0.7538 0.6967 0.5474 RMSE:0.5507 0.5842 0.7234 Tau:0.6858 0.6449 0.4994\n",
      "Epoch: 169 Step: 52559 Index:0.7583 R2:0.7867 0.7583 0.5941 RMSE:0.5074 0.5261 0.6930 Tau:0.7104 0.6862 0.5180\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 170 Step: 52870 Index:0.7336 R2:0.7884 0.7336 0.5467 RMSE:0.5419 0.5562 0.7796 Tau:0.7102 0.6748 0.5049\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 171 Step: 53181 Index:0.7131 R2:0.7860 0.7131 0.5607 RMSE:0.5299 0.5702 0.7461 Tau:0.7093 0.6550 0.5159\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 172 Step: 53492 Index:0.7050 R2:0.7790 0.7050 0.5559 RMSE:0.5242 0.5757 0.7620 Tau:0.7020 0.6546 0.5078\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 173 Step: 53803 Index:0.7246 R2:0.7839 0.7246 0.5576 RMSE:0.5087 0.5603 0.7264 Tau:0.7102 0.6695 0.5080\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 174 Step: 54114 Index:0.7146 R2:0.7720 0.7146 0.5566 RMSE:0.5349 0.5700 0.7522 Tau:0.6991 0.6638 0.5167\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 175 Step: 54425 Index:0.7139 R2:0.7881 0.7139 0.5479 RMSE:0.5152 0.5675 0.7367 Tau:0.7117 0.6735 0.5084\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 176 Step: 54736 Index:0.7210 R2:0.8000 0.7210 0.5692 RMSE:0.4957 0.5602 0.7255 Tau:0.7196 0.6726 0.5079\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 177 Step: 55047 Index:0.7367 R2:0.7934 0.7367 0.5790 RMSE:0.5088 0.5429 0.7227 Tau:0.7158 0.6743 0.5263\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 178 Step: 55358 Index:0.6837 R2:0.7659 0.6837 0.5450 RMSE:0.5289 0.6049 0.7242 Tau:0.6953 0.6340 0.5135\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 179 Step: 55669 Index:0.6946 R2:0.7767 0.6946 0.5469 RMSE:0.5231 0.5850 0.7509 Tau:0.7014 0.6484 0.5061\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 180 Step: 55980 Index:0.7013 R2:0.8003 0.7013 0.5669 RMSE:0.4912 0.5852 0.7120 Tau:0.7197 0.6331 0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 181 Step: 56291 Index:0.7345 R2:0.7965 0.7345 0.5903 RMSE:0.4948 0.5471 0.7006 Tau:0.7164 0.6792 0.5231\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 182 Step: 56602 Index:0.7041 R2:0.7961 0.7041 0.5653 RMSE:0.4951 0.5812 0.7116 Tau:0.7169 0.6555 0.5166\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 183 Step: 56913 Index:0.7357 R2:0.7982 0.7357 0.5732 RMSE:0.4933 0.5623 0.7058 Tau:0.7210 0.6739 0.5260\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 184 Step: 57224 Index:0.7032 R2:0.7802 0.7032 0.5310 RMSE:0.5174 0.5846 0.7453 Tau:0.7047 0.6445 0.5068\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 185 Step: 57535 Index:0.7268 R2:0.7976 0.7268 0.5660 RMSE:0.4960 0.5545 0.7272 Tau:0.7175 0.6664 0.5207\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 186 Step: 57846 Index:0.7012 R2:0.7625 0.7012 0.5130 RMSE:0.5654 0.5884 0.8293 Tau:0.6891 0.6410 0.4831\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 187 Step: 58157 Index:0.6545 R2:0.7489 0.6545 0.5547 RMSE:0.5707 0.6295 0.7839 Tau:0.6796 0.6146 0.4972\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 188 Step: 58468 Index:0.7246 R2:0.7797 0.7246 0.5664 RMSE:0.5366 0.5580 0.7675 Tau:0.7063 0.6708 0.5104\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 189 Step: 58779 Index:0.7201 R2:0.7937 0.7201 0.5635 RMSE:0.5054 0.5601 0.7465 Tau:0.7143 0.6484 0.5172\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 190 Step: 59090 Index:0.7171 R2:0.7783 0.7171 0.5313 RMSE:0.5502 0.5734 0.7986 Tau:0.7062 0.6621 0.5263\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 191 Step: 59401 Index:0.7165 R2:0.8039 0.7165 0.5812 RMSE:0.4929 0.5687 0.7323 Tau:0.7227 0.6542 0.5164\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 192 Step: 59712 Index:0.7240 R2:0.7955 0.7240 0.5668 RMSE:0.4974 0.5598 0.7341 Tau:0.7181 0.6651 0.5135\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 193 Step: 60023 Index:0.6777 R2:0.7713 0.6777 0.5247 RMSE:0.5329 0.6017 0.7699 Tau:0.6977 0.6331 0.4956\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 194 Step: 60334 Index:0.7048 R2:0.7798 0.7048 0.5512 RMSE:0.5202 0.5766 0.7659 Tau:0.7065 0.6463 0.5079\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 195 Step: 60645 Index:0.7172 R2:0.8098 0.7172 0.5765 RMSE:0.4871 0.5798 0.6903 Tau:0.7270 0.6691 0.5213\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 196 Step: 60956 Index:0.7454 R2:0.8072 0.7454 0.5592 RMSE:0.4960 0.5373 0.7495 Tau:0.7277 0.6831 0.5362\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 197 Step: 61267 Index:0.7051 R2:0.8092 0.7051 0.5617 RMSE:0.4841 0.5754 0.7206 Tau:0.7268 0.6550 0.5235\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 198 Step: 61578 Index:0.7216 R2:0.7799 0.7216 0.5685 RMSE:0.5182 0.5769 0.7689 Tau:0.7048 0.6717 0.5091\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 199 Step: 61889 Index:0.7155 R2:0.8128 0.7155 0.5472 RMSE:0.4801 0.5662 0.7351 Tau:0.7318 0.6638 0.5223\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 199 r2:0.5941 RMSE:0.6930 WTI:0.3576 AP:0.6980 Tau:0.5180 \n",
      " \n",
      " Top-1:0.4286 Top-1-fp:0.0000 \n",
      " Top-5:0.4286 Top-5-fp:0.0857 \n",
      " Top-10:0.6901 Top-10-fp:0.0845 \n",
      " Top-15:0.6355 Top-15-fp:0.1215 \n",
      " Top-20:0.7063 Top-20-fp:0.1608 \n",
      " Top-25:0.7207 Top-25-fp:0.2067 \n",
      " Top-30:0.7256 Top-30-fp:0.2326 \n",
      " Top-40:0.7028 Top-40-fp:0.2902 \n",
      " Top-50:0.7933 Top-50-fp:0.3352 \n",
      " \n",
      " Top50:0.6400 Top50-fp:0.0600 \n",
      " Top100:0.6500 Top100-fp:0.1100 \n",
      " Top150:0.7067 Top150-fp:0.1667 \n",
      " Top200:0.7250 Top200-fp:0.2250 \n",
      " Top250:0.7200 Top250-fp:0.2520 \n",
      " Top300:0.7033 Top300-fp:0.2967 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
