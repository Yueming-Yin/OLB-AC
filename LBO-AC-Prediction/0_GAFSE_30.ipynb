{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P0DMS8_0.3333333333333333_250\n",
      "model_file/0_GAFSE_Ki_P0DMS8_0.3333333333333333_250_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P0DMS8_0.3333333333333333_250_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P0DMS8_0.3333333333333333_250_test.csv\"\n",
    "test_active = 250\n",
    "val_rate = 0.15\n",
    "random_seed = 2023\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC(C)CCN1C=C2C(=N1)N=C(N3C2=NC(=N3)C4=CC=CO4)N... -1.477121\n",
      "1  CC(CC1=CC=CC=C1)NC2=NC3=C(C(=N2)N)N=CN3C4C(C(C... -3.087781\n",
      "2    CCCOC(=O)C1=C(N=C(C(=C1C)C(=O)OCC)C2=CC=CC=C2)C -2.322219\n",
      "3      CCN1C(=O)C2=C(N=C(N2)C3=CC=CC=C3)N(C1=O)CCSCC -2.610000\n",
      "4  C1=CC=C(C=C1)C#CC2=NC3=C(C(=N2)N)N=CN3C4C(C(C(... -1.204120\n",
      "number of all smiles:  895\n",
      "number of successfully processed smiles:  895\n",
      "                                              smiles     value  \\\n",
      "0  CC(C)CCN1C=C2C(=N1)N=C(N3C2=NC(=N3)C4=CC=CO4)N... -1.477121   \n",
      "1  CC(CC1=CC=CC=C1)NC2=NC3=C(C(=N2)N)N=CN3C4C(C(C... -3.087781   \n",
      "2    CCCOC(=O)C1=C(N=C(C(=C1C)C(=O)OCC)C2=CC=CC=C2)C -2.322219   \n",
      "3      CCN1C(=O)C2=C(N=C(N2)C3=CC=CC=C3)N(C1=O)CCSCC -2.610000   \n",
      "4  C1=CC=C(C=C1)C#CC2=NC3=C(C(=N2)N)N=CN3C4C(C(C(... -1.204120   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  COc1ccc(NC(=O)Nc2nc3nn(CCC(C)C)cc3c3nc(-c4ccco...  \n",
      "1    CC(Cc1ccccc1)Nc1nc(N)c2ncn(C3OC(CO)C(O)C3O)c2n1  \n",
      "2         CCCOC(=O)c1c(C)nc(-c2ccccc2)c(C(=O)OCC)c1C  \n",
      "3       CCSCCn1c(=O)n(CC)c(=O)c2[nH]c(-c3ccccc3)nc21  \n",
      "4         Nc1nc(C#Cc2ccccc2)nc2c1ncn2C1OC(CO)C(O)C1O  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1121\n",
      "number of successfully processed smiles:  1121\n",
      "(1121, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CONC1=NC(=NC2=C1N=CN2C3C(C(C(O3)CO)O)O)[N+](=O... -1.555094   \n",
      "1  CCNC(=O)C1C(C(C(O1)N2C=NC3=C2N=C(N=C3NC)N4C=C(... -0.447158   \n",
      "2       COC1=CC=C(C=C1)NC(=O)NC2=NC(=NS2)C3=CC=CC=C3 -3.518514   \n",
      "3       C1=CC=C2C(=C1)N=C(C3=NC(=NN23)C4=CC=CO4)NCCO -1.480007   \n",
      "4  CC1=NC(=C(C(C1C(=O)OC)C2=CC=CC=C2[N+](=O)[O-])... -3.918555   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0      CONc1nc([N+](=O)[O-])nc2c1ncn2C1OC(CO)C(O)C1O  \n",
      "1  CCNC(=O)C1OC(n2cnc3c(NC)nc(-n4cc(Cc5ccccc5)nn4...  \n",
      "2              COc1ccc(NC(=O)Nc2nc(-c3ccccc3)ns2)cc1  \n",
      "3                  OCCNc1nc2ccccc2n2nc(-c3ccco3)nc12  \n",
      "4  COC(=O)C1=C(C)N=C(C)C(C(=O)OC)C1c1ccccc1[N+](=...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P0DMS8_0.3333333333333333_250_train.pickle\n",
      "./data/benchmark/Ki_P0DMS8_0.3333333333333333_250_train\n",
      "2016\n",
      "feature dicts file saved as ./data/benchmark/Ki_P0DMS8_0.3333333333333333_250_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761, 3) (134, 3) (1121, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 8\n",
    "patience = 60\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P0DMS8_0.3333333333333333_250_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3044680/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 140 Index:-1.3455 R2:0.0124 0.0295 0.0176 RMSE:1.1723 1.2236 1.2204 Tau:-0.0947 -0.1219 0.0208\n",
      "Epoch: 2 Step: 280 Index:-0.9435 R2:0.0666 0.0512 0.0495 RMSE:1.0850 1.1418 1.1352 Tau:0.1907 0.1983 0.1672\n",
      "Epoch: 3 Step: 420 Index:-0.8572 R2:0.1524 0.1100 0.1536 RMSE:1.0652 1.1397 1.1144 Tau:0.2739 0.2825 0.2022\n",
      "Epoch: 4 Step: 560 Index:-0.7833 R2:0.1848 0.1490 0.1920 RMSE:1.0183 1.0852 1.0629 Tau:0.2982 0.3018 0.2089\n",
      "Epoch: 5 Step: 700 Index:-0.7670 R2:0.2021 0.1618 0.2156 RMSE:1.0131 1.0715 1.0464 Tau:0.3126 0.3045 0.2136\n",
      "Epoch: 6 Step: 840 Index:-0.7547 R2:0.2230 0.1770 0.2357 RMSE:0.9909 1.0583 1.0272 Tau:0.3264 0.3036 0.2233\n",
      "Epoch: 7 Step: 980 Index:-0.7236 R2:0.2503 0.1955 0.2654 RMSE:0.9786 1.0501 1.0156 Tau:0.3516 0.3265 0.2266\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 8 Step: 1120 Index:-0.7952 R2:0.2470 0.2071 0.2611 RMSE:1.0764 1.1112 1.0913 Tau:0.3461 0.3160 0.2177\n",
      "Epoch: 9 Step: 1260 Index:-0.7027 R2:0.2805 0.2122 0.2925 RMSE:0.9549 1.0378 0.9938 Tau:0.3727 0.3351 0.2480\n",
      "Epoch: 10 Step: 1400 Index:-0.6845 R2:0.2853 0.2336 0.2891 RMSE:0.9615 1.0257 0.9937 Tau:0.3747 0.3411 0.2349\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 11 Step: 1540 Index:-0.7450 R2:0.3060 0.2454 0.3139 RMSE:0.9952 1.0990 1.0431 Tau:0.3969 0.3539 0.2410\n",
      "Epoch: 12 Step: 1680 Index:-0.6315 R2:0.3195 0.2797 0.3209 RMSE:0.9234 0.9942 0.9651 Tau:0.3992 0.3627 0.2391\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 13 Step: 1820 Index:-0.6927 R2:0.3438 0.2776 0.3456 RMSE:0.9509 1.0590 1.0002 Tau:0.4187 0.3663 0.2470\n",
      "Epoch: 14 Step: 1960 Index:-0.6198 R2:0.3596 0.3118 0.3673 RMSE:0.9097 0.9991 0.9557 Tau:0.4236 0.3793 0.2301\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 15 Step: 2100 Index:-0.6989 R2:0.3514 0.3185 0.3496 RMSE:0.9872 1.0836 1.0436 Tau:0.4202 0.3847 0.2374\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 16 Step: 2240 Index:-0.7463 R2:0.3448 0.3081 0.3331 RMSE:1.0380 1.1346 1.0938 Tau:0.4214 0.3883 0.2457\n",
      "Epoch: 17 Step: 2380 Index:-0.6006 R2:0.3728 0.3106 0.3630 RMSE:0.8858 0.9768 0.9343 Tau:0.4358 0.3762 0.2466\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 18 Step: 2520 Index:-0.6531 R2:0.3767 0.3221 0.3546 RMSE:0.9377 1.0425 1.0022 Tau:0.4355 0.3894 0.2561\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 19 Step: 2660 Index:-0.6132 R2:0.3884 0.3274 0.3736 RMSE:0.8914 0.9956 0.9503 Tau:0.4403 0.3825 0.2448\n",
      "Epoch: 20 Step: 2800 Index:-0.5897 R2:0.3947 0.3248 0.3779 RMSE:0.8712 0.9742 0.9261 Tau:0.4463 0.3845 0.2592\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 21 Step: 2940 Index:-0.6083 R2:0.3718 0.3334 0.3483 RMSE:0.9020 0.9941 0.9697 Tau:0.4271 0.3858 0.2418\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 22 Step: 3080 Index:-0.5939 R2:0.4035 0.3422 0.3778 RMSE:0.8813 0.9857 0.9471 Tau:0.4520 0.3919 0.2438\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 23 Step: 3220 Index:-0.6225 R2:0.4070 0.3331 0.3787 RMSE:0.8968 1.0167 0.9641 Tau:0.4590 0.3941 0.2642\n",
      "Epoch: 24 Step: 3360 Index:-0.5759 R2:0.4079 0.3287 0.3733 RMSE:0.8740 0.9592 0.9274 Tau:0.4556 0.3834 0.2664\n",
      "Epoch: 25 Step: 3500 Index:-0.5335 R2:0.4159 0.3547 0.3888 RMSE:0.8537 0.9362 0.9059 Tau:0.4606 0.4027 0.2541\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 26 Step: 3640 Index:-0.5579 R2:0.4294 0.3461 0.4012 RMSE:0.8489 0.9563 0.9084 Tau:0.4703 0.3984 0.2595\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 27 Step: 3780 Index:-0.5544 R2:0.4226 0.3469 0.3816 RMSE:0.8467 0.9562 0.9171 Tau:0.4677 0.4018 0.2704\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 28 Step: 3920 Index:-0.5361 R2:0.4323 0.3534 0.4010 RMSE:0.8388 0.9395 0.8969 Tau:0.4709 0.4033 0.2608\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 29 Step: 4060 Index:-0.5688 R2:0.4414 0.3499 0.3970 RMSE:0.8457 0.9746 0.9214 Tau:0.4804 0.4058 0.2787\n",
      "Epoch: 30 Step: 4200 Index:-0.5302 R2:0.4334 0.3535 0.3881 RMSE:0.8371 0.9347 0.9028 Tau:0.4748 0.4045 0.2657\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 31 Step: 4340 Index:-0.5647 R2:0.4465 0.3378 0.3970 RMSE:0.8322 0.9647 0.9068 Tau:0.4847 0.4000 0.2827\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 32 Step: 4480 Index:-0.5310 R2:0.4519 0.3617 0.4053 RMSE:0.8276 0.9458 0.9009 Tau:0.4858 0.4148 0.2670\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 33 Step: 4620 Index:-0.5345 R2:0.4521 0.3728 0.3967 RMSE:0.8364 0.9587 0.9230 Tau:0.4868 0.4242 0.2741\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 34 Step: 4760 Index:-0.5388 R2:0.4673 0.3587 0.4119 RMSE:0.8203 0.9435 0.8945 Tau:0.4983 0.4047 0.2734\n",
      "Epoch: 35 Step: 4900 Index:-0.4909 R2:0.4631 0.3757 0.4051 RMSE:0.8165 0.9189 0.8903 Tau:0.4922 0.4281 0.2823\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 36 Step: 5040 Index:-0.5405 R2:0.4617 0.3547 0.3970 RMSE:0.8176 0.9524 0.9051 Tau:0.4969 0.4119 0.2856\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 37 Step: 5180 Index:-0.5158 R2:0.4702 0.3891 0.4089 RMSE:0.8242 0.9508 0.9161 Tau:0.4935 0.4350 0.2755\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 38 Step: 5320 Index:-0.5039 R2:0.4821 0.3847 0.4156 RMSE:0.8043 0.9324 0.8925 Tau:0.5051 0.4285 0.2852\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 39 Step: 5460 Index:-0.5532 R2:0.4894 0.3855 0.4186 RMSE:0.8295 0.9794 0.9293 Tau:0.5090 0.4263 0.2841\n",
      "Epoch: 40 Step: 5600 Index:-0.4756 R2:0.4806 0.3936 0.4093 RMSE:0.8343 0.9130 0.9026 Tau:0.5015 0.4375 0.2871\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 41 Step: 5740 Index:-0.4921 R2:0.4881 0.3915 0.4152 RMSE:0.8005 0.9289 0.8954 Tau:0.5090 0.4368 0.2801\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 42 Step: 5880 Index:-0.5761 R2:0.4909 0.3832 0.4130 RMSE:0.8500 1.0070 0.9551 Tau:0.5151 0.4310 0.2948\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 43 Step: 6020 Index:-0.5064 R2:0.4847 0.3794 0.3920 RMSE:0.7987 0.9315 0.9092 Tau:0.5106 0.4251 0.2963\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 44 Step: 6160 Index:-0.5129 R2:0.5067 0.3895 0.4132 RMSE:0.8001 0.9539 0.9141 Tau:0.5229 0.4411 0.2984\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 45 Step: 6300 Index:-0.5073 R2:0.4832 0.3903 0.4113 RMSE:0.8084 0.9406 0.9048 Tau:0.5020 0.4332 0.2825\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 46 Step: 6440 Index:-0.5098 R2:0.5105 0.4052 0.4157 RMSE:0.8049 0.9531 0.9215 Tau:0.5232 0.4433 0.2930\n",
      "Epoch: 47 Step: 6580 Index:-0.4604 R2:0.5129 0.4002 0.4148 RMSE:0.7749 0.9044 0.8839 Tau:0.5278 0.4440 0.3073\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 48 Step: 6720 Index:-0.4619 R2:0.5231 0.4077 0.4238 RMSE:0.7690 0.9122 0.8830 Tau:0.5309 0.4503 0.3083\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 49 Step: 6860 Index:-0.5115 R2:0.5079 0.3700 0.4022 RMSE:0.7857 0.9269 0.8934 Tau:0.5259 0.4155 0.3056\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 50 Step: 7000 Index:-0.4941 R2:0.5224 0.4026 0.4189 RMSE:0.7830 0.9386 0.9026 Tau:0.5311 0.4444 0.3018\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 51 Step: 7140 Index:-0.5135 R2:0.5185 0.3795 0.4166 RMSE:0.7770 0.9343 0.8885 Tau:0.5323 0.4209 0.3066\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 52 Step: 7280 Index:-0.4638 R2:0.5261 0.3947 0.4109 RMSE:0.7707 0.9038 0.8872 Tau:0.5372 0.4400 0.3092\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 53 Step: 7420 Index:-0.4696 R2:0.5305 0.3920 0.4192 RMSE:0.7644 0.9100 0.8797 Tau:0.5390 0.4404 0.3057\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 54 Step: 7560 Index:-0.4983 R2:0.5178 0.3762 0.3990 RMSE:0.7760 0.9174 0.8962 Tau:0.5325 0.4191 0.3090\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 55 Step: 7700 Index:-0.4817 R2:0.5304 0.3876 0.4121 RMSE:0.7860 0.9116 0.8930 Tau:0.5382 0.4298 0.3052\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 56 Step: 7840 Index:-0.5326 R2:0.5405 0.4042 0.4140 RMSE:0.8070 0.9802 0.9457 Tau:0.5463 0.4476 0.3068\n",
      "Epoch: 57 Step: 7980 Index:-0.4157 R2:0.5409 0.4263 0.4231 RMSE:0.7617 0.8797 0.8799 Tau:0.5400 0.4640 0.3102\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 58 Step: 8120 Index:-0.4673 R2:0.5326 0.4007 0.4051 RMSE:0.7612 0.9059 0.8918 Tau:0.5387 0.4386 0.3152\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 59 Step: 8260 Index:-0.4434 R2:0.5438 0.4163 0.4237 RMSE:0.7494 0.8975 0.8822 Tau:0.5465 0.4541 0.3100\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 60 Step: 8400 Index:-0.4415 R2:0.5534 0.4221 0.4258 RMSE:0.7458 0.8976 0.8803 Tau:0.5513 0.4561 0.3114\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 61 Step: 8540 Index:-0.4388 R2:0.5554 0.4218 0.4244 RMSE:0.7445 0.8964 0.8803 Tau:0.5528 0.4577 0.3174\n",
      "Epoch: 62 Step: 8680 Index:-0.3915 R2:0.5523 0.4431 0.4257 RMSE:0.7430 0.8712 0.8777 Tau:0.5476 0.4797 0.3064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 63 Step: 8820 Index:-0.5317 R2:0.5315 0.3815 0.3857 RMSE:0.7776 0.9503 0.9305 Tau:0.5407 0.4186 0.3194\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 64 Step: 8960 Index:-0.5079 R2:0.5186 0.3705 0.3948 RMSE:0.7755 0.9214 0.8990 Tau:0.5325 0.4135 0.3077\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 65 Step: 9100 Index:-0.4148 R2:0.5516 0.4332 0.4175 RMSE:0.7813 0.8853 0.9029 Tau:0.5489 0.4705 0.3218\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 66 Step: 9240 Index:-0.4826 R2:0.5523 0.4266 0.4066 RMSE:0.7788 0.9419 0.9365 Tau:0.5532 0.4593 0.3139\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 67 Step: 9380 Index:-0.4369 R2:0.5643 0.4157 0.4261 RMSE:0.7380 0.8883 0.8753 Tau:0.5583 0.4514 0.3217\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 68 Step: 9520 Index:-0.4099 R2:0.5776 0.4359 0.4293 RMSE:0.7275 0.8755 0.8720 Tau:0.5674 0.4656 0.3191\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 69 Step: 9660 Index:-0.4033 R2:0.5716 0.4526 0.4381 RMSE:0.7316 0.8821 0.8789 Tau:0.5614 0.4788 0.3167\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 70 Step: 9800 Index:-0.3935 R2:0.5676 0.4536 0.4327 RMSE:0.7322 0.8735 0.8785 Tau:0.5599 0.4799 0.3117\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 71 Step: 9940 Index:-0.4903 R2:0.5228 0.3873 0.3934 RMSE:0.7671 0.9116 0.9010 Tau:0.5318 0.4213 0.3086\n",
      "Epoch: 72 Step: 10080 Index:-0.3879 R2:0.5771 0.4538 0.4428 RMSE:0.7217 0.8640 0.8646 Tau:0.5689 0.4761 0.3161\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 73 Step: 10220 Index:-0.5098 R2:0.5842 0.4327 0.4322 RMSE:0.7852 0.9711 0.9437 Tau:0.5709 0.4613 0.3241\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 74 Step: 10360 Index:-0.5945 R2:0.5253 0.3617 0.3823 RMSE:0.8138 1.0001 0.9633 Tau:0.5373 0.4056 0.3192\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 75 Step: 10500 Index:-0.4122 R2:0.5898 0.4337 0.4434 RMSE:0.7270 0.8748 0.8655 Tau:0.5736 0.4626 0.3288\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 76 Step: 10640 Index:-0.4142 R2:0.5921 0.4394 0.4292 RMSE:0.7153 0.8806 0.8776 Tau:0.5794 0.4665 0.3245\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 77 Step: 10780 Index:-0.4063 R2:0.5938 0.4386 0.4416 RMSE:0.7122 0.8723 0.8627 Tau:0.5799 0.4660 0.3297\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 78 Step: 10920 Index:-0.4081 R2:0.5830 0.4351 0.4415 RMSE:0.7184 0.8743 0.8646 Tau:0.5671 0.4662 0.3188\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 79 Step: 11060 Index:-0.4018 R2:0.5982 0.4436 0.4467 RMSE:0.7062 0.8712 0.8606 Tau:0.5774 0.4694 0.3198\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 80 Step: 11200 Index:-0.4040 R2:0.6013 0.4583 0.4484 RMSE:0.7157 0.8873 0.8813 Tau:0.5808 0.4833 0.3208\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 81 Step: 11340 Index:-0.4205 R2:0.5980 0.4478 0.4364 RMSE:0.7150 0.8917 0.8896 Tau:0.5780 0.4712 0.3276\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 82 Step: 11480 Index:-0.4864 R2:0.5997 0.4362 0.4374 RMSE:0.7585 0.9515 0.9318 Tau:0.5789 0.4651 0.3383\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 83 Step: 11620 Index:-0.4181 R2:0.5869 0.4395 0.4457 RMSE:0.7137 0.8803 0.8652 Tau:0.5746 0.4622 0.3251\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 84 Step: 11760 Index:-0.4182 R2:0.6095 0.4523 0.4423 RMSE:0.7127 0.8880 0.8795 Tau:0.5860 0.4698 0.3199\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 85 Step: 11900 Index:-0.4323 R2:0.6031 0.4532 0.4612 RMSE:0.7305 0.9118 0.8880 Tau:0.5827 0.4795 0.3140\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 86 Step: 12040 Index:-0.4202 R2:0.6171 0.4525 0.4464 RMSE:0.7081 0.8927 0.8842 Tau:0.5914 0.4725 0.3339\n",
      "EarlyStopping counter: 15 out of 60\n",
      "Epoch: 87 Step: 12180 Index:-0.4399 R2:0.6197 0.4750 0.4580 RMSE:0.7431 0.9304 0.9266 Tau:0.5917 0.4905 0.3287\n",
      "EarlyStopping counter: 16 out of 60\n",
      "Epoch: 88 Step: 12320 Index:-0.4258 R2:0.6243 0.4405 0.4483 RMSE:0.6929 0.8913 0.8755 Tau:0.5949 0.4656 0.3340\n",
      "EarlyStopping counter: 17 out of 60\n",
      "Epoch: 89 Step: 12460 Index:-0.4332 R2:0.5983 0.4389 0.4231 RMSE:0.7192 0.8994 0.9070 Tau:0.5782 0.4662 0.3396\n",
      "Epoch: 90 Step: 12600 Index:-0.3696 R2:0.6233 0.4706 0.4587 RMSE:0.6852 0.8552 0.8556 Tau:0.6003 0.4855 0.3269\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 91 Step: 12740 Index:-0.4446 R2:0.6238 0.4422 0.4474 RMSE:0.7123 0.9106 0.8917 Tau:0.5971 0.4660 0.3233\n",
      "Epoch: 92 Step: 12880 Index:-0.3688 R2:0.6308 0.4618 0.4503 RMSE:0.6901 0.8555 0.8560 Tau:0.5982 0.4867 0.3193\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 93 Step: 13020 Index:-0.4920 R2:0.6256 0.4532 0.4437 RMSE:0.7701 0.9612 0.9501 Tau:0.5980 0.4691 0.3219\n",
      "Epoch: 94 Step: 13160 Index:-0.3500 R2:0.6150 0.4732 0.4564 RMSE:0.6889 0.8447 0.8555 Tau:0.5873 0.4948 0.3257\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 95 Step: 13300 Index:-0.3919 R2:0.6243 0.4501 0.4561 RMSE:0.6811 0.8629 0.8556 Tau:0.5972 0.4709 0.3358\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 96 Step: 13440 Index:-0.4245 R2:0.6345 0.4506 0.4501 RMSE:0.6959 0.8991 0.8927 Tau:0.6013 0.4745 0.3268\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 97 Step: 13580 Index:-0.3744 R2:0.6461 0.4597 0.4565 RMSE:0.6613 0.8550 0.8561 Tau:0.6096 0.4806 0.3303\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 98 Step: 13720 Index:-0.3774 R2:0.6498 0.4626 0.4643 RMSE:0.6578 0.8538 0.8496 Tau:0.6123 0.4763 0.3314\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 99 Step: 13860 Index:-0.4036 R2:0.6444 0.4582 0.4541 RMSE:0.6745 0.8764 0.8737 Tau:0.6080 0.4727 0.3308\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 100 Step: 14000 Index:-0.4272 R2:0.6285 0.4522 0.4535 RMSE:0.7016 0.9026 0.8864 Tau:0.6013 0.4754 0.3281\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 101 Step: 14140 Index:-0.4055 R2:0.6389 0.4532 0.4335 RMSE:0.6810 0.8787 0.8891 Tau:0.6030 0.4732 0.3334\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 102 Step: 14280 Index:-0.4484 R2:0.5959 0.4176 0.4319 RMSE:0.7051 0.8900 0.8745 Tau:0.5788 0.4415 0.3399\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 103 Step: 14420 Index:-0.4345 R2:0.5092 0.4483 0.3958 RMSE:0.8032 0.9202 0.9477 Tau:0.5442 0.4858 0.2667\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 104 Step: 14560 Index:-0.3608 R2:0.6479 0.4805 0.4643 RMSE:0.6638 0.8522 0.8660 Tau:0.6108 0.4914 0.3232\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 105 Step: 14700 Index:-0.4167 R2:0.6461 0.4557 0.4626 RMSE:0.6794 0.8883 0.8808 Tau:0.6104 0.4716 0.3312\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 106 Step: 14840 Index:-0.3658 R2:0.6638 0.4780 0.4654 RMSE:0.6602 0.8615 0.8665 Tau:0.6231 0.4956 0.3198\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 107 Step: 14980 Index:-0.3886 R2:0.6340 0.4635 0.4415 RMSE:0.6852 0.8690 0.8910 Tau:0.6070 0.4804 0.3299\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 108 Step: 15120 Index:-0.3567 R2:0.6653 0.4750 0.4534 RMSE:0.6672 0.8483 0.8657 Tau:0.6228 0.4916 0.3336\n",
      "EarlyStopping counter: 15 out of 60\n",
      "Epoch: 109 Step: 15260 Index:-0.3833 R2:0.6553 0.4733 0.4705 RMSE:0.6718 0.8753 0.8663 Tau:0.6148 0.4921 0.3273\n",
      "EarlyStopping counter: 16 out of 60\n",
      "Epoch: 110 Step: 15400 Index:-0.3862 R2:0.6625 0.4839 0.4654 RMSE:0.6742 0.8754 0.8882 Tau:0.6240 0.4891 0.3290\n",
      "Epoch: 111 Step: 15540 Index:-0.3455 R2:0.6426 0.4821 0.4635 RMSE:0.6853 0.8418 0.8623 Tau:0.6114 0.4963 0.3343\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 112 Step: 15680 Index:-0.3457 R2:0.6197 0.4742 0.4490 RMSE:0.6866 0.8429 0.8590 Tau:0.6026 0.4972 0.3027\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 113 Step: 15820 Index:-0.3951 R2:0.6400 0.4769 0.4477 RMSE:0.7168 0.8975 0.9106 Tau:0.6063 0.5024 0.3331\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 114 Step: 15960 Index:-0.3459 R2:0.6688 0.4806 0.4701 RMSE:0.6438 0.8416 0.8448 Tau:0.6225 0.4956 0.3242\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 115 Step: 16100 Index:-0.3835 R2:0.6682 0.4864 0.4756 RMSE:0.6735 0.8829 0.8839 Tau:0.6205 0.4995 0.3184\n",
      "Epoch: 116 Step: 16240 Index:-0.3250 R2:0.6848 0.4956 0.4811 RMSE:0.6245 0.8249 0.8363 Tau:0.6342 0.4999 0.3315\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 117 Step: 16380 Index:-0.3583 R2:0.6850 0.4712 0.4760 RMSE:0.6318 0.8452 0.8414 Tau:0.6361 0.4869 0.3315\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 118 Step: 16520 Index:-0.3875 R2:0.6777 0.4617 0.4701 RMSE:0.6372 0.8621 0.8514 Tau:0.6302 0.4745 0.3362\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 119 Step: 16660 Index:-0.3274 R2:0.6923 0.4926 0.4762 RMSE:0.6204 0.8271 0.8399 Tau:0.6411 0.4997 0.3250\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 120 Step: 16800 Index:-0.3505 R2:0.6782 0.4805 0.4726 RMSE:0.6533 0.8424 0.8398 Tau:0.6319 0.4918 0.3302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 121 Step: 16940 Index:-0.3769 R2:0.6825 0.4706 0.4617 RMSE:0.6382 0.8631 0.8731 Tau:0.6332 0.4862 0.3380\n",
      "Epoch: 122 Step: 17080 Index:-0.3210 R2:0.6860 0.4939 0.4669 RMSE:0.6249 0.8286 0.8489 Tau:0.6365 0.5076 0.3165\n",
      "Epoch: 123 Step: 17220 Index:-0.3107 R2:0.6705 0.5064 0.4588 RMSE:0.6375 0.8241 0.8695 Tau:0.6339 0.5134 0.3348\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 124 Step: 17360 Index:-0.4182 R2:0.6530 0.4647 0.4510 RMSE:0.6881 0.8916 0.8922 Tau:0.6188 0.4734 0.3354\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 125 Step: 17500 Index:-0.3602 R2:0.6843 0.4652 0.4624 RMSE:0.6324 0.8500 0.8472 Tau:0.6378 0.4898 0.3418\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 126 Step: 17640 Index:-0.3254 R2:0.6766 0.4932 0.4578 RMSE:0.6336 0.8309 0.8669 Tau:0.6391 0.5055 0.3351\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 127 Step: 17780 Index:-0.3272 R2:0.6799 0.5011 0.4640 RMSE:0.6365 0.8393 0.8703 Tau:0.6366 0.5120 0.3371\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 128 Step: 17920 Index:-0.3634 R2:0.6868 0.4786 0.4689 RMSE:0.6588 0.8546 0.8599 Tau:0.6405 0.4912 0.3387\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 129 Step: 18060 Index:-0.3402 R2:0.6424 0.4921 0.4618 RMSE:0.6769 0.8533 0.8655 Tau:0.6124 0.5132 0.3306\n",
      "Epoch: 130 Step: 18200 Index:-0.3093 R2:0.6768 0.5135 0.4710 RMSE:0.6423 0.8308 0.8536 Tau:0.6326 0.5215 0.3325\n",
      "Epoch: 131 Step: 18340 Index:-0.3059 R2:0.6778 0.5199 0.4629 RMSE:0.6483 0.8337 0.8653 Tau:0.6315 0.5278 0.3336\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 132 Step: 18480 Index:-0.3382 R2:0.6650 0.5041 0.4523 RMSE:0.6656 0.8547 0.9007 Tau:0.6264 0.5165 0.3060\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 133 Step: 18620 Index:-0.3517 R2:0.6680 0.4739 0.4498 RMSE:0.6519 0.8448 0.8639 Tau:0.6301 0.4932 0.3456\n",
      "Epoch: 134 Step: 18760 Index:-0.2982 R2:0.6813 0.5092 0.4597 RMSE:0.6299 0.8134 0.8574 Tau:0.6354 0.5152 0.3438\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 135 Step: 18900 Index:-0.3044 R2:0.6886 0.5057 0.4588 RMSE:0.6217 0.8182 0.8576 Tau:0.6401 0.5138 0.3382\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 136 Step: 19040 Index:-0.3271 R2:0.6985 0.4972 0.4638 RMSE:0.6307 0.8292 0.8518 Tau:0.6469 0.5022 0.3420\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 137 Step: 19180 Index:-0.3558 R2:0.6928 0.4825 0.4692 RMSE:0.6533 0.8481 0.8550 Tau:0.6432 0.4923 0.3401\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 138 Step: 19320 Index:-0.3498 R2:0.7017 0.5101 0.4695 RMSE:0.6469 0.8562 0.8797 Tau:0.6460 0.5064 0.3470\n",
      "Epoch: 139 Step: 19460 Index:-0.2534 R2:0.6933 0.5401 0.4687 RMSE:0.6163 0.7891 0.8501 Tau:0.6455 0.5356 0.3151\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 140 Step: 19600 Index:-0.3319 R2:0.7102 0.5053 0.4620 RMSE:0.6175 0.8388 0.8748 Tau:0.6567 0.5069 0.3310\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 141 Step: 19740 Index:-0.3291 R2:0.7134 0.5085 0.4675 RMSE:0.6347 0.8479 0.8790 Tau:0.6599 0.5188 0.3318\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 142 Step: 19880 Index:-0.4084 R2:0.7122 0.5117 0.4724 RMSE:0.7072 0.9224 0.9432 Tau:0.6542 0.5141 0.3326\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 143 Step: 20020 Index:-0.4015 R2:0.7103 0.5039 0.4625 RMSE:0.7038 0.9131 0.9467 Tau:0.6548 0.5116 0.3328\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 144 Step: 20160 Index:-0.3106 R2:0.7014 0.5080 0.4663 RMSE:0.6097 0.8197 0.8623 Tau:0.6569 0.5091 0.3354\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 145 Step: 20300 Index:-0.2890 R2:0.7140 0.5153 0.4799 RMSE:0.5990 0.8123 0.8382 Tau:0.6584 0.5233 0.3434\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 146 Step: 20440 Index:-0.3294 R2:0.7218 0.5055 0.4593 RMSE:0.6296 0.8470 0.8784 Tau:0.6654 0.5177 0.3342\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 147 Step: 20580 Index:-0.2994 R2:0.7151 0.5109 0.4706 RMSE:0.5976 0.8209 0.8602 Tau:0.6564 0.5215 0.3294\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 148 Step: 20720 Index:-0.3522 R2:0.7117 0.4857 0.4613 RMSE:0.6140 0.8454 0.8649 Tau:0.6569 0.4932 0.3216\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 149 Step: 20860 Index:-0.2921 R2:0.7299 0.5173 0.4815 RMSE:0.5878 0.8149 0.8400 Tau:0.6703 0.5228 0.3300\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 150 Step: 21000 Index:-0.3029 R2:0.7292 0.5130 0.4723 RMSE:0.5837 0.8161 0.8518 Tau:0.6680 0.5132 0.3304\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 151 Step: 21140 Index:-0.3189 R2:0.7361 0.5030 0.4855 RMSE:0.5826 0.8316 0.8407 Tau:0.6716 0.5127 0.3216\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 152 Step: 21280 Index:-0.3093 R2:0.7289 0.5102 0.4833 RMSE:0.5984 0.8221 0.8438 Tau:0.6660 0.5127 0.3379\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 153 Step: 21420 Index:-0.4033 R2:0.7053 0.5078 0.4634 RMSE:0.7249 0.9272 0.9582 Tau:0.6537 0.5239 0.3225\n",
      "EarlyStopping counter: 15 out of 60\n",
      "Epoch: 154 Step: 21560 Index:-0.2892 R2:0.7265 0.5247 0.4778 RMSE:0.5800 0.8028 0.8524 Tau:0.6666 0.5136 0.3307\n",
      "EarlyStopping counter: 16 out of 60\n",
      "Epoch: 155 Step: 21700 Index:-0.3089 R2:0.7336 0.5135 0.4778 RMSE:0.5733 0.8149 0.8589 Tau:0.6728 0.5060 0.3367\n",
      "EarlyStopping counter: 17 out of 60\n",
      "Epoch: 156 Step: 21840 Index:-0.3100 R2:0.7397 0.5054 0.4820 RMSE:0.5805 0.8274 0.8442 Tau:0.6769 0.5174 0.3388\n",
      "EarlyStopping counter: 18 out of 60\n",
      "Epoch: 157 Step: 21980 Index:-0.2769 R2:0.7342 0.5237 0.4705 RMSE:0.5735 0.8035 0.8532 Tau:0.6759 0.5266 0.3261\n",
      "EarlyStopping counter: 19 out of 60\n",
      "Epoch: 158 Step: 22120 Index:-0.3483 R2:0.7384 0.4909 0.4620 RMSE:0.5982 0.8549 0.8912 Tau:0.6757 0.5067 0.3391\n",
      "EarlyStopping counter: 20 out of 60\n",
      "Epoch: 159 Step: 22260 Index:-0.3241 R2:0.7465 0.4945 0.4713 RMSE:0.5688 0.8278 0.8462 Tau:0.6814 0.5037 0.3394\n",
      "EarlyStopping counter: 21 out of 60\n",
      "Epoch: 160 Step: 22400 Index:-0.3139 R2:0.7472 0.5041 0.4651 RMSE:0.5683 0.8255 0.8640 Tau:0.6836 0.5116 0.3379\n",
      "EarlyStopping counter: 22 out of 60\n",
      "Epoch: 161 Step: 22540 Index:-0.2962 R2:0.7330 0.5112 0.4683 RMSE:0.5822 0.8125 0.8461 Tau:0.6695 0.5163 0.3375\n",
      "EarlyStopping counter: 23 out of 60\n",
      "Epoch: 162 Step: 22680 Index:-0.3800 R2:0.7448 0.5015 0.4614 RMSE:0.6347 0.8844 0.9191 Tau:0.6796 0.5044 0.3250\n",
      "EarlyStopping counter: 24 out of 60\n",
      "Epoch: 163 Step: 22820 Index:-0.3263 R2:0.7483 0.4991 0.4593 RMSE:0.5677 0.8312 0.8742 Tau:0.6841 0.5049 0.3432\n",
      "EarlyStopping counter: 25 out of 60\n",
      "Epoch: 164 Step: 22960 Index:-0.3380 R2:0.7528 0.4993 0.4718 RMSE:0.5663 0.8364 0.8684 Tau:0.6861 0.4983 0.3399\n",
      "EarlyStopping counter: 26 out of 60\n",
      "Epoch: 165 Step: 23100 Index:-0.3202 R2:0.7546 0.5168 0.4767 RMSE:0.6013 0.8318 0.8669 Tau:0.6864 0.5116 0.3336\n",
      "EarlyStopping counter: 27 out of 60\n",
      "Epoch: 166 Step: 23240 Index:-0.3611 R2:0.7594 0.4982 0.4745 RMSE:0.5672 0.8495 0.8776 Tau:0.6886 0.4885 0.3445\n",
      "EarlyStopping counter: 28 out of 60\n",
      "Epoch: 167 Step: 23380 Index:-0.3269 R2:0.7691 0.4990 0.4835 RMSE:0.5620 0.8335 0.8453 Tau:0.6976 0.5067 0.3412\n",
      "EarlyStopping counter: 29 out of 60\n",
      "Epoch: 168 Step: 23520 Index:-0.3320 R2:0.7605 0.4885 0.4594 RMSE:0.5451 0.8333 0.8626 Tau:0.6909 0.5013 0.3505\n",
      "EarlyStopping counter: 30 out of 60\n",
      "Epoch: 169 Step: 23660 Index:-0.3331 R2:0.7515 0.5179 0.4810 RMSE:0.5936 0.8517 0.8880 Tau:0.6829 0.5186 0.3290\n",
      "EarlyStopping counter: 31 out of 60\n",
      "Epoch: 170 Step: 23800 Index:-0.2669 R2:0.7692 0.5333 0.4726 RMSE:0.5376 0.7940 0.8508 Tau:0.6958 0.5271 0.3269\n",
      "EarlyStopping counter: 32 out of 60\n",
      "Epoch: 171 Step: 23940 Index:-0.3028 R2:0.7736 0.5128 0.4713 RMSE:0.5311 0.8166 0.8620 Tau:0.7002 0.5138 0.3344\n",
      "EarlyStopping counter: 33 out of 60\n",
      "Epoch: 172 Step: 24080 Index:-0.2816 R2:0.7636 0.5158 0.4668 RMSE:0.5493 0.8085 0.8499 Tau:0.7047 0.5269 0.3349\n",
      "EarlyStopping counter: 34 out of 60\n",
      "Epoch: 173 Step: 24220 Index:-0.2906 R2:0.7658 0.5205 0.4843 RMSE:0.5416 0.8128 0.8484 Tau:0.6949 0.5221 0.3273\n",
      "EarlyStopping counter: 35 out of 60\n",
      "Epoch: 174 Step: 24360 Index:-0.2908 R2:0.7460 0.5075 0.4701 RMSE:0.5664 0.8148 0.8465 Tau:0.6832 0.5239 0.3348\n",
      "EarlyStopping counter: 36 out of 60\n",
      "Epoch: 175 Step: 24500 Index:-0.3272 R2:0.7683 0.5008 0.4640 RMSE:0.5440 0.8285 0.8660 Tau:0.6983 0.5013 0.3483\n",
      "EarlyStopping counter: 37 out of 60\n",
      "Epoch: 176 Step: 24640 Index:-0.3335 R2:0.7758 0.4904 0.4722 RMSE:0.5412 0.8451 0.8652 Tau:0.7021 0.5116 0.3482\n",
      "EarlyStopping counter: 38 out of 60\n",
      "Epoch: 177 Step: 24780 Index:-0.2959 R2:0.7813 0.5158 0.4895 RMSE:0.5285 0.8114 0.8385 Tau:0.7071 0.5154 0.3352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 39 out of 60\n",
      "Epoch: 178 Step: 24920 Index:-0.3265 R2:0.7853 0.5035 0.4823 RMSE:0.5354 0.8354 0.8628 Tau:0.7080 0.5089 0.3284\n",
      "EarlyStopping counter: 40 out of 60\n",
      "Epoch: 179 Step: 25060 Index:-0.3198 R2:0.7814 0.5045 0.4763 RMSE:0.5278 0.8215 0.8511 Tau:0.7087 0.5017 0.3594\n",
      "EarlyStopping counter: 41 out of 60\n",
      "Epoch: 180 Step: 25200 Index:-0.2961 R2:0.7817 0.5182 0.4708 RMSE:0.5208 0.8120 0.8642 Tau:0.7084 0.5159 0.3471\n",
      "EarlyStopping counter: 42 out of 60\n",
      "Epoch: 181 Step: 25340 Index:-0.2789 R2:0.7774 0.5219 0.4680 RMSE:0.5281 0.8049 0.8627 Tau:0.7012 0.5260 0.3363\n",
      "EarlyStopping counter: 43 out of 60\n",
      "Epoch: 182 Step: 25480 Index:-0.3868 R2:0.7614 0.4826 0.4647 RMSE:0.5785 0.8719 0.8929 Tau:0.6947 0.4851 0.3544\n",
      "EarlyStopping counter: 44 out of 60\n",
      "Epoch: 183 Step: 25620 Index:-0.3332 R2:0.7845 0.4925 0.4817 RMSE:0.5295 0.8300 0.8411 Tau:0.7133 0.4968 0.3484\n",
      "EarlyStopping counter: 45 out of 60\n",
      "Epoch: 184 Step: 25760 Index:-0.2910 R2:0.7954 0.5115 0.4805 RMSE:0.5134 0.8132 0.8426 Tau:0.7180 0.5221 0.3312\n",
      "EarlyStopping counter: 46 out of 60\n",
      "Epoch: 185 Step: 25900 Index:-0.3141 R2:0.7833 0.5060 0.4778 RMSE:0.5196 0.8183 0.8507 Tau:0.7112 0.5042 0.3500\n",
      "EarlyStopping counter: 47 out of 60\n",
      "Epoch: 186 Step: 26040 Index:-0.3348 R2:0.7820 0.4928 0.4675 RMSE:0.5370 0.8311 0.8571 Tau:0.7064 0.4963 0.3504\n",
      "EarlyStopping counter: 48 out of 60\n",
      "Epoch: 187 Step: 26180 Index:-0.3157 R2:0.8024 0.5006 0.4654 RMSE:0.4988 0.8286 0.8683 Tau:0.7218 0.5129 0.3511\n",
      "EarlyStopping counter: 49 out of 60\n",
      "Epoch: 188 Step: 26320 Index:-0.3054 R2:0.7847 0.5150 0.4656 RMSE:0.5285 0.8248 0.8892 Tau:0.7109 0.5195 0.3424\n",
      "EarlyStopping counter: 50 out of 60\n",
      "Epoch: 189 Step: 26460 Index:-0.3436 R2:0.7959 0.5007 0.4713 RMSE:0.5166 0.8451 0.8934 Tau:0.7200 0.5015 0.3461\n",
      "EarlyStopping counter: 51 out of 60\n",
      "Epoch: 190 Step: 26600 Index:-0.3357 R2:0.7991 0.4834 0.4738 RMSE:0.5005 0.8419 0.8559 Tau:0.7184 0.5062 0.3625\n",
      "EarlyStopping counter: 52 out of 60\n",
      "Epoch: 191 Step: 26740 Index:-0.3575 R2:0.7745 0.4739 0.4618 RMSE:0.5370 0.8446 0.8569 Tau:0.6992 0.4871 0.3465\n",
      "EarlyStopping counter: 53 out of 60\n",
      "Epoch: 192 Step: 26880 Index:-0.3265 R2:0.7653 0.4868 0.4620 RMSE:0.5546 0.8412 0.8604 Tau:0.6991 0.5147 0.3406\n",
      "EarlyStopping counter: 54 out of 60\n",
      "Epoch: 193 Step: 27020 Index:-0.3980 R2:0.7883 0.4526 0.4481 RMSE:0.5220 0.8750 0.8900 Tau:0.7174 0.4770 0.3459\n",
      "EarlyStopping counter: 55 out of 60\n",
      "Epoch: 194 Step: 27160 Index:-0.3759 R2:0.8029 0.4644 0.4504 RMSE:0.5125 0.8617 0.8842 Tau:0.7234 0.4858 0.3254\n",
      "EarlyStopping counter: 56 out of 60\n",
      "Epoch: 195 Step: 27300 Index:-0.4004 R2:0.8083 0.4560 0.4560 RMSE:0.5126 0.8803 0.8956 Tau:0.7268 0.4799 0.3431\n",
      "EarlyStopping counter: 57 out of 60\n",
      "Epoch: 196 Step: 27440 Index:-0.3344 R2:0.8165 0.4888 0.4556 RMSE:0.4846 0.8410 0.8794 Tau:0.7351 0.5067 0.3517\n",
      "EarlyStopping counter: 58 out of 60\n",
      "Epoch: 197 Step: 27580 Index:-0.3466 R2:0.8213 0.4952 0.4741 RMSE:0.4826 0.8344 0.8571 Tau:0.7386 0.4878 0.3539\n",
      "EarlyStopping counter: 59 out of 60\n",
      "Epoch: 198 Step: 27720 Index:-0.3706 R2:0.8146 0.4762 0.4524 RMSE:0.4996 0.8521 0.8830 Tau:0.7324 0.4815 0.3548\n",
      "EarlyStopping counter: 60 out of 60\n",
      "Epoch: 199 Step: 27860 Index:-0.3819 R2:0.8131 0.4692 0.4465 RMSE:0.5180 0.8630 0.8920 Tau:0.7311 0.4811 0.3439\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 199 r2:0.4687 RMSE:0.8501 WTI:0.3188 AP:0.4544 Tau:0.3151 \n",
      " \n",
      " Top-1:0.4545 Top-1-fp:0.0000 \n",
      " Top-5:0.5536 Top-5-fp:0.2321 \n",
      " Top-10:0.5179 Top-10-fp:0.3750 \n",
      " Top-15:0.5119 Top-15-fp:0.4464 \n",
      " Top-20:0.5223 Top-20-fp:0.4777 \n",
      " Top-25:0.5440 Top-25-fp:0.5143 \n",
      " Top-30:0.5880 Top-30-fp:0.5625 \n",
      " Top-40:0.6800 Top-40-fp:0.6205 \n",
      " Top-50:0.7520 Top-50-fp:0.6643 \n",
      " \n",
      " Top50:0.5200 Top50-fp:0.2000 \n",
      " Top100:0.5300 Top100-fp:0.3300 \n",
      " Top150:0.5133 Top150-fp:0.4267 \n",
      " Top200:0.5200 Top200-fp:0.4650 \n",
      " Top250:0.5000 Top250-fp:0.5000 \n",
      " Top300:0.5640 Top300-fp:0.5300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
