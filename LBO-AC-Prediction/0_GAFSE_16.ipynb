{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC50_P35372_1_180\n",
      "model_file/0_GAFSE_EC50_P35372_1_180_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/EC50_P35372_1_180_train.csv\"\n",
    "test_filename = \"./data/benchmark/EC50_P35372_1_180_test.csv\"\n",
    "test_active = 180\n",
    "val_rate = 0.1\n",
    "random_seed = 2\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC1CN(CCC1(C)C2=CC(=CC=C2)O)CC(C(C)C)NC(=O)C3C... -2.079181\n",
      "1  CC1=CC=C(C=C1)C=CC(=O)N2CC3C(C2)C4(C=CC35C6CC7... -0.931966\n",
      "2   CC1(CCCCCCC1)N2CCC(CC2)N3C4=CC=CC=C4N=C3C5CCCNC5 -1.531479\n",
      "3         CC1C2CC3=C(C1(CCN2CC4CC4)C)C=C(C=C3)C(=O)N -0.431364\n",
      "4  C1CC1CN2CCC34C5C(CCC3(C2CC6=C4C(=C(C=C6)O)O5)O... -0.453318\n",
      "number of all smiles:  703\n",
      "number of successfully processed smiles:  703\n",
      "                                              smiles     value  \\\n",
      "0  CC1CN(CCC1(C)C2=CC(=CC=C2)O)CC(C(C)C)NC(=O)C3C... -2.079181   \n",
      "1  CC1=CC=C(C=C1)C=CC(=O)N2CC3C(C2)C4(C=CC35C6CC7... -0.931966   \n",
      "2   CC1(CCCCCCC1)N2CCC(CC2)N3C4=CC=CC=C4N=C3C5CCCNC5 -1.531479   \n",
      "3         CC1C2CC3=C(C1(CCN2CC4CC4)C)C=C(C=C3)C(=O)N -0.431364   \n",
      "4  C1CC1CN2CCC34C5C(CCC3(C2CC6=C4C(=C(C=C6)O)O5)O... -0.453318   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CC(C)C(CN1CCC(C)(c2cccc(O)c2)C(C)C1)NC(=O)C1Cc...  \n",
      "1  COC12C=CC3(C4CN(C(=O)C=Cc5ccc(C)cc5)CC41)C1Cc4...  \n",
      "2     CC1(N2CCC(n3c(C4CCCNC4)nc4ccccc43)CC2)CCCCCCC1  \n",
      "3              CC1C2Cc3ccc(C(N)=O)cc3C1(C)CCN2CC1CC1  \n",
      "4  Cl.O=C(CNC(=O)c1ccncc1)NC1CCC2(O)C3Cc4ccc(O)c5...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  982\n",
      "number of successfully processed smiles:  982\n",
      "(982, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CCN(C1)CCN2C3=CC=CC=C3N=C2NC(=O)C4=CC(=C(C=C... -3.255273   \n",
      "1  CC(C(=O)NC(CC1=CC=CC=C1)C(=O)NCC(=O)NC(CC2=CC=... -1.010088   \n",
      "2  COC12CCC3(CC1COCC4=CC=CC=C4)C5CC6=C7C3(C2OC7=C... -1.055760   \n",
      "3  C=CCN1CCC23C4C(=O)CCC2(C1CC5=C3C(=C(C=C5)OC(=O... -0.301030   \n",
      "4  C=CCN1CCC23C4C(=O)CCC2(C1CC5=C3C(=C(C=C5)OC(=O... -0.301030   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0       O=C(Nc1nc2ccccc2n1CCN1CCCC1)c1ccc(Cl)c(Cl)c1  \n",
      "1  CC(NC(=O)C(N)Cc1ccc(O)cc1)C(=O)NC(Cc1ccccc1)C(...  \n",
      "2  COC12CCC3(CC1COCc1ccccc1)C1Cc4ccc(F)c5c4C3(CCN...  \n",
      "3  C=CCN1CCC23c4c5ccc(OC(=O)CCCCCCCCC(=O)Oc6ccc7c...  \n",
      "4  C=CCN1CCC23c4c5ccc(OC(=O)CCCCCCCCC(=O)Oc6ccc7c...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/EC50_P35372_1_180_train.pickle\n",
      "./data/benchmark/EC50_P35372_1_180_train\n",
      "1685\n",
      "feature dicts file saved as ./data/benchmark/EC50_P35372_1_180_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 3) (70, 3) (982, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_EC50_P35372_1_180_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3335470/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 98 Index:0.0630 R2:0.0206 0.0630 0.0600 RMSE:1.3773 1.6952 1.2681 Tau:-0.0708 -0.1909 -0.2285\n",
      "Epoch: 2 Step: 196 Index:0.0647 R2:0.0718 0.0647 0.1306 RMSE:1.2888 1.5075 1.1619 Tau:0.2098 0.3077 0.1572\n",
      "Epoch: 3 Step: 294 Index:0.3064 R2:0.1891 0.3064 0.2415 RMSE:1.2787 1.5485 1.1199 Tau:0.3018 0.4046 0.2238\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 4 Step: 392 Index:0.2143 R2:0.1746 0.2143 0.2358 RMSE:1.2185 1.3889 1.0851 Tau:0.2881 0.3648 0.3474\n",
      "Epoch: 5 Step: 490 Index:0.3620 R2:0.2657 0.3620 0.3242 RMSE:1.1620 1.3664 0.9984 Tau:0.3625 0.4195 0.3496\n",
      "Epoch: 6 Step: 588 Index:0.3675 R2:0.2868 0.3675 0.3643 RMSE:1.1472 1.3723 0.9639 Tau:0.3890 0.4344 0.3762\n",
      "Epoch: 7 Step: 686 Index:0.4234 R2:0.3143 0.4234 0.3830 RMSE:1.1161 1.3182 0.9361 Tau:0.4031 0.4658 0.3759\n",
      "Epoch: 8 Step: 784 Index:0.4299 R2:0.3265 0.4299 0.3887 RMSE:1.0866 1.2618 0.9185 Tau:0.4074 0.4683 0.3872\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 9 Step: 882 Index:0.4136 R2:0.3265 0.4136 0.3913 RMSE:1.0844 1.2618 0.9168 Tau:0.4054 0.4617 0.3929\n",
      "Epoch: 10 Step: 980 Index:0.4452 R2:0.3450 0.4452 0.4123 RMSE:1.0811 1.2120 0.9541 Tau:0.4269 0.4865 0.3923\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 11 Step: 1078 Index:0.4290 R2:0.3478 0.4290 0.4037 RMSE:1.0606 1.2028 0.9205 Tau:0.4183 0.4692 0.4019\n",
      "Epoch: 12 Step: 1176 Index:0.4536 R2:0.3706 0.4536 0.4250 RMSE:1.0540 1.1725 0.9355 Tau:0.4387 0.4849 0.4045\n",
      "Epoch: 13 Step: 1274 Index:0.4756 R2:0.3831 0.4756 0.4304 RMSE:1.0392 1.2074 0.8853 Tau:0.4442 0.5023 0.4021\n",
      "Epoch: 14 Step: 1372 Index:0.4855 R2:0.3910 0.4855 0.4347 RMSE:1.0348 1.2120 0.8828 Tau:0.4552 0.5255 0.4065\n",
      "Epoch: 15 Step: 1470 Index:0.4967 R2:0.4025 0.4967 0.4477 RMSE:1.0220 1.1638 0.8933 Tau:0.4666 0.5288 0.4108\n",
      "Epoch: 16 Step: 1568 Index:0.5111 R2:0.4060 0.5111 0.4484 RMSE:1.0342 1.2181 0.8741 Tau:0.4674 0.5271 0.4018\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 17 Step: 1666 Index:0.4907 R2:0.4193 0.4907 0.4581 RMSE:1.0052 1.1569 0.8794 Tau:0.4670 0.4948 0.4119\n",
      "Epoch: 18 Step: 1764 Index:0.5285 R2:0.4259 0.5285 0.4631 RMSE:1.0244 1.1940 0.8706 Tau:0.4756 0.5246 0.4117\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 19 Step: 1862 Index:0.4865 R2:0.4236 0.4865 0.4602 RMSE:1.0330 1.2448 0.8728 Tau:0.4727 0.4940 0.4069\n",
      "Epoch: 20 Step: 1960 Index:0.5356 R2:0.4432 0.5356 0.4687 RMSE:1.0007 1.1773 0.8581 Tau:0.4875 0.5329 0.4142\n",
      "Epoch: 21 Step: 2058 Index:0.5500 R2:0.4526 0.5500 0.4797 RMSE:1.0191 1.2044 0.8635 Tau:0.4928 0.5362 0.4134\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 22 Step: 2156 Index:0.5471 R2:0.4569 0.5471 0.4825 RMSE:0.9903 1.1550 0.8521 Tau:0.4917 0.5130 0.4114\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 23 Step: 2254 Index:0.4811 R2:0.4164 0.4811 0.4519 RMSE:1.0175 1.1642 0.8992 Tau:0.4630 0.4965 0.4166\n",
      "Epoch: 24 Step: 2352 Index:0.5701 R2:0.4753 0.5701 0.4866 RMSE:0.9526 1.0648 0.8638 Tau:0.5053 0.5304 0.4185\n",
      "Epoch: 25 Step: 2450 Index:0.5723 R2:0.4813 0.5723 0.4900 RMSE:0.9483 1.0694 0.8567 Tau:0.5074 0.5346 0.4158\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 26 Step: 2548 Index:0.5370 R2:0.4754 0.5370 0.4945 RMSE:1.2229 1.4489 1.0526 Tau:0.5050 0.5039 0.4088\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 27 Step: 2646 Index:0.5605 R2:0.4991 0.5605 0.5029 RMSE:0.9324 1.0716 0.8457 Tau:0.5152 0.5081 0.4166\n",
      "Epoch: 28 Step: 2744 Index:0.5928 R2:0.5028 0.5928 0.5059 RMSE:0.9428 1.0983 0.8293 Tau:0.5267 0.5536 0.4207\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 29 Step: 2842 Index:0.5861 R2:0.5166 0.5861 0.5137 RMSE:0.9162 1.0579 0.8264 Tau:0.5297 0.5304 0.4179\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 30 Step: 2940 Index:0.5771 R2:0.5158 0.5771 0.5098 RMSE:0.9169 1.0189 0.8642 Tau:0.5253 0.5280 0.4209\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 31 Step: 3038 Index:0.5784 R2:0.5224 0.5784 0.5131 RMSE:0.9115 1.0156 0.8676 Tau:0.5268 0.5147 0.4167\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 32 Step: 3136 Index:0.5735 R2:0.5286 0.5735 0.5190 RMSE:0.9099 1.0618 0.8314 Tau:0.5333 0.5213 0.4118\n",
      "Epoch: 33 Step: 3234 Index:0.6088 R2:0.5448 0.6088 0.5281 RMSE:0.9435 1.1383 0.8235 Tau:0.5476 0.5536 0.4126\n",
      "Epoch: 34 Step: 3332 Index:0.6261 R2:0.5528 0.6261 0.5248 RMSE:0.8893 1.0188 0.8215 Tau:0.5494 0.5453 0.4190\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 35 Step: 3430 Index:0.6082 R2:0.5564 0.6082 0.5353 RMSE:0.8851 1.0141 0.8224 Tau:0.5543 0.5453 0.4189\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 36 Step: 3528 Index:0.6037 R2:0.5585 0.6037 0.5367 RMSE:0.8849 1.0383 0.8083 Tau:0.5502 0.5420 0.4202\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 37 Step: 3626 Index:0.6243 R2:0.5691 0.6243 0.5465 RMSE:0.8780 1.0376 0.7922 Tau:0.5592 0.5429 0.4121\n",
      "Epoch: 38 Step: 3724 Index:0.6449 R2:0.5703 0.6449 0.5420 RMSE:0.8780 1.0320 0.7964 Tau:0.5615 0.5644 0.4125\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 39 Step: 3822 Index:0.6136 R2:0.5599 0.6136 0.5273 RMSE:0.8982 1.0195 0.8521 Tau:0.5519 0.5437 0.4212\n",
      "Epoch: 40 Step: 3920 Index:0.6631 R2:0.5658 0.6631 0.5431 RMSE:0.9630 1.1443 0.8515 Tau:0.5745 0.6091 0.4206\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 41 Step: 4018 Index:0.6358 R2:0.5888 0.6358 0.5479 RMSE:0.8458 0.9507 0.8303 Tau:0.5690 0.5545 0.4182\n",
      "Epoch: 42 Step: 4116 Index:0.6666 R2:0.5731 0.6666 0.5357 RMSE:0.8648 0.9156 0.8549 Tau:0.5636 0.5718 0.4201\n",
      "Epoch: 43 Step: 4214 Index:0.6772 R2:0.5977 0.6772 0.5462 RMSE:0.8379 0.9473 0.8035 Tau:0.5746 0.5718 0.4146\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 44 Step: 4312 Index:0.6698 R2:0.6066 0.6698 0.5519 RMSE:0.8326 0.9022 0.8466 Tau:0.5778 0.5520 0.4160\n",
      "Epoch: 45 Step: 4410 Index:0.7019 R2:0.6217 0.7019 0.5658 RMSE:0.8182 0.9297 0.7818 Tau:0.5923 0.5876 0.4182\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 46 Step: 4508 Index:0.6958 R2:0.6164 0.6958 0.5570 RMSE:0.8422 0.9790 0.7943 Tau:0.5933 0.5983 0.4194\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 47 Step: 4606 Index:0.7016 R2:0.6290 0.7016 0.5606 RMSE:0.8110 0.9141 0.7922 Tau:0.5982 0.5776 0.4198\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 48 Step: 4704 Index:0.6729 R2:0.6046 0.6729 0.5486 RMSE:0.8315 0.9551 0.8037 Tau:0.5781 0.5636 0.4051\n",
      "Epoch: 49 Step: 4802 Index:0.7105 R2:0.6275 0.7105 0.5628 RMSE:0.8021 0.8716 0.7995 Tau:0.5951 0.5876 0.4201\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 50 Step: 4900 Index:0.6357 R2:0.6406 0.6357 0.5755 RMSE:0.7923 0.9511 0.7842 Tau:0.5989 0.5602 0.4151\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 51 Step: 4998 Index:0.6966 R2:0.6343 0.6966 0.5684 RMSE:0.8541 0.9955 0.8125 Tau:0.6092 0.5793 0.4216\n",
      "Epoch: 52 Step: 5096 Index:0.7243 R2:0.6469 0.7243 0.5699 RMSE:0.7838 0.8751 0.7868 Tau:0.6087 0.6099 0.4166\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 53 Step: 5194 Index:0.6824 R2:0.6109 0.6824 0.5590 RMSE:0.8425 0.9846 0.7836 Tau:0.5969 0.5867 0.4094\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 54 Step: 5292 Index:0.7048 R2:0.6541 0.7048 0.5772 RMSE:0.8073 0.9617 0.7740 Tau:0.6110 0.5884 0.4167\n",
      "Epoch: 55 Step: 5390 Index:0.7375 R2:0.6593 0.7375 0.5774 RMSE:0.7917 0.8849 0.7931 Tau:0.6167 0.6141 0.4193\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 56 Step: 5488 Index:0.7070 R2:0.6623 0.7070 0.5846 RMSE:0.7693 0.8774 0.7724 Tau:0.6167 0.5892 0.4220\n",
      "Epoch: 57 Step: 5586 Index:0.7442 R2:0.6666 0.7442 0.5753 RMSE:0.7764 0.8453 0.7960 Tau:0.6201 0.6017 0.4214\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 58 Step: 5684 Index:0.7239 R2:0.6566 0.7239 0.5649 RMSE:0.7704 0.8587 0.8030 Tau:0.6141 0.5992 0.4123\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 59 Step: 5782 Index:0.6724 R2:0.6640 0.6724 0.5748 RMSE:0.7634 0.8877 0.8073 Tau:0.6191 0.5810 0.4217\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 60 Step: 5880 Index:0.7364 R2:0.6862 0.7364 0.5790 RMSE:0.7862 0.9114 0.7840 Tau:0.6299 0.6008 0.4232\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 61 Step: 5978 Index:0.7418 R2:0.6760 0.7418 0.5805 RMSE:0.7962 0.8787 0.8063 Tau:0.6181 0.6008 0.4234\n",
      "Epoch: 62 Step: 6076 Index:0.7514 R2:0.6804 0.7514 0.5877 RMSE:0.7425 0.8126 0.7827 Tau:0.6281 0.6099 0.4198\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 63 Step: 6174 Index:0.7255 R2:0.6902 0.7255 0.5820 RMSE:0.7373 0.8507 0.7745 Tau:0.6333 0.5959 0.4166\n",
      "Epoch: 64 Step: 6272 Index:0.7705 R2:0.7011 0.7705 0.5988 RMSE:0.7409 0.8201 0.7625 Tau:0.6396 0.6240 0.4230\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 65 Step: 6370 Index:0.7146 R2:0.6856 0.7146 0.6080 RMSE:0.7725 0.9164 0.7553 Tau:0.6314 0.5959 0.4314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 66 Step: 6468 Index:0.7652 R2:0.6983 0.7652 0.5928 RMSE:0.7337 0.8221 0.7663 Tau:0.6361 0.6273 0.4203\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 67 Step: 6566 Index:0.7623 R2:0.6896 0.7623 0.5797 RMSE:0.7293 0.7712 0.8144 Tau:0.6277 0.6207 0.4227\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 68 Step: 6664 Index:0.7307 R2:0.7050 0.7307 0.6014 RMSE:0.7152 0.8216 0.7678 Tau:0.6348 0.5983 0.4201\n",
      "Epoch: 69 Step: 6762 Index:0.7720 R2:0.7041 0.7720 0.5851 RMSE:0.7229 0.7804 0.7815 Tau:0.6419 0.6166 0.4209\n",
      "Epoch: 70 Step: 6860 Index:0.7770 R2:0.7100 0.7770 0.5909 RMSE:0.7168 0.7650 0.7878 Tau:0.6397 0.6157 0.4233\n",
      "Epoch: 71 Step: 6958 Index:0.7814 R2:0.7024 0.7814 0.5882 RMSE:0.7416 0.7871 0.7970 Tau:0.6397 0.6306 0.4262\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 72 Step: 7056 Index:0.7591 R2:0.7034 0.7591 0.5836 RMSE:0.7382 0.7976 0.8097 Tau:0.6355 0.6008 0.4123\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 73 Step: 7154 Index:0.7437 R2:0.6953 0.7437 0.5895 RMSE:0.7732 0.8554 0.8238 Tau:0.6342 0.6157 0.4308\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 74 Step: 7252 Index:0.4233 R2:0.5280 0.4233 0.5208 RMSE:0.9838 1.1830 0.9126 Tau:0.5693 0.5296 0.4179\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 75 Step: 7350 Index:0.7469 R2:0.6972 0.7469 0.5899 RMSE:0.7246 0.8261 0.7734 Tau:0.6353 0.6091 0.4187\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 76 Step: 7448 Index:0.7576 R2:0.7091 0.7576 0.6069 RMSE:0.7156 0.8143 0.7510 Tau:0.6491 0.6116 0.4261\n",
      "Epoch: 77 Step: 7546 Index:0.7966 R2:0.7228 0.7966 0.6085 RMSE:0.7361 0.8071 0.7727 Tau:0.6547 0.6389 0.4284\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 78 Step: 7644 Index:0.7944 R2:0.7244 0.7944 0.6054 RMSE:0.6898 0.7267 0.7774 Tau:0.6524 0.6373 0.4236\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 79 Step: 7742 Index:0.7893 R2:0.7255 0.7893 0.5911 RMSE:0.6947 0.7520 0.7784 Tau:0.6480 0.6315 0.4151\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 80 Step: 7840 Index:0.7893 R2:0.7245 0.7893 0.5971 RMSE:0.6886 0.7276 0.7779 Tau:0.6483 0.6331 0.4274\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 81 Step: 7938 Index:0.7512 R2:0.7182 0.7512 0.5873 RMSE:0.7028 0.7727 0.8014 Tau:0.6449 0.6058 0.4174\n",
      "Epoch: 82 Step: 8036 Index:0.8154 R2:0.7389 0.8154 0.6104 RMSE:0.6842 0.7303 0.7587 Tau:0.6587 0.6571 0.4283\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 83 Step: 8134 Index:0.8120 R2:0.7433 0.8120 0.6138 RMSE:0.7553 0.8196 0.8025 Tau:0.6627 0.6654 0.4277\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 84 Step: 8232 Index:0.7929 R2:0.7468 0.7929 0.6044 RMSE:0.6691 0.7455 0.7585 Tau:0.6653 0.6323 0.4236\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 85 Step: 8330 Index:0.8154 R2:0.7382 0.8154 0.5862 RMSE:0.6946 0.7484 0.7844 Tau:0.6598 0.6538 0.4213\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 86 Step: 8428 Index:0.7860 R2:0.7373 0.7860 0.6142 RMSE:0.6736 0.7112 0.7977 Tau:0.6552 0.6207 0.4283\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 87 Step: 8526 Index:0.7588 R2:0.7291 0.7588 0.5924 RMSE:0.6973 0.7582 0.8369 Tau:0.6510 0.6017 0.4227\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 88 Step: 8624 Index:0.6925 R2:0.6803 0.6925 0.5747 RMSE:0.7532 0.9319 0.7820 Tau:0.6355 0.5925 0.4221\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 89 Step: 8722 Index:0.8079 R2:0.7478 0.8079 0.5966 RMSE:0.6702 0.6931 0.7969 Tau:0.6633 0.6406 0.4206\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 90 Step: 8820 Index:0.7753 R2:0.7516 0.7753 0.6085 RMSE:0.6576 0.7373 0.7722 Tau:0.6677 0.6282 0.4218\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 91 Step: 8918 Index:0.8074 R2:0.7562 0.8074 0.6212 RMSE:0.6919 0.7732 0.7541 Tau:0.6705 0.6588 0.4276\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 92 Step: 9016 Index:0.7976 R2:0.7463 0.7976 0.6044 RMSE:0.7397 0.8407 0.7880 Tau:0.6733 0.6522 0.4283\n",
      "Epoch: 93 Step: 9114 Index:0.8270 R2:0.7594 0.8270 0.6159 RMSE:0.6480 0.6723 0.7628 Tau:0.6740 0.6646 0.4263\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 94 Step: 9212 Index:0.8252 R2:0.7637 0.8252 0.6099 RMSE:0.7006 0.7577 0.7900 Tau:0.6753 0.6605 0.4249\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 95 Step: 9310 Index:0.8124 R2:0.7485 0.8124 0.6123 RMSE:0.7082 0.7367 0.7954 Tau:0.6632 0.6513 0.4260\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 96 Step: 9408 Index:0.7599 R2:0.7254 0.7599 0.5766 RMSE:0.7749 0.9383 0.8439 Tau:0.6565 0.6182 0.4175\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 97 Step: 9506 Index:0.8098 R2:0.7567 0.8098 0.6073 RMSE:0.6936 0.7565 0.7808 Tau:0.6787 0.6629 0.4291\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 98 Step: 9604 Index:0.8046 R2:0.7641 0.8046 0.5995 RMSE:0.6384 0.6934 0.7884 Tau:0.6734 0.6364 0.4268\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 99 Step: 9702 Index:0.8247 R2:0.7508 0.8247 0.6157 RMSE:0.6639 0.6833 0.7657 Tau:0.6765 0.6812 0.4370\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 100 Step: 9800 Index:0.8142 R2:0.7698 0.8142 0.6152 RMSE:0.6431 0.6760 0.7877 Tau:0.6812 0.6497 0.4286\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 101 Step: 9898 Index:0.7908 R2:0.7562 0.7908 0.6086 RMSE:0.6560 0.7348 0.7818 Tau:0.6665 0.6323 0.4307\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 102 Step: 9996 Index:0.8210 R2:0.7719 0.8210 0.6054 RMSE:0.7188 0.8135 0.8146 Tau:0.6848 0.6754 0.4240\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 103 Step: 10094 Index:0.8157 R2:0.7771 0.8157 0.6254 RMSE:0.6329 0.6991 0.7459 Tau:0.6834 0.6596 0.4263\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 104 Step: 10192 Index:0.8145 R2:0.7691 0.8145 0.6078 RMSE:0.6358 0.6914 0.7691 Tau:0.6829 0.6555 0.4271\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 105 Step: 10290 Index:0.8209 R2:0.7762 0.8209 0.6098 RMSE:0.6713 0.7389 0.7928 Tau:0.6830 0.6505 0.4249\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 106 Step: 10388 Index:0.8093 R2:0.7743 0.8093 0.6229 RMSE:0.6232 0.6722 0.7745 Tau:0.6808 0.6414 0.4290\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 107 Step: 10486 Index:0.8265 R2:0.7798 0.8265 0.6104 RMSE:0.6665 0.7443 0.7862 Tau:0.6914 0.6621 0.4277\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 108 Step: 10584 Index:0.8227 R2:0.7786 0.8227 0.6220 RMSE:0.6483 0.7302 0.7493 Tau:0.6896 0.6613 0.4339\n",
      "Epoch: 109 Step: 10682 Index:0.8381 R2:0.7853 0.8381 0.6208 RMSE:0.6107 0.6384 0.7544 Tau:0.6876 0.6704 0.4349\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 110 Step: 10780 Index:0.8262 R2:0.7871 0.8262 0.6352 RMSE:0.6081 0.6564 0.7380 Tau:0.6895 0.6663 0.4340\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 111 Step: 10878 Index:0.8348 R2:0.7819 0.8348 0.6129 RMSE:0.6188 0.6309 0.7846 Tau:0.6886 0.6737 0.4299\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 112 Step: 10976 Index:0.8286 R2:0.7733 0.8286 0.5794 RMSE:0.6248 0.6486 0.8001 Tau:0.6841 0.6513 0.4155\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 113 Step: 11074 Index:0.7978 R2:0.7549 0.7978 0.5648 RMSE:0.6525 0.7047 0.8250 Tau:0.6699 0.6207 0.4096\n",
      "Epoch: 114 Step: 11172 Index:0.8433 R2:0.7982 0.8433 0.6181 RMSE:0.5914 0.6153 0.7668 Tau:0.6990 0.6720 0.4277\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 115 Step: 11270 Index:0.8355 R2:0.7897 0.8355 0.5950 RMSE:0.6071 0.6297 0.7980 Tau:0.6899 0.6629 0.4232\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 116 Step: 11368 Index:0.8263 R2:0.7910 0.8263 0.6091 RMSE:0.6160 0.6934 0.7657 Tau:0.6947 0.6497 0.4280\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 117 Step: 11466 Index:0.8184 R2:0.7739 0.8184 0.6229 RMSE:0.6365 0.6810 0.7620 Tau:0.6854 0.6563 0.4382\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 118 Step: 11564 Index:0.8276 R2:0.7903 0.8276 0.6165 RMSE:0.6053 0.6484 0.7799 Tau:0.6947 0.6563 0.4299\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 119 Step: 11662 Index:0.8119 R2:0.7628 0.8119 0.6001 RMSE:0.6891 0.7519 0.7891 Tau:0.6867 0.6737 0.4295\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 120 Step: 11760 Index:0.8343 R2:0.8049 0.8343 0.6294 RMSE:0.5931 0.6567 0.7514 Tau:0.7040 0.6770 0.4337\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 121 Step: 11858 Index:0.8257 R2:0.8020 0.8257 0.6356 RMSE:0.5943 0.6529 0.7593 Tau:0.7034 0.6663 0.4368\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 122 Step: 11956 Index:0.8348 R2:0.8116 0.8348 0.6405 RMSE:0.5809 0.6496 0.7302 Tau:0.7081 0.6729 0.4344\n",
      "Epoch: 123 Step: 12054 Index:0.8476 R2:0.8086 0.8476 0.6233 RMSE:0.5869 0.6386 0.7388 Tau:0.7048 0.6903 0.4328\n",
      "Epoch: 124 Step: 12152 Index:0.8544 R2:0.8122 0.8544 0.6337 RMSE:0.6275 0.6745 0.7571 Tau:0.7132 0.7002 0.4355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 125 Step: 12250 Index:0.7981 R2:0.7655 0.7981 0.5619 RMSE:0.6654 0.7025 0.8348 Tau:0.6750 0.6257 0.4138\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 126 Step: 12348 Index:0.8224 R2:0.7921 0.8224 0.6066 RMSE:0.6038 0.6477 0.8039 Tau:0.6945 0.6422 0.4280\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 127 Step: 12446 Index:0.8424 R2:0.8175 0.8424 0.6402 RMSE:0.5731 0.6443 0.7315 Tau:0.7137 0.6845 0.4378\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 128 Step: 12544 Index:0.8457 R2:0.8088 0.8457 0.6235 RMSE:0.5804 0.6055 0.7799 Tau:0.7031 0.6696 0.4368\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 129 Step: 12642 Index:0.8468 R2:0.8103 0.8468 0.6140 RMSE:0.6197 0.6907 0.7907 Tau:0.7074 0.6687 0.4348\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 130 Step: 12740 Index:0.8451 R2:0.8132 0.8451 0.6357 RMSE:0.5701 0.6142 0.7510 Tau:0.7129 0.6803 0.4416\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 131 Step: 12838 Index:0.8302 R2:0.8203 0.8302 0.6277 RMSE:0.5612 0.6487 0.7426 Tau:0.7168 0.6588 0.4365\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 132 Step: 12936 Index:0.8309 R2:0.8196 0.8309 0.6421 RMSE:0.5582 0.6364 0.7341 Tau:0.7160 0.6605 0.4426\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 133 Step: 13034 Index:0.8408 R2:0.8179 0.8408 0.6253 RMSE:0.5703 0.6215 0.7503 Tau:0.7141 0.6903 0.4355\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 134 Step: 13132 Index:0.8304 R2:0.8078 0.8304 0.6116 RMSE:0.6069 0.6551 0.7976 Tau:0.7074 0.6522 0.4314\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 135 Step: 13230 Index:0.8493 R2:0.8258 0.8493 0.6287 RMSE:0.5515 0.6089 0.7604 Tau:0.7214 0.6836 0.4414\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 136 Step: 13328 Index:0.8300 R2:0.8169 0.8300 0.6079 RMSE:0.6214 0.7389 0.7906 Tau:0.7105 0.6720 0.4319\n",
      "Epoch: 137 Step: 13426 Index:0.8602 R2:0.8056 0.8602 0.6260 RMSE:0.5976 0.6166 0.7639 Tau:0.7083 0.6944 0.4462\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 138 Step: 13524 Index:0.8381 R2:0.8232 0.8381 0.6089 RMSE:0.5877 0.7027 0.7619 Tau:0.7177 0.6795 0.4348\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 139 Step: 13622 Index:0.8461 R2:0.8229 0.8461 0.6195 RMSE:0.5508 0.6034 0.7729 Tau:0.7167 0.6638 0.4360\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 140 Step: 13720 Index:0.8307 R2:0.8081 0.8307 0.6010 RMSE:0.5811 0.6333 0.7997 Tau:0.7061 0.6580 0.4327\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 141 Step: 13818 Index:0.8582 R2:0.8256 0.8582 0.6213 RMSE:0.6296 0.7070 0.8066 Tau:0.7187 0.6952 0.4436\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 142 Step: 13916 Index:0.8387 R2:0.8265 0.8387 0.6327 RMSE:0.5535 0.6481 0.7559 Tau:0.7239 0.6778 0.4455\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 143 Step: 14014 Index:0.8519 R2:0.8191 0.8519 0.6423 RMSE:0.5633 0.6090 0.7339 Tau:0.7220 0.6969 0.4474\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 144 Step: 14112 Index:0.8243 R2:0.7946 0.8243 0.5947 RMSE:0.6125 0.6629 0.7737 Tau:0.6966 0.6605 0.4397\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 145 Step: 14210 Index:0.8369 R2:0.8281 0.8369 0.6236 RMSE:0.5541 0.6513 0.7529 Tau:0.7262 0.6720 0.4427\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 146 Step: 14308 Index:0.8477 R2:0.8324 0.8477 0.6260 RMSE:0.5638 0.6528 0.7531 Tau:0.7274 0.6878 0.4467\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 147 Step: 14406 Index:0.8579 R2:0.8381 0.8579 0.6243 RMSE:0.5322 0.5821 0.7512 Tau:0.7340 0.7002 0.4439\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 148 Step: 14504 Index:0.8374 R2:0.8289 0.8374 0.6226 RMSE:0.5922 0.7106 0.7845 Tau:0.7251 0.6745 0.4407\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 149 Step: 14602 Index:0.8376 R2:0.8298 0.8376 0.6057 RMSE:0.5447 0.6288 0.7657 Tau:0.7247 0.6770 0.4344\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 150 Step: 14700 Index:0.8430 R2:0.8350 0.8430 0.6283 RMSE:0.5553 0.6531 0.7400 Tau:0.7354 0.6903 0.4441\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 151 Step: 14798 Index:0.8458 R2:0.8399 0.8458 0.6338 RMSE:0.5374 0.6218 0.7363 Tau:0.7375 0.6861 0.4485\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 152 Step: 14896 Index:0.8527 R2:0.8321 0.8527 0.6208 RMSE:0.5523 0.6176 0.7385 Tau:0.7342 0.6986 0.4426\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 153 Step: 14994 Index:0.8382 R2:0.8177 0.8382 0.6020 RMSE:0.5646 0.6341 0.8109 Tau:0.7198 0.6762 0.4359\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 154 Step: 15092 Index:0.8451 R2:0.8428 0.8451 0.6237 RMSE:0.5393 0.6143 0.7535 Tau:0.7356 0.6836 0.4436\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 155 Step: 15190 Index:0.8590 R2:0.8440 0.8590 0.6272 RMSE:0.5411 0.6176 0.7676 Tau:0.7362 0.6870 0.4479\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 156 Step: 15288 Index:0.8475 R2:0.8276 0.8475 0.6108 RMSE:0.5444 0.6103 0.7825 Tau:0.7183 0.6778 0.4464\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 157 Step: 15386 Index:0.8482 R2:0.8442 0.8482 0.6399 RMSE:0.5360 0.6273 0.7544 Tau:0.7396 0.6886 0.4512\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 158 Step: 15484 Index:0.8510 R2:0.8333 0.8510 0.6117 RMSE:0.5550 0.6298 0.7469 Tau:0.7388 0.6986 0.4435\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 159 Step: 15582 Index:0.8499 R2:0.8509 0.8499 0.6202 RMSE:0.5278 0.6242 0.7418 Tau:0.7436 0.6977 0.4425\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 160 Step: 15680 Index:0.8477 R2:0.8426 0.8477 0.6301 RMSE:0.5257 0.6120 0.7517 Tau:0.7342 0.6828 0.4471\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 161 Step: 15778 Index:0.8357 R2:0.8440 0.8357 0.6317 RMSE:0.5598 0.6844 0.7521 Tau:0.7439 0.6745 0.4509\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 162 Step: 15876 Index:0.8430 R2:0.8393 0.8430 0.6328 RMSE:0.5826 0.6887 0.7904 Tau:0.7335 0.6853 0.4506\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 163 Step: 15974 Index:0.8555 R2:0.8528 0.8555 0.6443 RMSE:0.5863 0.6993 0.7636 Tau:0.7451 0.6952 0.4504\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 164 Step: 16072 Index:0.8555 R2:0.8558 0.8555 0.6488 RMSE:0.5032 0.5902 0.7221 Tau:0.7534 0.7019 0.4566\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 165 Step: 16170 Index:0.8599 R2:0.8557 0.8599 0.6404 RMSE:0.4983 0.5758 0.7497 Tau:0.7491 0.7035 0.4550\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 166 Step: 16268 Index:0.8167 R2:0.8426 0.8167 0.6288 RMSE:0.5303 0.6611 0.7414 Tau:0.7365 0.6472 0.4477\n",
      "Epoch: 167 Step: 16366 Index:0.8666 R2:0.8580 0.8666 0.6315 RMSE:0.5167 0.5685 0.7412 Tau:0.7476 0.7101 0.4513\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 168 Step: 16464 Index:0.8500 R2:0.8496 0.8500 0.6316 RMSE:0.5373 0.6392 0.7743 Tau:0.7414 0.6944 0.4529\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 169 Step: 16562 Index:0.8277 R2:0.8365 0.8277 0.6419 RMSE:0.5494 0.6687 0.7888 Tau:0.7344 0.6497 0.4598\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 170 Step: 16660 Index:0.8410 R2:0.8582 0.8410 0.6462 RMSE:0.4951 0.6248 0.7565 Tau:0.7508 0.6812 0.4559\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 171 Step: 16758 Index:0.8296 R2:0.8381 0.8296 0.6119 RMSE:0.5401 0.6472 0.8221 Tau:0.7413 0.6754 0.4551\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 172 Step: 16856 Index:0.8560 R2:0.8493 0.8560 0.6199 RMSE:0.5479 0.6383 0.7923 Tau:0.7527 0.7077 0.4548\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 173 Step: 16954 Index:0.8362 R2:0.8379 0.8362 0.6110 RMSE:0.5497 0.6392 0.8359 Tau:0.7335 0.6580 0.4541\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 174 Step: 17052 Index:0.8365 R2:0.8448 0.8365 0.6313 RMSE:0.5209 0.6217 0.7560 Tau:0.7440 0.6845 0.4592\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 175 Step: 17150 Index:0.8426 R2:0.8618 0.8426 0.6359 RMSE:0.5019 0.6265 0.7290 Tau:0.7538 0.6977 0.4561\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 176 Step: 17248 Index:0.8644 R2:0.8550 0.8644 0.6309 RMSE:0.5015 0.5671 0.7616 Tau:0.7459 0.6952 0.4527\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 177 Step: 17346 Index:0.8530 R2:0.8588 0.8530 0.6340 RMSE:0.5446 0.6744 0.7610 Tau:0.7506 0.6952 0.4584\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 178 Step: 17444 Index:0.8631 R2:0.8244 0.8631 0.6155 RMSE:0.6176 0.6673 0.8218 Tau:0.7273 0.7060 0.4537\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 179 Step: 17542 Index:0.8655 R2:0.8573 0.8655 0.6239 RMSE:0.5036 0.5754 0.7411 Tau:0.7473 0.7176 0.4561\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 180 Step: 17640 Index:0.8490 R2:0.8683 0.8490 0.6433 RMSE:0.4996 0.6338 0.7427 Tau:0.7621 0.6977 0.4602\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 181 Step: 17738 Index:0.8537 R2:0.8653 0.8537 0.6404 RMSE:0.5033 0.6301 0.7559 Tau:0.7563 0.6994 0.4607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 182 Step: 17836 Index:0.8634 R2:0.8641 0.8634 0.6441 RMSE:0.4887 0.5780 0.7283 Tau:0.7560 0.7077 0.4582\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 183 Step: 17934 Index:0.8404 R2:0.8460 0.8404 0.6035 RMSE:0.5152 0.6250 0.8024 Tau:0.7427 0.6952 0.4621\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 184 Step: 18032 Index:0.8421 R2:0.8680 0.8421 0.6340 RMSE:0.5051 0.6615 0.7862 Tau:0.7595 0.7002 0.4587\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 185 Step: 18130 Index:0.8344 R2:0.8621 0.8344 0.6225 RMSE:0.4861 0.6441 0.7821 Tau:0.7551 0.7035 0.4595\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 186 Step: 18228 Index:0.8234 R2:0.8645 0.8234 0.6400 RMSE:0.5229 0.6726 0.7967 Tau:0.7595 0.6828 0.4635\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 187 Step: 18326 Index:0.8527 R2:0.8563 0.8527 0.6524 RMSE:0.5210 0.6403 0.7177 Tau:0.7531 0.6986 0.4652\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 188 Step: 18424 Index:0.8310 R2:0.8659 0.8310 0.6124 RMSE:0.4815 0.6418 0.7686 Tau:0.7601 0.6836 0.4609\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 189 Step: 18522 Index:0.8310 R2:0.8561 0.8310 0.6126 RMSE:0.5134 0.6709 0.8174 Tau:0.7517 0.7027 0.4611\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 190 Step: 18620 Index:0.8452 R2:0.8749 0.8452 0.6250 RMSE:0.4652 0.6220 0.7671 Tau:0.7647 0.7077 0.4602\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 191 Step: 18718 Index:0.8248 R2:0.8657 0.8248 0.6093 RMSE:0.5313 0.7110 0.8050 Tau:0.7620 0.7010 0.4553\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 192 Step: 18816 Index:0.8524 R2:0.8672 0.8524 0.6365 RMSE:0.4981 0.6211 0.7340 Tau:0.7640 0.7110 0.4615\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 193 Step: 18914 Index:0.8370 R2:0.8702 0.8370 0.6038 RMSE:0.4768 0.6487 0.7990 Tau:0.7651 0.6952 0.4593\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 194 Step: 19012 Index:0.8329 R2:0.8592 0.8329 0.6080 RMSE:0.4912 0.6398 0.7853 Tau:0.7531 0.6944 0.4574\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 195 Step: 19110 Index:0.7968 R2:0.8374 0.7968 0.5951 RMSE:0.5315 0.7426 0.8092 Tau:0.7409 0.6629 0.4502\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 196 Step: 19208 Index:0.8361 R2:0.8591 0.8361 0.5887 RMSE:0.4927 0.6513 0.8433 Tau:0.7506 0.7002 0.4614\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 197 Step: 19306 Index:0.7661 R2:0.8330 0.7661 0.5903 RMSE:0.6070 0.8507 0.8175 Tau:0.7444 0.6745 0.4594\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 197 r2:0.6315 RMSE:0.7412 WTI:0.3905 AP:0.5981 Tau:0.4513 \n",
      " \n",
      " Top-1:0.2222 Top-1-fp:0.1111 \n",
      " Top-5:0.6531 Top-5-fp:0.1429 \n",
      " Top-10:0.6633 Top-10-fp:0.3163 \n",
      " Top-15:0.6122 Top-15-fp:0.3878 \n",
      " Top-20:0.6167 Top-20-fp:0.4337 \n",
      " Top-25:0.7444 Top-25-fp:0.4531 \n",
      " Top-30:0.8278 Top-30-fp:0.4932 \n",
      " Top-40:0.9222 Top-40-fp:0.5765 \n",
      " Top-50:0.9722 Top-50-fp:0.6436 \n",
      " \n",
      " Top50:0.6400 Top50-fp:0.1400 \n",
      " Top100:0.6700 Top100-fp:0.3100 \n",
      " Top150:0.6067 Top150-fp:0.3933 \n",
      " Top200:0.6278 Top200-fp:0.4350 \n",
      " Top250:0.7500 Top250-fp:0.4600 \n",
      " Top300:0.8333 Top300-fp:0.5000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
