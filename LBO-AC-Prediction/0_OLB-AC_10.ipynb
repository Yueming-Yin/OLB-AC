{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC50_P34972_1_100\n",
      "model_file/0_GAFSE_IC50_P34972_1_100_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/IC50_P34972_1_100_train.csv\"\n",
    "test_filename = \"./data/benchmark/IC50_P34972_1_100_test.csv\"\n",
    "test_active = 100\n",
    "val_rate = 0.1\n",
    "random_seed = 5\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC(C(CC1=CC=C(C=C1)Cl)C2=CC=CC(=C2)C#N)NS(=O)(... -2.304706\n",
      "1  CCC1=C(NC2=CC=CC=C21)C(=O)NCCC3=CC=C(C=C3)N4CC... -3.952792\n",
      "2  CCCCCOC1=C(C=CC2=C1NC(=O)C(=C2)NC(=O)CC3=CC4=C... -1.300487\n",
      "3  CC(C)(C)C(=O)C1=C(C2=CC(=C(N=C2O1)C3=CC=CC=C3C... -3.278754\n",
      "4  CC1=C(N(N=C1C2=NC(=O)C(N2C)(C)C)C3=C(C=C(C=C3)... -3.024691\n",
      "number of all smiles:  805\n",
      "number of successfully processed smiles:  805\n",
      "                                              smiles     value  \\\n",
      "0  CC(C(CC1=CC=C(C=C1)Cl)C2=CC=CC(=C2)C#N)NS(=O)(... -2.304706   \n",
      "1  CCC1=C(NC2=CC=CC=C21)C(=O)NCCC3=CC=C(C=C3)N4CC... -3.952792   \n",
      "2  CCCCCOC1=C(C=CC2=C1NC(=O)C(=C2)NC(=O)CC3=CC4=C... -1.300487   \n",
      "3  CC(C)(C)C(=O)C1=C(C2=CC(=C(N=C2O1)C3=CC=CC=C3C... -3.278754   \n",
      "4  CC1=C(N(N=C1C2=NC(=O)C(N2C)(C)C)C3=C(C=C(C=C3)... -3.024691   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CC(NS(=O)(=O)c1cccc(Cl)c1Cl)C(Cc1ccc(Cl)cc1)c1...  \n",
      "1     CCc1c(C(=O)NCCc2ccc(N3CCCCC3)cc2)[nH]c2ccccc12  \n",
      "2  CCCCCOc1c(OC)ccc2cc(NC(=O)Cc3ccc4c(c3)OCO4)c(=...  \n",
      "3  CC(C)(C)C(=O)c1oc2nc(-c3ccccc3Cl)c(-c3ccc(Cl)c...  \n",
      "4  COc1ccc(-c2c(C)c(C3=NC(=O)C(C)(C)N3C)nn2-c2ccc...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.1\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  529\n",
      "number of successfully processed smiles:  529\n",
      "(529, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CC(C(=O)NC1CC(OC2=C1C=C(C(=N2)C3=C(C=C(C=C3)Cl... -3.676694   \n",
      "1  CCCCCCC(C)(C)C1=CC(=C(C(=C1)OC)C2=CC3CCC2(C3(C... -3.359835   \n",
      "2  CC(C)CC1COC2=CC=CC3=C2N1C=C(C3=O)C(=O)NC45CC6C... -1.556303   \n",
      "3  C1CC1CNS(=O)(=O)C2=C(C=C(C=C2)NC3=C(N=CC=C3)Cl... -1.980458   \n",
      "4  CC1(C(=O)N=C(N1C)C2=NN(C(=C2Br)C3=CC=C(C=C3)Cl... -3.196066   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CC(O)C(=O)NC1CC(C)(C)Oc2nc(-c3ccc(Cl)cc3Cl)c(-...  \n",
      "1  CCCCCCC(C)(C)c1cc(OC)c(C2=CC3CCC2(C(=O)O)C3(C)...  \n",
      "2  CC(C)CC1COc2cccc3c(=O)c(C(=O)NC45CC6CC(CC(C6)C...  \n",
      "3      O=S(=O)(NCC1CC1)c1ccc(Nc2cccnc2Cl)cc1C(F)(F)F  \n",
      "4  CN1C(c2nn(-c3ccc(Cl)cc3Cl)c(-c3ccc(Cl)cc3)c2Br...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/IC50_P34972_1_100_train.pickle\n",
      "./data/benchmark/IC50_P34972_1_100_train\n",
      "1334\n",
      "feature dicts file saved as ./data/benchmark/IC50_P34972_1_100_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(725, 3) (80, 3) (529, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=3e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=3e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=3e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler_AFSE = StepLR(optimizer_AFSE, step_size=10, gamma=0.9)\n",
    "scheduler_GRN = StepLR(optimizer_GRN, step_size=10, gamma=0.9)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 60\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_IC50_P34972_1_100_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3915426/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 72 Index:0.0395 R2:0.0478 0.0395 0.0480 RMSE:1.0405 0.9716 1.0516 Tau:0.1625 0.1282 -0.0096\n",
      "Epoch: 2 Step: 144 Index:0.0680 R2:0.0793 0.0680 0.0798 RMSE:1.0087 0.8939 0.9743 Tau:0.1993 0.1801 -0.0128\n",
      "Epoch: 3 Step: 216 Index:0.1327 R2:0.1474 0.1327 0.1374 RMSE:0.9714 0.8983 0.9839 Tau:0.2534 0.2295 -0.0225\n",
      "Epoch: 4 Step: 288 Index:0.1985 R2:0.2257 0.1985 0.1962 RMSE:0.9321 0.8604 0.9471 Tau:0.3104 0.2739 -0.0184\n",
      "Epoch: 5 Step: 360 Index:0.2256 R2:0.2700 0.2256 0.2283 RMSE:0.8847 0.8019 0.8885 Tau:0.3464 0.3062 -0.0066\n",
      "Epoch: 6 Step: 432 Index:0.2539 R2:0.3074 0.2539 0.2466 RMSE:0.8739 0.8052 0.8956 Tau:0.3774 0.3442 0.0122\n",
      "Epoch: 7 Step: 504 Index:0.2607 R2:0.3126 0.2607 0.2551 RMSE:0.9046 0.8050 0.8956 Tau:0.3825 0.3448 0.0230\n",
      "Epoch: 8 Step: 576 Index:0.2848 R2:0.3514 0.2848 0.2819 RMSE:0.9045 0.8577 0.9467 Tau:0.4102 0.3593 0.0431\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 9 Step: 648 Index:0.2790 R2:0.3615 0.2790 0.2865 RMSE:0.8654 0.7855 0.8612 Tau:0.4055 0.3530 0.0621\n",
      "Epoch: 10 Step: 720 Index:0.3093 R2:0.3785 0.3093 0.3068 RMSE:0.8379 0.7680 0.8496 Tau:0.4226 0.3682 0.0557\n",
      "Epoch: 11 Step: 792 Index:0.3164 R2:0.3907 0.3164 0.3188 RMSE:0.8068 0.7569 0.8397 Tau:0.4295 0.3682 0.0561\n",
      "Epoch: 12 Step: 864 Index:0.3256 R2:0.3994 0.3256 0.3291 RMSE:0.8439 0.7877 0.8589 Tau:0.4369 0.3802 0.0608\n",
      "Epoch: 13 Step: 936 Index:0.3281 R2:0.4259 0.3281 0.3382 RMSE:0.8044 0.7610 0.8512 Tau:0.4382 0.3790 0.0873\n",
      "Epoch: 14 Step: 1008 Index:0.3358 R2:0.4377 0.3358 0.3503 RMSE:0.7784 0.7421 0.8178 Tau:0.4463 0.3809 0.0844\n",
      "Epoch: 15 Step: 1080 Index:0.3409 R2:0.4482 0.3409 0.3593 RMSE:0.7693 0.7419 0.8130 Tau:0.4518 0.3904 0.0869\n",
      "Epoch: 16 Step: 1152 Index:0.3560 R2:0.4485 0.3560 0.3546 RMSE:0.7824 0.7398 0.8138 Tau:0.4527 0.3935 0.0838\n",
      "Epoch: 17 Step: 1224 Index:0.3563 R2:0.4577 0.3563 0.3670 RMSE:0.7713 0.7499 0.8312 Tau:0.4610 0.3992 0.0809\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 18 Step: 1296 Index:0.3374 R2:0.4683 0.3374 0.3749 RMSE:0.7905 0.7785 0.8600 Tau:0.4614 0.3714 0.0945\n",
      "Epoch: 19 Step: 1368 Index:0.3657 R2:0.4491 0.3657 0.3581 RMSE:0.8122 0.7617 0.8344 Tau:0.4589 0.4208 0.0778\n",
      "Epoch: 20 Step: 1440 Index:0.3729 R2:0.4763 0.3729 0.3628 RMSE:0.7527 0.7257 0.8202 Tau:0.4622 0.4049 0.1088\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 21 Step: 1512 Index:0.3693 R2:0.4759 0.3693 0.3838 RMSE:0.7482 0.7336 0.8026 Tau:0.4675 0.4119 0.0827\n",
      "Epoch: 22 Step: 1584 Index:0.3736 R2:0.4776 0.3736 0.3785 RMSE:0.7602 0.7323 0.7998 Tau:0.4711 0.4138 0.0886\n",
      "Epoch: 23 Step: 1656 Index:0.3754 R2:0.4811 0.3754 0.3763 RMSE:0.8222 0.7861 0.8378 Tau:0.4642 0.3929 0.0908\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 24 Step: 1728 Index:0.3709 R2:0.5011 0.3709 0.3910 RMSE:0.7402 0.7255 0.7875 Tau:0.4735 0.3980 0.1102\n",
      "Epoch: 25 Step: 1800 Index:0.3959 R2:0.5030 0.3959 0.3890 RMSE:0.7398 0.7172 0.8114 Tau:0.4836 0.4157 0.1162\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 26 Step: 1872 Index:0.3841 R2:0.5053 0.3841 0.4010 RMSE:0.7273 0.7219 0.7911 Tau:0.4794 0.4043 0.1069\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 27 Step: 1944 Index:0.3624 R2:0.5184 0.3624 0.3991 RMSE:0.7447 0.7531 0.7888 Tau:0.4781 0.3885 0.1292\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 28 Step: 2016 Index:0.3811 R2:0.5053 0.3811 0.4020 RMSE:0.7701 0.7551 0.8045 Tau:0.4865 0.4094 0.0945\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 29 Step: 2088 Index:0.3862 R2:0.5112 0.3862 0.3875 RMSE:0.7729 0.7531 0.8160 Tau:0.4882 0.4189 0.1121\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 30 Step: 2160 Index:0.3779 R2:0.5347 0.3779 0.4205 RMSE:0.7111 0.7265 0.7695 Tau:0.4959 0.4100 0.1263\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 31 Step: 2232 Index:0.3708 R2:0.5389 0.3708 0.4229 RMSE:0.7158 0.7446 0.8017 Tau:0.4989 0.4144 0.1457\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 32 Step: 2304 Index:0.3799 R2:0.5403 0.3799 0.4211 RMSE:0.7123 0.7258 0.7688 Tau:0.4942 0.4030 0.1577\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 33 Step: 2376 Index:0.3762 R2:0.5464 0.3762 0.4324 RMSE:0.6981 0.7371 0.7793 Tau:0.5040 0.4151 0.1519\n",
      "Epoch: 34 Step: 2448 Index:0.3973 R2:0.5491 0.3973 0.4270 RMSE:0.7113 0.7178 0.7663 Tau:0.5043 0.4233 0.1342\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 35 Step: 2520 Index:0.3822 R2:0.5558 0.3822 0.4403 RMSE:0.6972 0.7308 0.7582 Tau:0.5069 0.4151 0.1574\n",
      "Epoch: 36 Step: 2592 Index:0.3974 R2:0.5675 0.3974 0.4395 RMSE:0.7027 0.7302 0.7621 Tau:0.5200 0.4347 0.1770\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 37 Step: 2664 Index:0.3802 R2:0.5742 0.3802 0.4479 RMSE:0.6768 0.7263 0.7521 Tau:0.5204 0.4195 0.1854\n",
      "Epoch: 38 Step: 2736 Index:0.4017 R2:0.5833 0.4017 0.4449 RMSE:0.6952 0.7171 0.7861 Tau:0.5274 0.4296 0.2147\n",
      "Epoch: 39 Step: 2808 Index:0.4078 R2:0.5820 0.4078 0.4449 RMSE:0.6915 0.7039 0.7536 Tau:0.5321 0.4410 0.2082\n",
      "Epoch: 40 Step: 2880 Index:0.4282 R2:0.5675 0.4282 0.4410 RMSE:0.6933 0.7007 0.7594 Tau:0.5195 0.4505 0.1714\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 41 Step: 2952 Index:0.4241 R2:0.5897 0.4241 0.4579 RMSE:0.6708 0.7021 0.7460 Tau:0.5372 0.4531 0.1990\n",
      "Epoch: 42 Step: 3024 Index:0.4430 R2:0.5817 0.4430 0.4540 RMSE:0.6795 0.6892 0.7495 Tau:0.5344 0.4512 0.1783\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 43 Step: 3096 Index:0.4087 R2:0.5778 0.4087 0.4316 RMSE:0.7783 0.8030 0.8337 Tau:0.5421 0.4455 0.2185\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 44 Step: 3168 Index:0.4287 R2:0.6111 0.4287 0.4753 RMSE:0.6620 0.6921 0.7317 Tau:0.5446 0.4404 0.2142\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 45 Step: 3240 Index:0.4059 R2:0.6161 0.4059 0.4786 RMSE:0.6650 0.7174 0.7677 Tau:0.5560 0.4423 0.2286\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 46 Step: 3312 Index:0.4239 R2:0.6206 0.4239 0.4900 RMSE:0.6629 0.7138 0.7259 Tau:0.5486 0.4467 0.2299\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 47 Step: 3384 Index:0.4303 R2:0.6103 0.4303 0.4910 RMSE:0.6522 0.7141 0.7319 Tau:0.5479 0.4480 0.2102\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 48 Step: 3456 Index:0.4315 R2:0.6265 0.4315 0.5046 RMSE:0.6480 0.6886 0.7120 Tau:0.5649 0.4594 0.2393\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 49 Step: 3528 Index:0.4275 R2:0.6287 0.4275 0.5033 RMSE:0.6308 0.6952 0.7136 Tau:0.5669 0.4550 0.2259\n",
      "Epoch: 50 Step: 3600 Index:0.4590 R2:0.6293 0.4590 0.4939 RMSE:0.6363 0.6703 0.7178 Tau:0.5630 0.4708 0.2226\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 51 Step: 3672 Index:0.4212 R2:0.6306 0.4212 0.5023 RMSE:0.6617 0.7350 0.7266 Tau:0.5657 0.4505 0.2316\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 52 Step: 3744 Index:0.4319 R2:0.6348 0.4319 0.4885 RMSE:0.6265 0.6928 0.7242 Tau:0.5610 0.4512 0.2628\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 53 Step: 3816 Index:0.4098 R2:0.6374 0.4098 0.4981 RMSE:0.6406 0.7070 0.7162 Tau:0.5610 0.4315 0.2634\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 54 Step: 3888 Index:0.4443 R2:0.6442 0.4443 0.5137 RMSE:0.6564 0.7128 0.7202 Tau:0.5697 0.4455 0.2300\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 55 Step: 3960 Index:0.4360 R2:0.6539 0.4360 0.5291 RMSE:0.6658 0.7512 0.7278 Tau:0.5831 0.4569 0.2376\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 56 Step: 4032 Index:0.4291 R2:0.6510 0.4291 0.5084 RMSE:0.6260 0.6956 0.7085 Tau:0.5796 0.4486 0.2639\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 57 Step: 4104 Index:0.4237 R2:0.6672 0.4237 0.5293 RMSE:0.6563 0.7310 0.7165 Tau:0.5972 0.4600 0.2738\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 58 Step: 4176 Index:0.4487 R2:0.6559 0.4487 0.5161 RMSE:0.6071 0.6939 0.7102 Tau:0.5888 0.4727 0.2474\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 59 Step: 4248 Index:0.4565 R2:0.6759 0.4565 0.5220 RMSE:0.5940 0.6710 0.6997 Tau:0.6023 0.4759 0.2772\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 60 Step: 4320 Index:0.4416 R2:0.6730 0.4416 0.5379 RMSE:0.5934 0.6938 0.6964 Tau:0.5973 0.4683 0.2781\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 61 Step: 4392 Index:0.4450 R2:0.6814 0.4450 0.5469 RMSE:0.6331 0.7061 0.7348 Tau:0.6033 0.4683 0.2623\n",
      "Epoch: 62 Step: 4464 Index:0.4683 R2:0.6809 0.4683 0.5365 RMSE:0.5931 0.6620 0.6904 Tau:0.6002 0.4803 0.2671\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 63 Step: 4536 Index:0.4616 R2:0.6767 0.4616 0.5382 RMSE:0.6032 0.7036 0.6988 Tau:0.6013 0.4727 0.2537\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 64 Step: 4608 Index:0.4510 R2:0.6851 0.4510 0.5381 RMSE:0.6068 0.6910 0.6954 Tau:0.6117 0.4752 0.2756\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 65 Step: 4680 Index:0.4380 R2:0.6858 0.4380 0.5510 RMSE:0.5810 0.6936 0.6768 Tau:0.6105 0.4714 0.2750\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 66 Step: 4752 Index:0.4392 R2:0.6982 0.4392 0.5552 RMSE:0.5741 0.6879 0.6848 Tau:0.6227 0.4714 0.2878\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 67 Step: 4824 Index:0.4520 R2:0.7006 0.4520 0.5436 RMSE:0.5867 0.6916 0.6887 Tau:0.6269 0.4784 0.2953\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 68 Step: 4896 Index:0.4433 R2:0.6886 0.4433 0.5502 RMSE:0.5881 0.7034 0.6967 Tau:0.6152 0.4721 0.2676\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 69 Step: 4968 Index:0.4552 R2:0.7090 0.4552 0.5510 RMSE:0.5782 0.6884 0.6834 Tau:0.6317 0.4759 0.2885\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 70 Step: 5040 Index:0.4401 R2:0.7087 0.4401 0.5423 RMSE:0.5783 0.6995 0.6876 Tau:0.6257 0.4575 0.2877\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 71 Step: 5112 Index:0.4541 R2:0.6814 0.4541 0.5344 RMSE:0.6625 0.7506 0.7395 Tau:0.6017 0.4632 0.2591\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 72 Step: 5184 Index:0.4269 R2:0.7144 0.4269 0.5490 RMSE:0.5903 0.7076 0.7163 Tau:0.6398 0.4759 0.3029\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 73 Step: 5256 Index:0.4381 R2:0.7152 0.4381 0.5398 RMSE:0.5717 0.7112 0.6934 Tau:0.6400 0.4645 0.3126\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 74 Step: 5328 Index:0.4300 R2:0.7142 0.4300 0.5630 RMSE:0.5672 0.7377 0.6825 Tau:0.6328 0.4594 0.2886\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 75 Step: 5400 Index:0.4581 R2:0.7274 0.4581 0.5478 RMSE:0.5462 0.6736 0.6785 Tau:0.6459 0.4892 0.3099\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 76 Step: 5472 Index:0.4597 R2:0.7270 0.4597 0.5609 RMSE:0.5577 0.6747 0.6728 Tau:0.6386 0.4752 0.2876\n",
      "Epoch: 77 Step: 5544 Index:0.4740 R2:0.7227 0.4740 0.5643 RMSE:0.5534 0.6803 0.6842 Tau:0.6338 0.4828 0.2728\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 78 Step: 5616 Index:0.4516 R2:0.7347 0.4516 0.5691 RMSE:0.5331 0.6865 0.6639 Tau:0.6516 0.4702 0.2935\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 79 Step: 5688 Index:0.4379 R2:0.7347 0.4379 0.5621 RMSE:0.5648 0.7195 0.6889 Tau:0.6541 0.4664 0.3060\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 80 Step: 5760 Index:0.4574 R2:0.7335 0.4574 0.5627 RMSE:0.5343 0.6890 0.6747 Tau:0.6522 0.4873 0.3141\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 81 Step: 5832 Index:0.4594 R2:0.7409 0.4594 0.5588 RMSE:0.5288 0.6821 0.6778 Tau:0.6508 0.4892 0.3093\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 82 Step: 5904 Index:0.4391 R2:0.7353 0.4391 0.5602 RMSE:0.5394 0.6965 0.6802 Tau:0.6460 0.4670 0.2858\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 83 Step: 5976 Index:0.4512 R2:0.7490 0.4512 0.5631 RMSE:0.5253 0.6830 0.6684 Tau:0.6623 0.4835 0.3145\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 84 Step: 6048 Index:0.4733 R2:0.7471 0.4733 0.5634 RMSE:0.5220 0.6723 0.6683 Tau:0.6553 0.4778 0.2968\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 85 Step: 6120 Index:0.4145 R2:0.7482 0.4145 0.5567 RMSE:0.5246 0.7148 0.6737 Tau:0.6643 0.4702 0.3102\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 86 Step: 6192 Index:0.4538 R2:0.7533 0.4538 0.5395 RMSE:0.5217 0.6736 0.6853 Tau:0.6704 0.4733 0.3254\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 87 Step: 6264 Index:0.4730 R2:0.7578 0.4730 0.5772 RMSE:0.5171 0.6735 0.6653 Tau:0.6656 0.4942 0.3148\n",
      "Epoch: 88 Step: 6336 Index:0.4742 R2:0.7635 0.4742 0.5505 RMSE:0.5436 0.6835 0.6964 Tau:0.6685 0.4771 0.3159\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 89 Step: 6408 Index:0.4331 R2:0.7470 0.4331 0.5562 RMSE:0.5234 0.7060 0.6762 Tau:0.6644 0.4695 0.3114\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 90 Step: 6480 Index:0.4530 R2:0.7583 0.4530 0.5652 RMSE:0.5111 0.6855 0.6670 Tau:0.6638 0.4816 0.2959\n",
      "Epoch: 91 Step: 6552 Index:0.4757 R2:0.7668 0.4757 0.5582 RMSE:0.5369 0.6793 0.6878 Tau:0.6719 0.4822 0.3152\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 92 Step: 6624 Index:0.4499 R2:0.7711 0.4499 0.5592 RMSE:0.5111 0.6872 0.6855 Tau:0.6774 0.4708 0.3215\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 93 Step: 6696 Index:0.4673 R2:0.7675 0.4673 0.5724 RMSE:0.5019 0.6765 0.6612 Tau:0.6735 0.4860 0.3193\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 94 Step: 6768 Index:0.4428 R2:0.7661 0.4428 0.5559 RMSE:0.5325 0.6944 0.6843 Tau:0.6747 0.4714 0.3245\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 95 Step: 6840 Index:0.4606 R2:0.7608 0.4606 0.5540 RMSE:0.5146 0.7089 0.6916 Tau:0.6717 0.4714 0.3197\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 96 Step: 6912 Index:0.4513 R2:0.7742 0.4513 0.5544 RMSE:0.5164 0.6990 0.6886 Tau:0.6816 0.4702 0.3356\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 97 Step: 6984 Index:0.4447 R2:0.7638 0.4447 0.5593 RMSE:0.5127 0.6996 0.6773 Tau:0.6720 0.4695 0.3085\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 98 Step: 7056 Index:0.4430 R2:0.7831 0.4430 0.5619 RMSE:0.4989 0.6920 0.6733 Tau:0.6856 0.4771 0.3385\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 99 Step: 7128 Index:0.4616 R2:0.7814 0.4616 0.5511 RMSE:0.4844 0.6786 0.6774 Tau:0.6889 0.4828 0.3457\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 100 Step: 7200 Index:0.4569 R2:0.7867 0.4569 0.5642 RMSE:0.4870 0.6832 0.6698 Tau:0.6904 0.4771 0.3334\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 101 Step: 7272 Index:0.4470 R2:0.7803 0.4470 0.5469 RMSE:0.4847 0.6942 0.6832 Tau:0.6865 0.4759 0.3332\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 102 Step: 7344 Index:0.4531 R2:0.7807 0.4531 0.5561 RMSE:0.5219 0.7012 0.6936 Tau:0.6861 0.4721 0.3376\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 103 Step: 7416 Index:0.4607 R2:0.7890 0.4607 0.5536 RMSE:0.4799 0.6759 0.6753 Tau:0.6929 0.4885 0.3362\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 104 Step: 7488 Index:0.4686 R2:0.7888 0.4686 0.5594 RMSE:0.4915 0.6879 0.6815 Tau:0.6901 0.4746 0.3338\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 105 Step: 7560 Index:0.4522 R2:0.7845 0.4522 0.5646 RMSE:0.4813 0.6842 0.6663 Tau:0.6816 0.4702 0.3227\n",
      "EarlyStopping counter: 15 out of 60\n",
      "Epoch: 106 Step: 7632 Index:0.4601 R2:0.7910 0.4601 0.5575 RMSE:0.5105 0.6975 0.6993 Tau:0.6928 0.4860 0.3325\n",
      "EarlyStopping counter: 16 out of 60\n",
      "Epoch: 107 Step: 7704 Index:0.4489 R2:0.7930 0.4489 0.5646 RMSE:0.4917 0.6978 0.6805 Tau:0.6970 0.4689 0.3307\n",
      "EarlyStopping counter: 17 out of 60\n",
      "Epoch: 108 Step: 7776 Index:0.4528 R2:0.7954 0.4528 0.5585 RMSE:0.4803 0.6815 0.6757 Tau:0.6941 0.4936 0.3325\n",
      "EarlyStopping counter: 18 out of 60\n",
      "Epoch: 109 Step: 7848 Index:0.4421 R2:0.7992 0.4421 0.5367 RMSE:0.5001 0.6947 0.7002 Tau:0.7003 0.4828 0.3499\n",
      "EarlyStopping counter: 19 out of 60\n",
      "Epoch: 110 Step: 7920 Index:0.4660 R2:0.7966 0.4660 0.5366 RMSE:0.4729 0.6695 0.6870 Tau:0.6979 0.4860 0.3522\n",
      "EarlyStopping counter: 20 out of 60\n",
      "Epoch: 111 Step: 7992 Index:0.4418 R2:0.7949 0.4418 0.5647 RMSE:0.4735 0.7044 0.6725 Tau:0.6961 0.4702 0.3318\n",
      "EarlyStopping counter: 21 out of 60\n",
      "Epoch: 112 Step: 8064 Index:0.4641 R2:0.7978 0.4641 0.5267 RMSE:0.4897 0.6796 0.7089 Tau:0.7040 0.4828 0.3512\n",
      "EarlyStopping counter: 22 out of 60\n",
      "Epoch: 113 Step: 8136 Index:0.4433 R2:0.7929 0.4433 0.5517 RMSE:0.4827 0.7058 0.6903 Tau:0.6947 0.4645 0.3344\n",
      "EarlyStopping counter: 23 out of 60\n",
      "Epoch: 114 Step: 8208 Index:0.4532 R2:0.7993 0.4532 0.5493 RMSE:0.4992 0.6788 0.6913 Tau:0.6998 0.4708 0.3308\n",
      "EarlyStopping counter: 24 out of 60\n",
      "Epoch: 115 Step: 8280 Index:0.4599 R2:0.8093 0.4599 0.5564 RMSE:0.4641 0.6795 0.6784 Tau:0.7082 0.4885 0.3424\n",
      "EarlyStopping counter: 25 out of 60\n",
      "Epoch: 116 Step: 8352 Index:0.4609 R2:0.8075 0.4609 0.5524 RMSE:0.4601 0.6743 0.6757 Tau:0.7088 0.4765 0.3444\n",
      "EarlyStopping counter: 26 out of 60\n",
      "Epoch: 117 Step: 8424 Index:0.4754 R2:0.8038 0.4754 0.5556 RMSE:0.4615 0.6798 0.6808 Tau:0.7036 0.4917 0.3459\n",
      "EarlyStopping counter: 27 out of 60\n",
      "Epoch: 118 Step: 8496 Index:0.4340 R2:0.8053 0.4340 0.5369 RMSE:0.4663 0.6946 0.6894 Tau:0.7116 0.4683 0.3513\n",
      "EarlyStopping counter: 28 out of 60\n",
      "Epoch: 119 Step: 8568 Index:0.4545 R2:0.8068 0.4545 0.5515 RMSE:0.4717 0.6983 0.6881 Tau:0.7053 0.4740 0.3378\n",
      "EarlyStopping counter: 29 out of 60\n",
      "Epoch: 120 Step: 8640 Index:0.4491 R2:0.8068 0.4491 0.5512 RMSE:0.4548 0.6923 0.6797 Tau:0.7050 0.4797 0.3380\n",
      "EarlyStopping counter: 30 out of 60\n",
      "Epoch: 121 Step: 8712 Index:0.4687 R2:0.8088 0.4687 0.5498 RMSE:0.4591 0.6836 0.6853 Tau:0.7046 0.4847 0.3409\n",
      "EarlyStopping counter: 31 out of 60\n",
      "Epoch: 122 Step: 8784 Index:0.4561 R2:0.8137 0.4561 0.5493 RMSE:0.4572 0.6832 0.6818 Tau:0.7087 0.4771 0.3434\n",
      "EarlyStopping counter: 32 out of 60\n",
      "Epoch: 123 Step: 8856 Index:0.4636 R2:0.8148 0.4636 0.5614 RMSE:0.4478 0.6821 0.6725 Tau:0.7112 0.4778 0.3447\n",
      "Epoch: 124 Step: 8928 Index:0.4882 R2:0.8092 0.4882 0.5547 RMSE:0.4582 0.6760 0.6843 Tau:0.7071 0.4835 0.3353\n",
      "EarlyStopping counter: 1 out of 60\n",
      "Epoch: 125 Step: 9000 Index:0.4768 R2:0.8143 0.4768 0.5519 RMSE:0.4467 0.6742 0.6804 Tau:0.7120 0.4854 0.3424\n",
      "EarlyStopping counter: 2 out of 60\n",
      "Epoch: 126 Step: 9072 Index:0.4587 R2:0.8085 0.4587 0.5236 RMSE:0.4700 0.6875 0.7139 Tau:0.7109 0.4765 0.3501\n",
      "EarlyStopping counter: 3 out of 60\n",
      "Epoch: 127 Step: 9144 Index:0.4797 R2:0.8174 0.4797 0.5523 RMSE:0.4597 0.6776 0.6864 Tau:0.7123 0.4949 0.3394\n",
      "EarlyStopping counter: 4 out of 60\n",
      "Epoch: 128 Step: 9216 Index:0.4286 R2:0.8184 0.4286 0.5347 RMSE:0.4540 0.6962 0.6923 Tau:0.7180 0.4651 0.3423\n",
      "EarlyStopping counter: 5 out of 60\n",
      "Epoch: 129 Step: 9288 Index:0.4769 R2:0.8210 0.4769 0.5624 RMSE:0.4471 0.6691 0.6757 Tau:0.7169 0.4854 0.3381\n",
      "EarlyStopping counter: 6 out of 60\n",
      "Epoch: 130 Step: 9360 Index:0.4433 R2:0.8054 0.4433 0.5447 RMSE:0.4765 0.7085 0.6984 Tau:0.7055 0.4702 0.3374\n",
      "EarlyStopping counter: 7 out of 60\n",
      "Epoch: 131 Step: 9432 Index:0.4717 R2:0.8220 0.4717 0.5525 RMSE:0.4387 0.6706 0.6771 Tau:0.7187 0.4930 0.3522\n",
      "EarlyStopping counter: 8 out of 60\n",
      "Epoch: 132 Step: 9504 Index:0.4717 R2:0.8244 0.4717 0.5499 RMSE:0.4353 0.6732 0.6789 Tau:0.7194 0.4847 0.3466\n",
      "EarlyStopping counter: 9 out of 60\n",
      "Epoch: 133 Step: 9576 Index:0.4605 R2:0.8214 0.4605 0.5611 RMSE:0.4521 0.6902 0.6818 Tau:0.7159 0.4822 0.3440\n",
      "EarlyStopping counter: 10 out of 60\n",
      "Epoch: 134 Step: 9648 Index:0.4616 R2:0.8258 0.4616 0.5446 RMSE:0.4380 0.6783 0.6856 Tau:0.7224 0.4771 0.3518\n",
      "EarlyStopping counter: 11 out of 60\n",
      "Epoch: 135 Step: 9720 Index:0.4448 R2:0.8239 0.4448 0.5637 RMSE:0.4388 0.6956 0.6698 Tau:0.7180 0.4822 0.3404\n",
      "EarlyStopping counter: 12 out of 60\n",
      "Epoch: 136 Step: 9792 Index:0.4420 R2:0.8298 0.4420 0.5566 RMSE:0.4396 0.6981 0.6817 Tau:0.7253 0.4771 0.3550\n",
      "EarlyStopping counter: 13 out of 60\n",
      "Epoch: 137 Step: 9864 Index:0.4501 R2:0.8239 0.4501 0.5581 RMSE:0.4370 0.6919 0.6739 Tau:0.7170 0.4784 0.3384\n",
      "EarlyStopping counter: 14 out of 60\n",
      "Epoch: 138 Step: 9936 Index:0.4437 R2:0.8270 0.4437 0.5363 RMSE:0.4377 0.6881 0.6885 Tau:0.7269 0.4765 0.3532\n",
      "EarlyStopping counter: 15 out of 60\n",
      "Epoch: 139 Step: 10008 Index:0.4518 R2:0.8184 0.4518 0.5357 RMSE:0.4501 0.7019 0.6977 Tau:0.7186 0.4708 0.3472\n",
      "EarlyStopping counter: 16 out of 60\n",
      "Epoch: 140 Step: 10080 Index:0.4725 R2:0.8237 0.4725 0.5489 RMSE:0.4340 0.6809 0.6833 Tau:0.7194 0.4866 0.3477\n",
      "EarlyStopping counter: 17 out of 60\n",
      "Epoch: 141 Step: 10152 Index:0.4631 R2:0.8293 0.4631 0.5319 RMSE:0.4523 0.6792 0.7035 Tau:0.7243 0.4961 0.3496\n",
      "EarlyStopping counter: 18 out of 60\n",
      "Epoch: 142 Step: 10224 Index:0.4791 R2:0.8261 0.4791 0.5517 RMSE:0.4323 0.6831 0.6842 Tau:0.7204 0.4866 0.3428\n",
      "EarlyStopping counter: 19 out of 60\n",
      "Epoch: 143 Step: 10296 Index:0.4501 R2:0.8291 0.4501 0.5413 RMSE:0.4300 0.6875 0.6879 Tau:0.7281 0.4765 0.3534\n",
      "EarlyStopping counter: 20 out of 60\n",
      "Epoch: 144 Step: 10368 Index:0.4542 R2:0.8304 0.4542 0.5558 RMSE:0.4281 0.6909 0.6794 Tau:0.7251 0.4790 0.3443\n",
      "EarlyStopping counter: 21 out of 60\n",
      "Epoch: 145 Step: 10440 Index:0.4674 R2:0.8271 0.4674 0.5587 RMSE:0.4338 0.6827 0.6739 Tau:0.7231 0.4879 0.3473\n",
      "EarlyStopping counter: 22 out of 60\n",
      "Epoch: 146 Step: 10512 Index:0.4648 R2:0.8323 0.4648 0.5593 RMSE:0.4314 0.6900 0.6837 Tau:0.7278 0.4778 0.3517\n",
      "EarlyStopping counter: 23 out of 60\n",
      "Epoch: 147 Step: 10584 Index:0.4599 R2:0.8297 0.4599 0.5501 RMSE:0.4386 0.6893 0.6834 Tau:0.7221 0.4746 0.3445\n",
      "EarlyStopping counter: 24 out of 60\n",
      "Epoch: 148 Step: 10656 Index:0.4649 R2:0.8316 0.4649 0.5231 RMSE:0.4405 0.6808 0.7030 Tau:0.7241 0.4873 0.3514\n",
      "EarlyStopping counter: 25 out of 60\n",
      "Epoch: 149 Step: 10728 Index:0.4515 R2:0.8400 0.4515 0.5342 RMSE:0.4247 0.6819 0.6930 Tau:0.7334 0.4727 0.3499\n",
      "EarlyStopping counter: 26 out of 60\n",
      "Epoch: 150 Step: 10800 Index:0.4634 R2:0.8351 0.4634 0.5405 RMSE:0.4202 0.6804 0.6880 Tau:0.7281 0.4911 0.3510\n",
      "EarlyStopping counter: 27 out of 60\n",
      "Epoch: 151 Step: 10872 Index:0.4507 R2:0.8274 0.4507 0.5327 RMSE:0.4361 0.6945 0.7009 Tau:0.7256 0.4752 0.3588\n",
      "EarlyStopping counter: 28 out of 60\n",
      "Epoch: 152 Step: 10944 Index:0.4344 R2:0.8402 0.4344 0.5443 RMSE:0.4346 0.7040 0.6946 Tau:0.7342 0.4645 0.3536\n",
      "EarlyStopping counter: 29 out of 60\n",
      "Epoch: 153 Step: 11016 Index:0.4615 R2:0.8349 0.4615 0.5625 RMSE:0.4221 0.6908 0.6732 Tau:0.7282 0.4765 0.3427\n",
      "EarlyStopping counter: 30 out of 60\n",
      "Epoch: 154 Step: 11088 Index:0.4461 R2:0.8407 0.4461 0.5468 RMSE:0.4151 0.6873 0.6812 Tau:0.7331 0.4657 0.3489\n",
      "EarlyStopping counter: 31 out of 60\n",
      "Epoch: 155 Step: 11160 Index:0.4819 R2:0.8342 0.4819 0.5419 RMSE:0.4276 0.6750 0.6884 Tau:0.7308 0.4860 0.3525\n",
      "EarlyStopping counter: 32 out of 60\n",
      "Epoch: 156 Step: 11232 Index:0.4745 R2:0.8296 0.4745 0.5475 RMSE:0.4297 0.6821 0.6875 Tau:0.7219 0.4702 0.3381\n",
      "EarlyStopping counter: 33 out of 60\n",
      "Epoch: 157 Step: 11304 Index:0.4552 R2:0.8409 0.4552 0.5178 RMSE:0.4223 0.6749 0.7022 Tau:0.7360 0.4708 0.3655\n",
      "EarlyStopping counter: 34 out of 60\n",
      "Epoch: 158 Step: 11376 Index:0.4473 R2:0.8440 0.4473 0.5363 RMSE:0.4110 0.6882 0.6901 Tau:0.7379 0.4702 0.3526\n",
      "EarlyStopping counter: 35 out of 60\n",
      "Epoch: 159 Step: 11448 Index:0.4540 R2:0.8417 0.4540 0.5278 RMSE:0.4183 0.6853 0.6960 Tau:0.7380 0.4822 0.3591\n",
      "EarlyStopping counter: 36 out of 60\n",
      "Epoch: 160 Step: 11520 Index:0.4502 R2:0.8423 0.4502 0.5520 RMSE:0.4120 0.6917 0.6810 Tau:0.7366 0.4714 0.3542\n",
      "EarlyStopping counter: 37 out of 60\n",
      "Epoch: 161 Step: 11592 Index:0.4629 R2:0.8421 0.4629 0.5360 RMSE:0.4196 0.6913 0.7019 Tau:0.7365 0.4771 0.3614\n",
      "EarlyStopping counter: 38 out of 60\n",
      "Epoch: 162 Step: 11664 Index:0.4655 R2:0.8434 0.4655 0.5408 RMSE:0.4099 0.6808 0.6909 Tau:0.7382 0.4816 0.3547\n",
      "EarlyStopping counter: 39 out of 60\n",
      "Epoch: 163 Step: 11736 Index:0.4444 R2:0.8482 0.4444 0.5305 RMSE:0.4138 0.6888 0.6922 Tau:0.7419 0.4733 0.3591\n",
      "EarlyStopping counter: 40 out of 60\n",
      "Epoch: 164 Step: 11808 Index:0.4667 R2:0.8458 0.4667 0.5421 RMSE:0.4080 0.6810 0.6900 Tau:0.7399 0.4911 0.3562\n",
      "EarlyStopping counter: 41 out of 60\n",
      "Epoch: 165 Step: 11880 Index:0.4704 R2:0.8434 0.4704 0.5440 RMSE:0.4115 0.6823 0.6861 Tau:0.7389 0.4911 0.3574\n",
      "EarlyStopping counter: 42 out of 60\n",
      "Epoch: 166 Step: 11952 Index:0.4458 R2:0.8478 0.4458 0.5355 RMSE:0.4072 0.6867 0.6888 Tau:0.7429 0.4689 0.3558\n",
      "EarlyStopping counter: 43 out of 60\n",
      "Epoch: 167 Step: 12024 Index:0.4738 R2:0.8454 0.4738 0.5356 RMSE:0.4275 0.6793 0.6946 Tau:0.7384 0.4987 0.3585\n",
      "EarlyStopping counter: 44 out of 60\n",
      "Epoch: 168 Step: 12096 Index:0.4498 R2:0.8461 0.4498 0.5419 RMSE:0.4361 0.7035 0.7141 Tau:0.7388 0.4689 0.3595\n",
      "EarlyStopping counter: 45 out of 60\n",
      "Epoch: 169 Step: 12168 Index:0.4710 R2:0.8435 0.4710 0.5464 RMSE:0.4116 0.6847 0.6858 Tau:0.7373 0.4778 0.3559\n",
      "EarlyStopping counter: 46 out of 60\n",
      "Epoch: 170 Step: 12240 Index:0.4520 R2:0.8484 0.4520 0.5270 RMSE:0.4079 0.6805 0.6966 Tau:0.7451 0.4765 0.3593\n",
      "EarlyStopping counter: 47 out of 60\n",
      "Epoch: 171 Step: 12312 Index:0.4579 R2:0.8512 0.4579 0.5392 RMSE:0.4021 0.6797 0.6880 Tau:0.7455 0.4778 0.3636\n",
      "EarlyStopping counter: 48 out of 60\n",
      "Epoch: 172 Step: 12384 Index:0.4700 R2:0.8484 0.4700 0.5370 RMSE:0.4072 0.6715 0.6877 Tau:0.7409 0.4835 0.3544\n",
      "EarlyStopping counter: 49 out of 60\n",
      "Epoch: 173 Step: 12456 Index:0.4635 R2:0.8524 0.4635 0.5350 RMSE:0.4028 0.6790 0.6952 Tau:0.7466 0.4809 0.3596\n",
      "EarlyStopping counter: 50 out of 60\n",
      "Epoch: 174 Step: 12528 Index:0.4502 R2:0.8514 0.4502 0.5327 RMSE:0.4047 0.6883 0.6962 Tau:0.7458 0.4733 0.3585\n",
      "EarlyStopping counter: 51 out of 60\n",
      "Epoch: 175 Step: 12600 Index:0.4610 R2:0.8523 0.4610 0.5525 RMSE:0.3986 0.6823 0.6785 Tau:0.7446 0.4790 0.3626\n",
      "EarlyStopping counter: 52 out of 60\n",
      "Epoch: 176 Step: 12672 Index:0.4662 R2:0.8501 0.4662 0.5417 RMSE:0.4027 0.6784 0.6860 Tau:0.7434 0.4790 0.3535\n",
      "EarlyStopping counter: 53 out of 60\n",
      "Epoch: 177 Step: 12744 Index:0.4686 R2:0.8512 0.4686 0.5379 RMSE:0.4018 0.6790 0.6891 Tau:0.7445 0.4803 0.3561\n",
      "EarlyStopping counter: 54 out of 60\n",
      "Epoch: 178 Step: 12816 Index:0.4609 R2:0.8503 0.4609 0.5465 RMSE:0.4017 0.6846 0.6850 Tau:0.7429 0.4727 0.3559\n",
      "EarlyStopping counter: 55 out of 60\n",
      "Epoch: 179 Step: 12888 Index:0.4583 R2:0.8521 0.4583 0.5241 RMSE:0.4017 0.6792 0.7014 Tau:0.7475 0.4809 0.3656\n",
      "EarlyStopping counter: 56 out of 60\n",
      "Epoch: 180 Step: 12960 Index:0.4623 R2:0.8556 0.4623 0.5314 RMSE:0.3980 0.6787 0.6922 Tau:0.7488 0.4816 0.3589\n",
      "EarlyStopping counter: 57 out of 60\n",
      "Epoch: 181 Step: 13032 Index:0.4608 R2:0.8533 0.4608 0.5356 RMSE:0.4026 0.6867 0.6986 Tau:0.7484 0.4873 0.3636\n",
      "EarlyStopping counter: 58 out of 60\n",
      "Epoch: 182 Step: 13104 Index:0.4633 R2:0.8538 0.4633 0.5322 RMSE:0.4046 0.6743 0.6960 Tau:0.7471 0.4752 0.3615\n",
      "EarlyStopping counter: 59 out of 60\n",
      "Epoch: 183 Step: 13176 Index:0.4669 R2:0.8558 0.4669 0.5103 RMSE:0.4097 0.6779 0.7217 Tau:0.7502 0.4885 0.3658\n",
      "EarlyStopping counter: 60 out of 60\n",
      "Epoch: 184 Step: 13248 Index:0.4587 R2:0.8520 0.4587 0.5333 RMSE:0.4020 0.6917 0.7022 Tau:0.7436 0.4904 0.3620\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "    scheduler.step()\n",
    "    scheduler_AFSE.step()\n",
    "    scheduler_GRN.step()\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 184 r2:0.5547 RMSE:0.6843 WTI:0.3311 AP:0.4443 Tau:0.3353 \n",
      " \n",
      " Top-1:0.4000 Top-1-fp:0.2000 \n",
      " Top-5:0.6154 Top-5-fp:0.2308 \n",
      " Top-10:0.5000 Top-10-fp:0.4423 \n",
      " Top-15:0.4177 Top-15-fp:0.5696 \n",
      " Top-20:0.4300 Top-20-fp:0.5905 \n",
      " Top-25:0.4900 Top-25-fp:0.6288 \n",
      " Top-30:0.5700 Top-30-fp:0.6392 \n",
      " Top-40:0.7100 Top-40-fp:0.6635 \n",
      " Top-50:0.8700 Top-50-fp:0.6705 \n",
      " \n",
      " Top50:0.5000 Top50-fp:0.4200 \n",
      " Top100:0.4100 Top100-fp:0.5900 \n",
      " Top150:0.5400 Top150-fp:0.6400 \n",
      " Top200:0.6800 Top200-fp:0.6600 \n",
      " Top250:0.8300 Top250-fp:0.6680 \n",
      " Top300:0.9400 Top300-fp:0.6867 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
