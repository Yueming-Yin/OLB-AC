{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P21554_1_420\n",
      "model_file/0_GAFSE_Ki_P21554_1_420_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P21554_1_420_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P21554_1_420_test.csv\"\n",
    "test_active = 420\n",
    "val_rate = 0.05\n",
    "random_seed = 3\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/0_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"0_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/0_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CCCCCCC(C)(C)C1=CC(=CC(=C1)OCCCCCCCC(=O)NC(C)CO)O -1.232996\n",
      "1                     CCCCCCCCC=CCCCCCCCC(=O)NC1CC1O -2.812913\n",
      "2  CC1CCN(CC1)C(=O)C2=CC3=C(C=C2)N(C4=C3CN(CC4)C5... -1.748188\n",
      "3  CC1=CC=C(N1C2=C(C(=NN2C3=C(C=C(C=C3)Cl)Cl)C(=O... -2.158362\n",
      "4  CCCCCCC1(CC1(Br)Br)C2=CC3=C(C4CC(=CCC4C(O3)(C)...  0.149384\n",
      "number of all smiles:  2046\n",
      "number of successfully processed smiles:  2046\n",
      "                                              smiles     value  \\\n",
      "0  CCCCCCC(C)(C)C1=CC(=CC(=C1)OCCCCCCCC(=O)NC(C)CO)O -1.232996   \n",
      "1                     CCCCCCCCC=CCCCCCCCC(=O)NC1CC1O -2.812913   \n",
      "2  CC1CCN(CC1)C(=O)C2=CC3=C(C=C2)N(C4=C3CN(CC4)C5... -1.748188   \n",
      "3  CC1=CC=C(N1C2=C(C(=NN2C3=C(C=C(C=C3)Cl)Cl)C(=O... -2.158362   \n",
      "4  CCCCCCC1(CC1(Br)Br)C2=CC3=C(C4CC(=CCC4C(O3)(C)...  0.149384   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0     CCCCCCC(C)(C)c1cc(O)cc(OCCCCCCCC(=O)NC(C)CO)c1  \n",
      "1                     CCCCCCCCC=CCCCCCCCC(=O)NC1CC1O  \n",
      "2  C=CCn1c2c(c3cc(C(=O)N4CCC(C)CC4)ccc31)CN(C1CCC...  \n",
      "3  COc1cc(CNC(=O)c2nn(-c3ccc(Cl)cc3Cl)c(-n3c(C)cc...  \n",
      "4  CCCCCCC1(c2cc(O)c3c(c2)OC(C)(C)C2CC=C(C)CC32)C...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.1\n",
    "fingerprint_dim = 128\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 3 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1342\n",
      "number of successfully processed smiles:  1342\n",
      "(1342, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CCC(C(C1)NC(=O)C2=CN=C(C(=C2)C3=C(C=C(C=C3)C... -2.086360   \n",
      "1  CC1=C(C=C(C=C1)C(=O)NC2C(C3CCC2(C3)C)(C)C)S(=O... -2.113943   \n",
      "2  CCCCCN1C=C(C(=O)C2=C1C=C(C=C2)SC3=CC=CC=C3)C(=... -1.110590   \n",
      "3  CC1=C(N=C(N1C2=CC=C(C=C2)Cl)C3=C(C=C(C=C3)Cl)C... -2.232996   \n",
      "4  CCCCCCC(C)(C)C1=CC(=C2C3C=C(CCC3C(OC2=C1)(C)C)... -3.127105   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  O=C(NC1CCCCC1O)c1cnc(OCC2CC2)c(-c2ccc(Cl)cc2Cl)c1  \n",
      "1  Cc1ccc(C(=O)NC2C3(C)CCC(C3)C2(C)C)cc1S(=O)(=O)...  \n",
      "2  CCCCCn1cc(C(=O)NOCc2ccccc2)c(=O)c2ccc(Sc3ccccc...  \n",
      "3  Cc1c(C(=O)NCc2ccc(C(F)(F)F)cc2)nc(-c2ccc(Cl)cc...  \n",
      "4  CCCCCCC(C)(C)c1cc(NC(C)=O)c2c(c1)OC(C)(C)C1CCC...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P21554_1_420_train.pickle\n",
      "./data/benchmark/Ki_P21554_1_420_train\n",
      "3388\n",
      "feature dicts file saved as ./data/benchmark/Ki_P21554_1_420_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1944, 3) (102, 3) (1342, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=2e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=2e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=2e-4, weight_decay=10**-weight_decay)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.95)\n",
    "scheduler_AFSE = StepLR(optimizer_AFSE, step_size=10, gamma=0.95)\n",
    "scheduler_GRN = StepLR(optimizer_GRN, step_size=10, gamma=0.95)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    reconstruction_loss = 0\n",
    "    counter = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        reconstruction_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        counter += 1\n",
    "    reconstruction_loss = reconstruction_loss/counter\n",
    "    return reconstruction_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "        success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1)\n",
    "        reconstruction_loss = generate_loss_function(refer_atom_list, x_atom, validity_mask, atom_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "        atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "                                                torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "        refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                            torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "                                                            mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "        success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "                            bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "                                                     refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "        test_reconstruction_loss = generate_loss_function(atom_list_test, x_atom_test, validity_mask_test, atom_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6 * (vat_loss + test_vat_loss) + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/0_GAFSE_Ki_P21554_1_420_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177995/3510960041.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  y = torch.FloatTensor(y).reshape(-1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 194 Index:0.0002 R2:0.0177 0.0002 0.0196 RMSE:1.0944 1.1138 1.0995 Tau:0.0940 -0.0328 0.0151\n",
      "Epoch: 2 Step: 388 Index:0.1263 R2:0.0995 0.1263 0.1446 RMSE:1.0634 1.0651 1.0564 Tau:0.2176 0.2321 0.1789\n",
      "Epoch: 3 Step: 582 Index:0.1750 R2:0.1705 0.1750 0.1874 RMSE:1.0442 1.0426 1.0355 Tau:0.2829 0.2772 0.2415\n",
      "Epoch: 4 Step: 776 Index:0.1939 R2:0.2279 0.1939 0.2083 RMSE:0.9988 1.0148 1.0060 Tau:0.3257 0.2787 0.2695\n",
      "Epoch: 5 Step: 970 Index:0.2234 R2:0.2573 0.2234 0.2157 RMSE:0.9788 1.0107 1.0116 Tau:0.3509 0.3040 0.2676\n",
      "Epoch: 6 Step: 1164 Index:0.2429 R2:0.2835 0.2429 0.2396 RMSE:1.0172 1.0627 1.0596 Tau:0.3665 0.3172 0.2792\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 7 Step: 1358 Index:0.2291 R2:0.2913 0.2291 0.2435 RMSE:0.9318 0.9856 0.9704 Tau:0.3711 0.3091 0.2892\n",
      "Epoch: 8 Step: 1552 Index:0.2473 R2:0.3199 0.2473 0.2651 RMSE:0.9614 1.0236 1.0098 Tau:0.3898 0.3226 0.2854\n",
      "Epoch: 9 Step: 1746 Index:0.2693 R2:0.3335 0.2693 0.2823 RMSE:0.9108 0.9707 0.9563 Tau:0.3999 0.3374 0.2905\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 10 Step: 1940 Index:0.2667 R2:0.3384 0.2667 0.2781 RMSE:1.0303 1.0887 1.0868 Tau:0.4028 0.3394 0.2849\n",
      "Epoch: 11 Step: 2134 Index:0.2718 R2:0.3495 0.2718 0.2912 RMSE:0.9171 0.9652 0.9475 Tau:0.4109 0.3390 0.2927\n",
      "Epoch: 12 Step: 2328 Index:0.2924 R2:0.3708 0.2924 0.3230 RMSE:0.8936 0.9413 0.9200 Tau:0.4244 0.3557 0.2999\n",
      "Epoch: 13 Step: 2522 Index:0.3095 R2:0.3694 0.3095 0.3134 RMSE:0.8906 0.9367 0.9370 Tau:0.4228 0.3697 0.2855\n",
      "Epoch: 14 Step: 2716 Index:0.3118 R2:0.3864 0.3118 0.3288 RMSE:0.8790 0.9457 0.9313 Tau:0.4322 0.3693 0.2982\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 15 Step: 2910 Index:0.2546 R2:0.3914 0.2546 0.3128 RMSE:0.8693 0.9633 0.9256 Tau:0.4374 0.3172 0.2912\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 16 Step: 3104 Index:0.2799 R2:0.3969 0.2799 0.3185 RMSE:0.8638 0.9545 0.9271 Tau:0.4410 0.3413 0.2973\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 17 Step: 3298 Index:0.2954 R2:0.4164 0.2954 0.3503 RMSE:0.9271 0.9961 0.9834 Tau:0.4546 0.3487 0.2944\n",
      "Epoch: 18 Step: 3492 Index:0.3276 R2:0.4195 0.3276 0.3494 RMSE:0.8615 0.9314 0.9211 Tau:0.4540 0.3755 0.3008\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 19 Step: 3686 Index:0.2808 R2:0.4120 0.2808 0.3296 RMSE:0.8528 0.9502 0.9087 Tau:0.4494 0.3405 0.3111\n",
      "Epoch: 20 Step: 3880 Index:0.3383 R2:0.4387 0.3383 0.3649 RMSE:0.8319 0.9050 0.8859 Tau:0.4698 0.3825 0.3018\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 21 Step: 4074 Index:0.3161 R2:0.4511 0.3161 0.3553 RMSE:0.8368 0.9257 0.8937 Tau:0.4752 0.3662 0.3067\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 22 Step: 4268 Index:0.3333 R2:0.4403 0.3333 0.3658 RMSE:0.8373 0.9183 0.8981 Tau:0.4698 0.3798 0.3062\n",
      "Epoch: 23 Step: 4462 Index:0.3483 R2:0.4566 0.3483 0.3670 RMSE:0.8267 0.9093 0.9009 Tau:0.4791 0.3906 0.3060\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 24 Step: 4656 Index:0.2957 R2:0.4363 0.2957 0.3398 RMSE:0.8293 0.9438 0.9028 Tau:0.4635 0.3495 0.3197\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 25 Step: 4850 Index:0.3398 R2:0.4709 0.3398 0.3887 RMSE:0.8109 0.9044 0.8715 Tau:0.4924 0.3868 0.3162\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 26 Step: 5044 Index:0.3258 R2:0.4748 0.3258 0.3735 RMSE:0.8146 0.9184 0.8906 Tau:0.4899 0.3833 0.3084\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 27 Step: 5238 Index:0.3205 R2:0.4737 0.3205 0.3669 RMSE:0.8201 0.9336 0.9041 Tau:0.4889 0.3739 0.3156\n",
      "Epoch: 28 Step: 5432 Index:0.3593 R2:0.4953 0.3593 0.4031 RMSE:0.8520 0.9352 0.9243 Tau:0.5028 0.4000 0.3133\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 29 Step: 5626 Index:0.3303 R2:0.5040 0.3303 0.4176 RMSE:0.7843 0.9101 0.8485 Tau:0.5084 0.3821 0.3237\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 30 Step: 5820 Index:0.3281 R2:0.5097 0.3281 0.4013 RMSE:0.8093 0.9359 0.8956 Tau:0.5106 0.3840 0.3214\n",
      "Epoch: 31 Step: 6014 Index:0.3628 R2:0.5275 0.3628 0.4233 RMSE:0.7747 0.9020 0.8482 Tau:0.5222 0.4007 0.3347\n",
      "Epoch: 32 Step: 6208 Index:0.3884 R2:0.5407 0.3884 0.4376 RMSE:0.7893 0.8930 0.8699 Tau:0.5315 0.4182 0.3331\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 33 Step: 6402 Index:0.3669 R2:0.5397 0.3669 0.4391 RMSE:0.7522 0.8845 0.8297 Tau:0.5294 0.4097 0.3324\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 34 Step: 6596 Index:0.3791 R2:0.5364 0.3791 0.4369 RMSE:0.7586 0.8807 0.8406 Tau:0.5276 0.4147 0.3267\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 35 Step: 6790 Index:0.3772 R2:0.5371 0.3772 0.4432 RMSE:0.7560 0.8920 0.8416 Tau:0.5281 0.4186 0.3391\n",
      "Epoch: 36 Step: 6984 Index:0.3915 R2:0.5423 0.3915 0.4520 RMSE:0.7504 0.8726 0.8275 Tau:0.5275 0.4353 0.3454\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 37 Step: 7178 Index:0.3784 R2:0.5558 0.3784 0.4474 RMSE:0.7676 0.9070 0.8489 Tau:0.5364 0.4256 0.3506\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 38 Step: 7372 Index:0.3693 R2:0.5493 0.3693 0.4459 RMSE:0.7666 0.8936 0.8325 Tau:0.5380 0.4213 0.3297\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 39 Step: 7566 Index:0.3392 R2:0.5530 0.3392 0.4299 RMSE:0.7359 0.9179 0.8417 Tau:0.5347 0.3949 0.3496\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 40 Step: 7760 Index:0.3520 R2:0.5682 0.3520 0.4457 RMSE:0.7255 0.9012 0.8249 Tau:0.5440 0.4089 0.3571\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 41 Step: 7954 Index:0.3679 R2:0.5715 0.3679 0.4497 RMSE:0.7262 0.8917 0.8312 Tau:0.5451 0.4081 0.3444\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 42 Step: 8148 Index:0.3515 R2:0.5880 0.3515 0.4576 RMSE:0.7202 0.8955 0.8173 Tau:0.5573 0.4101 0.3472\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 43 Step: 8342 Index:0.3723 R2:0.5886 0.3723 0.4663 RMSE:0.7142 0.8860 0.8109 Tau:0.5569 0.4237 0.3514\n",
      "Epoch: 44 Step: 8536 Index:0.4218 R2:0.5873 0.4218 0.4951 RMSE:0.7410 0.8622 0.8088 Tau:0.5603 0.4610 0.3485\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 45 Step: 8730 Index:0.4191 R2:0.5934 0.4191 0.4819 RMSE:0.7503 0.8794 0.8392 Tau:0.5602 0.4582 0.3589\n",
      "Epoch: 46 Step: 8924 Index:0.4263 R2:0.5887 0.4263 0.4897 RMSE:0.7430 0.8619 0.8224 Tau:0.5616 0.4641 0.3570\n",
      "Epoch: 47 Step: 9118 Index:0.4307 R2:0.6185 0.4307 0.5069 RMSE:0.7625 0.8969 0.8457 Tau:0.5777 0.4641 0.3507\n",
      "Epoch: 48 Step: 9312 Index:0.4323 R2:0.6135 0.4323 0.5099 RMSE:0.7424 0.8781 0.8267 Tau:0.5733 0.4625 0.3650\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 49 Step: 9506 Index:0.3955 R2:0.6147 0.3955 0.4888 RMSE:0.6863 0.8644 0.7917 Tau:0.5765 0.4466 0.3570\n",
      "Epoch: 50 Step: 9700 Index:0.4522 R2:0.6308 0.4522 0.5062 RMSE:0.6821 0.8290 0.7897 Tau:0.5850 0.4780 0.3565\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 51 Step: 9894 Index:0.3854 R2:0.6378 0.3854 0.4939 RMSE:0.6807 0.8744 0.7898 Tau:0.5864 0.4349 0.3782\n",
      "Epoch: 52 Step: 10088 Index:0.4646 R2:0.6152 0.4646 0.5179 RMSE:0.6938 0.8232 0.7704 Tau:0.5778 0.5021 0.3568\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 53 Step: 10282 Index:0.4358 R2:0.6371 0.4358 0.5178 RMSE:0.6940 0.8522 0.7933 Tau:0.5877 0.4718 0.3651\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 54 Step: 10476 Index:0.4304 R2:0.6295 0.4304 0.5109 RMSE:0.6722 0.8401 0.7779 Tau:0.5807 0.4613 0.3668\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 55 Step: 10670 Index:0.4033 R2:0.6379 0.4033 0.5036 RMSE:0.6712 0.8771 0.7997 Tau:0.5878 0.4419 0.3694\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 56 Step: 10864 Index:0.4193 R2:0.6373 0.4193 0.4969 RMSE:0.6725 0.8523 0.7915 Tau:0.5868 0.4703 0.3723\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 57 Step: 11058 Index:0.4449 R2:0.6551 0.4449 0.5155 RMSE:0.6524 0.8300 0.7705 Tau:0.5988 0.4854 0.3715\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 58 Step: 11252 Index:0.3686 R2:0.6285 0.3686 0.4834 RMSE:0.7077 0.9028 0.8263 Tau:0.5848 0.4155 0.3697\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 59 Step: 11446 Index:0.4307 R2:0.6480 0.4307 0.5031 RMSE:0.6528 0.8398 0.7880 Tau:0.5935 0.4613 0.3541\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 60 Step: 11640 Index:0.4497 R2:0.6724 0.4497 0.5304 RMSE:0.6465 0.8296 0.7637 Tau:0.6107 0.4874 0.3731\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 61 Step: 11834 Index:0.4510 R2:0.6753 0.4510 0.5510 RMSE:0.6369 0.8255 0.7432 Tau:0.6147 0.4850 0.3773\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 62 Step: 12028 Index:0.4168 R2:0.6687 0.4168 0.5309 RMSE:0.6608 0.8642 0.7796 Tau:0.6083 0.4575 0.3719\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 63 Step: 12222 Index:0.4317 R2:0.6733 0.4317 0.5304 RMSE:0.6396 0.8435 0.7664 Tau:0.6106 0.4641 0.3693\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 64 Step: 12416 Index:0.4636 R2:0.6800 0.4636 0.5382 RMSE:0.6235 0.8167 0.7603 Tau:0.6151 0.4994 0.3774\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 65 Step: 12610 Index:0.3965 R2:0.6737 0.3965 0.5111 RMSE:0.6620 0.8769 0.7913 Tau:0.6125 0.4485 0.3827\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 66 Step: 12804 Index:0.4485 R2:0.6814 0.4485 0.5448 RMSE:0.6394 0.8332 0.7518 Tau:0.6169 0.4835 0.3812\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 67 Step: 12998 Index:0.4122 R2:0.6889 0.4122 0.5397 RMSE:0.6431 0.8583 0.7593 Tau:0.6235 0.4664 0.3961\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 68 Step: 13192 Index:0.4258 R2:0.6880 0.4258 0.5445 RMSE:0.6359 0.8510 0.7575 Tau:0.6225 0.4870 0.3907\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 69 Step: 13386 Index:0.4267 R2:0.6689 0.4267 0.5193 RMSE:0.6411 0.8428 0.7693 Tau:0.6076 0.4885 0.3809\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 70 Step: 13580 Index:0.4542 R2:0.6965 0.4542 0.5528 RMSE:0.6271 0.8283 0.7453 Tau:0.6280 0.5014 0.3867\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 71 Step: 13774 Index:0.4554 R2:0.7035 0.4554 0.5458 RMSE:0.6046 0.8222 0.7473 Tau:0.6345 0.4955 0.3975\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 72 Step: 13968 Index:0.4374 R2:0.7085 0.4374 0.5373 RMSE:0.6062 0.8402 0.7621 Tau:0.6377 0.4831 0.3881\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 73 Step: 14162 Index:0.3733 R2:0.6806 0.3733 0.5143 RMSE:0.6412 0.8911 0.7826 Tau:0.6163 0.4314 0.3996\n",
      "Epoch: 74 Step: 14356 Index:0.4733 R2:0.6989 0.4733 0.5436 RMSE:0.6074 0.8072 0.7481 Tau:0.6289 0.5192 0.3989\n",
      "Epoch: 75 Step: 14550 Index:0.4838 R2:0.7008 0.4838 0.5554 RMSE:0.6198 0.8095 0.7496 Tau:0.6305 0.5181 0.3849\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 76 Step: 14744 Index:0.4402 R2:0.7057 0.4402 0.5422 RMSE:0.5971 0.8381 0.7556 Tau:0.6338 0.4913 0.4003\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 77 Step: 14938 Index:0.4795 R2:0.7156 0.4795 0.5571 RMSE:0.5939 0.8023 0.7364 Tau:0.6395 0.5045 0.3922\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 78 Step: 15132 Index:0.4637 R2:0.7084 0.4637 0.5589 RMSE:0.5954 0.8153 0.7385 Tau:0.6363 0.5064 0.3820\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 79 Step: 15326 Index:0.4309 R2:0.7024 0.4309 0.5532 RMSE:0.6158 0.8407 0.7439 Tau:0.6331 0.4796 0.3876\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 80 Step: 15520 Index:0.4759 R2:0.7071 0.4759 0.5671 RMSE:0.6402 0.8370 0.7636 Tau:0.6359 0.5142 0.3857\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 81 Step: 15714 Index:0.4723 R2:0.7232 0.4723 0.5553 RMSE:0.5910 0.8116 0.7429 Tau:0.6467 0.5196 0.3968\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 82 Step: 15908 Index:0.4743 R2:0.7307 0.4743 0.5704 RMSE:0.6281 0.8477 0.7747 Tau:0.6514 0.5087 0.4038\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 83 Step: 16102 Index:0.4549 R2:0.7128 0.4549 0.5450 RMSE:0.6005 0.8303 0.7577 Tau:0.6375 0.5037 0.3901\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 84 Step: 16296 Index:0.4760 R2:0.7204 0.4760 0.5596 RMSE:0.5826 0.8091 0.7401 Tau:0.6455 0.5084 0.3989\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 85 Step: 16490 Index:0.4640 R2:0.7226 0.4640 0.5516 RMSE:0.6027 0.8308 0.7602 Tau:0.6465 0.5033 0.3907\n",
      "Epoch: 86 Step: 16684 Index:0.4991 R2:0.7328 0.4991 0.5632 RMSE:0.6146 0.8090 0.7577 Tau:0.6543 0.5254 0.3881\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 87 Step: 16878 Index:0.4611 R2:0.7231 0.4611 0.5503 RMSE:0.5979 0.8262 0.7509 Tau:0.6476 0.4940 0.3973\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 88 Step: 17072 Index:0.4776 R2:0.7327 0.4776 0.5510 RMSE:0.5762 0.8059 0.7451 Tau:0.6557 0.5165 0.3981\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 89 Step: 17266 Index:0.4947 R2:0.7327 0.4947 0.5583 RMSE:0.5705 0.7909 0.7369 Tau:0.6511 0.5188 0.4069\n",
      "Epoch: 90 Step: 17460 Index:0.5193 R2:0.7309 0.5193 0.5825 RMSE:0.5759 0.7720 0.7151 Tau:0.6587 0.5526 0.3918\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 91 Step: 17654 Index:0.5009 R2:0.7001 0.5009 0.5312 RMSE:0.6440 0.8109 0.8037 Tau:0.6388 0.5200 0.3733\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 92 Step: 17848 Index:0.5000 R2:0.7424 0.5000 0.5629 RMSE:0.5737 0.7987 0.7467 Tau:0.6610 0.5313 0.3957\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 93 Step: 18042 Index:0.4937 R2:0.7467 0.4937 0.5680 RMSE:0.5568 0.7928 0.7301 Tau:0.6634 0.5227 0.4046\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 94 Step: 18236 Index:0.4842 R2:0.7490 0.4842 0.5770 RMSE:0.5671 0.8066 0.7279 Tau:0.6663 0.5223 0.3875\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 95 Step: 18430 Index:0.5033 R2:0.7543 0.5033 0.5693 RMSE:0.5494 0.7859 0.7299 Tau:0.6703 0.5394 0.4102\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 96 Step: 18624 Index:0.5042 R2:0.7353 0.5042 0.5734 RMSE:0.5902 0.8083 0.7554 Tau:0.6544 0.5309 0.3907\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 97 Step: 18818 Index:0.4831 R2:0.7393 0.4831 0.5625 RMSE:0.6163 0.8465 0.7863 Tau:0.6577 0.5254 0.3973\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 98 Step: 19012 Index:0.4937 R2:0.7508 0.4937 0.5745 RMSE:0.5535 0.7946 0.7284 Tau:0.6680 0.5398 0.3970\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 99 Step: 19206 Index:0.4774 R2:0.7466 0.4774 0.5615 RMSE:0.5677 0.8228 0.7551 Tau:0.6634 0.5212 0.4016\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 100 Step: 19400 Index:0.4860 R2:0.7507 0.4860 0.5595 RMSE:0.5618 0.8052 0.7429 Tau:0.6693 0.5165 0.4030\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 101 Step: 19594 Index:0.4914 R2:0.7649 0.4914 0.5737 RMSE:0.5398 0.7948 0.7240 Tau:0.6779 0.5169 0.4013\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 102 Step: 19788 Index:0.4688 R2:0.7613 0.4688 0.5739 RMSE:0.5518 0.8119 0.7233 Tau:0.6775 0.5126 0.4163\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 103 Step: 19982 Index:0.4782 R2:0.7604 0.4782 0.5785 RMSE:0.5470 0.8093 0.7237 Tau:0.6776 0.5266 0.4201\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 104 Step: 20176 Index:0.5128 R2:0.7712 0.5128 0.5787 RMSE:0.5364 0.7761 0.7184 Tau:0.6838 0.5476 0.4065\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 105 Step: 20370 Index:0.4890 R2:0.7683 0.4890 0.5801 RMSE:0.5347 0.7953 0.7176 Tau:0.6824 0.5231 0.4053\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 106 Step: 20564 Index:0.4628 R2:0.7635 0.4628 0.5890 RMSE:0.5986 0.8665 0.7636 Tau:0.6763 0.4936 0.4111\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 107 Step: 20758 Index:0.4974 R2:0.7660 0.4974 0.5750 RMSE:0.5385 0.7905 0.7246 Tau:0.6801 0.5235 0.4030\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 108 Step: 20952 Index:0.4518 R2:0.7555 0.4518 0.5545 RMSE:0.5547 0.8296 0.7448 Tau:0.6732 0.4990 0.4016\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 109 Step: 21146 Index:0.5060 R2:0.7705 0.5060 0.5798 RMSE:0.5400 0.7881 0.7251 Tau:0.6814 0.5332 0.4053\n",
      "Epoch: 110 Step: 21340 Index:0.5213 R2:0.7619 0.5213 0.5869 RMSE:0.6047 0.8157 0.7652 Tau:0.6763 0.5449 0.4032\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 111 Step: 21534 Index:0.5194 R2:0.7781 0.5194 0.5855 RMSE:0.5184 0.7763 0.7189 Tau:0.6902 0.5495 0.4117\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 112 Step: 21728 Index:0.4984 R2:0.7635 0.4984 0.5835 RMSE:0.5425 0.8018 0.7305 Tau:0.6797 0.5235 0.4058\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 113 Step: 21922 Index:0.4954 R2:0.7733 0.4954 0.5575 RMSE:0.5484 0.8035 0.7538 Tau:0.6872 0.5321 0.4092\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 114 Step: 22116 Index:0.4874 R2:0.7792 0.4874 0.5748 RMSE:0.5484 0.8269 0.7541 Tau:0.6912 0.5387 0.4062\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 115 Step: 22310 Index:0.5001 R2:0.7814 0.5001 0.5809 RMSE:0.5330 0.8016 0.7358 Tau:0.6937 0.5305 0.4085\n",
      "Epoch: 116 Step: 22504 Index:0.5326 R2:0.7725 0.5326 0.5813 RMSE:0.5261 0.7691 0.7280 Tau:0.6846 0.5499 0.4007\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 117 Step: 22698 Index:0.4713 R2:0.7591 0.4713 0.5600 RMSE:0.5453 0.8245 0.7483 Tau:0.6738 0.5192 0.4081\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 118 Step: 22892 Index:0.4974 R2:0.7824 0.4974 0.5819 RMSE:0.5206 0.7945 0.7246 Tau:0.6969 0.5480 0.4016\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 119 Step: 23086 Index:0.5179 R2:0.7932 0.5179 0.5911 RMSE:0.5416 0.8074 0.7479 Tau:0.7023 0.5557 0.4046\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 120 Step: 23280 Index:0.5177 R2:0.7906 0.5177 0.5908 RMSE:0.5409 0.8089 0.7444 Tau:0.6998 0.5460 0.4176\n",
      "Epoch: 121 Step: 23474 Index:0.5419 R2:0.7989 0.5419 0.5993 RMSE:0.5005 0.7534 0.7033 Tau:0.7071 0.5589 0.4151\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 122 Step: 23668 Index:0.5036 R2:0.7909 0.5036 0.5859 RMSE:0.5155 0.7903 0.7223 Tau:0.7028 0.5367 0.4009\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 123 Step: 23862 Index:0.5088 R2:0.7849 0.5088 0.5753 RMSE:0.5131 0.7869 0.7294 Tau:0.6938 0.5352 0.4136\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 124 Step: 24056 Index:0.5371 R2:0.7964 0.5371 0.6007 RMSE:0.5063 0.7640 0.7085 Tau:0.7048 0.5557 0.4128\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 125 Step: 24250 Index:0.5287 R2:0.7992 0.5287 0.5878 RMSE:0.5010 0.7732 0.7214 Tau:0.7057 0.5534 0.4086\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 126 Step: 24444 Index:0.5196 R2:0.8042 0.5196 0.5944 RMSE:0.5501 0.8162 0.7603 Tau:0.7113 0.5340 0.4132\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 127 Step: 24638 Index:0.5358 R2:0.8028 0.5358 0.5931 RMSE:0.5251 0.7935 0.7452 Tau:0.7088 0.5612 0.4010\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 128 Step: 24832 Index:0.5394 R2:0.8041 0.5394 0.5933 RMSE:0.4969 0.7557 0.7085 Tau:0.7102 0.5631 0.4194\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 129 Step: 25026 Index:0.5205 R2:0.7879 0.5205 0.5784 RMSE:0.5382 0.8042 0.7532 Tau:0.6976 0.5422 0.4150\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 130 Step: 25220 Index:0.5257 R2:0.8012 0.5257 0.5938 RMSE:0.4967 0.7657 0.7056 Tau:0.7084 0.5592 0.4081\n",
      "Epoch: 131 Step: 25414 Index:0.5429 R2:0.8064 0.5429 0.5944 RMSE:0.4889 0.7536 0.7088 Tau:0.7131 0.5631 0.4123\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 132 Step: 25608 Index:0.5358 R2:0.8112 0.5358 0.5952 RMSE:0.4844 0.7574 0.7047 Tau:0.7178 0.5546 0.4257\n",
      "Epoch: 133 Step: 25802 Index:0.5537 R2:0.8123 0.5537 0.5919 RMSE:0.4782 0.7438 0.7111 Tau:0.7173 0.5697 0.4144\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 134 Step: 25996 Index:0.5434 R2:0.8094 0.5434 0.5991 RMSE:0.4888 0.7515 0.7014 Tau:0.7184 0.5612 0.4197\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 135 Step: 26190 Index:0.5247 R2:0.8068 0.5247 0.5967 RMSE:0.4899 0.7817 0.7184 Tau:0.7128 0.5394 0.4223\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 136 Step: 26384 Index:0.5389 R2:0.8132 0.5389 0.6014 RMSE:0.4800 0.7563 0.7018 Tau:0.7183 0.5612 0.4082\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 137 Step: 26578 Index:0.5262 R2:0.8163 0.5262 0.5930 RMSE:0.4864 0.7704 0.7119 Tau:0.7222 0.5480 0.4102\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 138 Step: 26772 Index:0.5272 R2:0.8074 0.5272 0.5950 RMSE:0.4830 0.7721 0.7138 Tau:0.7127 0.5491 0.4108\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 139 Step: 26966 Index:0.5397 R2:0.8172 0.5397 0.5852 RMSE:0.4833 0.7566 0.7158 Tau:0.7226 0.5659 0.4223\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 140 Step: 27160 Index:0.5294 R2:0.8131 0.5294 0.5943 RMSE:0.4858 0.7690 0.7096 Tau:0.7201 0.5596 0.4195\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 141 Step: 27354 Index:0.5419 R2:0.8168 0.5419 0.6004 RMSE:0.4740 0.7553 0.7031 Tau:0.7232 0.5523 0.4132\n",
      "Epoch: 142 Step: 27548 Index:0.5577 R2:0.8212 0.5577 0.5979 RMSE:0.4869 0.7439 0.7071 Tau:0.7257 0.5740 0.4113\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 143 Step: 27742 Index:0.5429 R2:0.8156 0.5429 0.6041 RMSE:0.4741 0.7548 0.7015 Tau:0.7211 0.5495 0.4093\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 144 Step: 27936 Index:0.5295 R2:0.8280 0.5295 0.5963 RMSE:0.4635 0.7632 0.7044 Tau:0.7329 0.5488 0.4189\n",
      "Epoch: 145 Step: 28130 Index:0.5579 R2:0.8261 0.5579 0.5961 RMSE:0.4740 0.7406 0.7045 Tau:0.7313 0.5794 0.4259\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 146 Step: 28324 Index:0.5460 R2:0.8221 0.5460 0.6026 RMSE:0.4644 0.7580 0.7089 Tau:0.7274 0.5659 0.4206\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 147 Step: 28518 Index:0.5439 R2:0.8056 0.5439 0.5936 RMSE:0.4866 0.7576 0.7174 Tau:0.7134 0.5596 0.4024\n",
      "Epoch: 148 Step: 28712 Index:0.5652 R2:0.8188 0.5652 0.6054 RMSE:0.4699 0.7389 0.7059 Tau:0.7252 0.5888 0.4179\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 149 Step: 28906 Index:0.5358 R2:0.8239 0.5358 0.5898 RMSE:0.4781 0.7596 0.7147 Tau:0.7321 0.5503 0.4149\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 150 Step: 29100 Index:0.5537 R2:0.8261 0.5537 0.5945 RMSE:0.4604 0.7469 0.7131 Tau:0.7292 0.5662 0.4113\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 151 Step: 29294 Index:0.5432 R2:0.8172 0.5432 0.5946 RMSE:0.4786 0.7658 0.7242 Tau:0.7238 0.5608 0.4144\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 152 Step: 29488 Index:0.5459 R2:0.8209 0.5459 0.5940 RMSE:0.4713 0.7624 0.7236 Tau:0.7275 0.5573 0.4149\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 153 Step: 29682 Index:0.5643 R2:0.8300 0.5643 0.6053 RMSE:0.4565 0.7347 0.6995 Tau:0.7351 0.5763 0.4124\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 154 Step: 29876 Index:0.5465 R2:0.8277 0.5465 0.6129 RMSE:0.4606 0.7599 0.7032 Tau:0.7331 0.5534 0.4137\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 155 Step: 30070 Index:0.5433 R2:0.8304 0.5433 0.5884 RMSE:0.4644 0.7531 0.7111 Tau:0.7353 0.5557 0.4233\n",
      "Epoch: 156 Step: 30264 Index:0.5720 R2:0.8385 0.5720 0.5994 RMSE:0.4483 0.7280 0.7020 Tau:0.7423 0.5756 0.4232\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 157 Step: 30458 Index:0.5494 R2:0.8339 0.5494 0.5966 RMSE:0.4554 0.7523 0.7127 Tau:0.7379 0.5736 0.4129\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 158 Step: 30652 Index:0.5659 R2:0.8348 0.5659 0.5806 RMSE:0.4502 0.7340 0.7211 Tau:0.7407 0.5837 0.4203\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 159 Step: 30846 Index:0.5536 R2:0.8365 0.5536 0.6057 RMSE:0.4503 0.7506 0.7047 Tau:0.7408 0.5697 0.4233\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 160 Step: 31040 Index:0.5555 R2:0.8441 0.5555 0.6145 RMSE:0.4472 0.7426 0.6906 Tau:0.7492 0.5639 0.4151\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 161 Step: 31234 Index:0.5543 R2:0.8424 0.5543 0.6087 RMSE:0.4419 0.7441 0.6970 Tau:0.7449 0.5674 0.4201\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 162 Step: 31428 Index:0.5666 R2:0.8402 0.5666 0.6071 RMSE:0.4498 0.7346 0.7029 Tau:0.7444 0.5728 0.4168\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 163 Step: 31622 Index:0.5569 R2:0.8396 0.5569 0.6050 RMSE:0.4661 0.7551 0.7165 Tau:0.7444 0.5717 0.4109\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 164 Step: 31816 Index:0.5488 R2:0.8450 0.5488 0.6083 RMSE:0.4930 0.7846 0.7415 Tau:0.7487 0.5627 0.4248\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 165 Step: 32010 Index:0.5653 R2:0.8490 0.5653 0.6126 RMSE:0.4329 0.7345 0.6933 Tau:0.7527 0.5767 0.4212\n",
      "Epoch: 166 Step: 32204 Index:0.5915 R2:0.8456 0.5915 0.6188 RMSE:0.4360 0.7116 0.6869 Tau:0.7486 0.5950 0.4221\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 167 Step: 32398 Index:0.5539 R2:0.8456 0.5539 0.6101 RMSE:0.4391 0.7509 0.7036 Tau:0.7493 0.5690 0.4292\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 168 Step: 32592 Index:0.5841 R2:0.8423 0.5841 0.6104 RMSE:0.4373 0.7233 0.7063 Tau:0.7462 0.5728 0.4190\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 169 Step: 32786 Index:0.5740 R2:0.8448 0.5740 0.6029 RMSE:0.4521 0.7282 0.6993 Tau:0.7485 0.5779 0.4291\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 170 Step: 32980 Index:0.5457 R2:0.8365 0.5457 0.5855 RMSE:0.5759 0.8343 0.8201 Tau:0.7440 0.5635 0.4154\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 171 Step: 33174 Index:0.5760 R2:0.8422 0.5760 0.6164 RMSE:0.4588 0.7397 0.7095 Tau:0.7479 0.5639 0.4170\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 172 Step: 33368 Index:0.5825 R2:0.8521 0.5825 0.6024 RMSE:0.4436 0.7207 0.6979 Tau:0.7570 0.6078 0.4215\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 173 Step: 33562 Index:0.5626 R2:0.8443 0.5626 0.5867 RMSE:0.4382 0.7433 0.7261 Tau:0.7482 0.5775 0.4245\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 174 Step: 33756 Index:0.5718 R2:0.8485 0.5718 0.5991 RMSE:0.4373 0.7299 0.7039 Tau:0.7528 0.5822 0.4308\n",
      "Epoch: 175 Step: 33950 Index:0.5972 R2:0.8537 0.5972 0.6008 RMSE:0.4247 0.7057 0.7046 Tau:0.7558 0.5927 0.4176\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 176 Step: 34144 Index:0.5607 R2:0.8508 0.5607 0.5966 RMSE:0.4345 0.7467 0.7194 Tau:0.7542 0.5713 0.4257\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 177 Step: 34338 Index:0.5884 R2:0.8499 0.5884 0.6085 RMSE:0.4560 0.7453 0.7335 Tau:0.7532 0.5837 0.4292\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 178 Step: 34532 Index:0.5689 R2:0.8524 0.5689 0.6013 RMSE:0.4246 0.7337 0.7058 Tau:0.7542 0.5814 0.4251\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 179 Step: 34726 Index:0.5758 R2:0.8577 0.5758 0.6095 RMSE:0.4541 0.7517 0.7328 Tau:0.7605 0.5826 0.4193\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 180 Step: 34920 Index:0.5616 R2:0.8552 0.5616 0.5966 RMSE:0.4307 0.7511 0.7241 Tau:0.7597 0.5833 0.4265\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 181 Step: 35114 Index:0.5670 R2:0.8552 0.5670 0.6110 RMSE:0.4529 0.7620 0.7285 Tau:0.7577 0.5709 0.4219\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 182 Step: 35308 Index:0.5908 R2:0.8538 0.5908 0.6112 RMSE:0.4346 0.7267 0.7114 Tau:0.7563 0.5826 0.4339\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 183 Step: 35502 Index:0.5721 R2:0.8568 0.5721 0.6044 RMSE:0.4235 0.7323 0.7096 Tau:0.7605 0.5713 0.4295\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 184 Step: 35696 Index:0.5837 R2:0.8577 0.5837 0.6014 RMSE:0.4163 0.7194 0.7071 Tau:0.7598 0.5814 0.4189\n",
      "Epoch: 185 Step: 35890 Index:0.5972 R2:0.8625 0.5972 0.6076 RMSE:0.4182 0.7125 0.7060 Tau:0.7650 0.5892 0.4312\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 186 Step: 36084 Index:0.5955 R2:0.8642 0.5955 0.6015 RMSE:0.4122 0.7143 0.7137 Tau:0.7674 0.5864 0.4330\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 187 Step: 36278 Index:0.5793 R2:0.8667 0.5793 0.6047 RMSE:0.4059 0.7250 0.7060 Tau:0.7707 0.5853 0.4298\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 188 Step: 36472 Index:0.5847 R2:0.8609 0.5847 0.6086 RMSE:0.4123 0.7195 0.7000 Tau:0.7621 0.5705 0.4291\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 189 Step: 36666 Index:0.5782 R2:0.8665 0.5782 0.6030 RMSE:0.4051 0.7292 0.7141 Tau:0.7689 0.5775 0.4318\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 190 Step: 36860 Index:0.5937 R2:0.8682 0.5937 0.6164 RMSE:0.4031 0.7098 0.6913 Tau:0.7710 0.5822 0.4291\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 191 Step: 37054 Index:0.5828 R2:0.8700 0.5828 0.6172 RMSE:0.3998 0.7207 0.6922 Tau:0.7722 0.5763 0.4261\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 192 Step: 37248 Index:0.5793 R2:0.8678 0.5793 0.5994 RMSE:0.4073 0.7232 0.7117 Tau:0.7716 0.5686 0.4279\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 193 Step: 37442 Index:0.5880 R2:0.8621 0.5880 0.5972 RMSE:0.4127 0.7151 0.7092 Tau:0.7663 0.5888 0.4305\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 194 Step: 37636 Index:0.5741 R2:0.8662 0.5741 0.6101 RMSE:0.4135 0.7372 0.7112 Tau:0.7725 0.5822 0.4263\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 195 Step: 37830 Index:0.5725 R2:0.8654 0.5725 0.6094 RMSE:0.4256 0.7505 0.7287 Tau:0.7702 0.5779 0.4316\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 196 Step: 38024 Index:0.5805 R2:0.8647 0.5805 0.6022 RMSE:0.4193 0.7311 0.7225 Tau:0.7684 0.5950 0.4223\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 197 Step: 38218 Index:0.5781 R2:0.8639 0.5781 0.6015 RMSE:0.4078 0.7269 0.7086 Tau:0.7671 0.5864 0.4289\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 198 Step: 38412 Index:0.5775 R2:0.8697 0.5775 0.6039 RMSE:0.3993 0.7275 0.7089 Tau:0.7713 0.5818 0.4275\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 199 Step: 38606 Index:0.5758 R2:0.8725 0.5758 0.6056 RMSE:0.4272 0.7548 0.7413 Tau:0.7756 0.5814 0.4336\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 200 Step: 38800 Index:0.5800 R2:0.8678 0.5800 0.5971 RMSE:0.4050 0.7262 0.7191 Tau:0.7728 0.5806 0.4165\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 201 Step: 38994 Index:0.5866 R2:0.8664 0.5866 0.6062 RMSE:0.4102 0.7160 0.6992 Tau:0.7712 0.5841 0.4309\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 202 Step: 39188 Index:0.5658 R2:0.8732 0.5658 0.5994 RMSE:0.3990 0.7342 0.7056 Tau:0.7759 0.5655 0.4257\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 203 Step: 39382 Index:0.5471 R2:0.8604 0.5471 0.5958 RMSE:0.4225 0.7750 0.7351 Tau:0.7626 0.5659 0.4404\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 204 Step: 39576 Index:0.5907 R2:0.8694 0.5907 0.6015 RMSE:0.4020 0.7130 0.7057 Tau:0.7723 0.5849 0.4341\n",
      "Epoch: 205 Step: 39770 Index:0.5978 R2:0.8742 0.5978 0.6131 RMSE:0.4252 0.7327 0.7289 Tau:0.7771 0.5923 0.4302\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 206 Step: 39964 Index:0.5935 R2:0.8755 0.5935 0.6021 RMSE:0.3900 0.7120 0.7090 Tau:0.7767 0.5993 0.4262\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 207 Step: 40158 Index:0.5833 R2:0.8783 0.5833 0.6006 RMSE:0.3875 0.7203 0.7080 Tau:0.7810 0.5880 0.4345\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 208 Step: 40352 Index:0.5755 R2:0.8712 0.5755 0.6093 RMSE:0.4489 0.7733 0.7555 Tau:0.7734 0.5833 0.4353\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 209 Step: 40546 Index:0.5774 R2:0.8795 0.5774 0.6089 RMSE:0.3910 0.7298 0.7062 Tau:0.7829 0.5798 0.4309\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 210 Step: 40740 Index:0.5918 R2:0.8836 0.5918 0.6120 RMSE:0.3852 0.7124 0.6982 Tau:0.7858 0.5888 0.4332\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 211 Step: 40934 Index:0.5964 R2:0.8784 0.5964 0.6087 RMSE:0.3872 0.7088 0.7057 Tau:0.7809 0.5919 0.4255\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 212 Step: 41128 Index:0.5921 R2:0.8748 0.5921 0.6026 RMSE:0.3905 0.7120 0.7085 Tau:0.7788 0.6020 0.4390\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 213 Step: 41322 Index:0.5742 R2:0.8803 0.5742 0.6270 RMSE:0.3884 0.7279 0.6837 Tau:0.7844 0.5744 0.4344\n",
      "Epoch: 214 Step: 41516 Index:0.6205 R2:0.8760 0.6205 0.6178 RMSE:0.3885 0.6907 0.7008 Tau:0.7784 0.6020 0.4304\n",
      "EarlyStopping counter: 1 out of 30\n",
      "Epoch: 215 Step: 41710 Index:0.5843 R2:0.8827 0.5843 0.6121 RMSE:0.3790 0.7190 0.6972 Tau:0.7855 0.5794 0.4316\n",
      "EarlyStopping counter: 2 out of 30\n",
      "Epoch: 216 Step: 41904 Index:0.5860 R2:0.8802 0.5860 0.6128 RMSE:0.3982 0.7356 0.7247 Tau:0.7832 0.5814 0.4412\n",
      "EarlyStopping counter: 3 out of 30\n",
      "Epoch: 217 Step: 42098 Index:0.6099 R2:0.8774 0.6099 0.5957 RMSE:0.3922 0.6945 0.7111 Tau:0.7810 0.5997 0.4266\n",
      "EarlyStopping counter: 4 out of 30\n",
      "Epoch: 218 Step: 42292 Index:0.6184 R2:0.8812 0.6184 0.6152 RMSE:0.3945 0.6874 0.6892 Tau:0.7847 0.6000 0.4375\n",
      "EarlyStopping counter: 5 out of 30\n",
      "Epoch: 219 Step: 42486 Index:0.5987 R2:0.8820 0.5987 0.6052 RMSE:0.3841 0.7167 0.7262 Tau:0.7842 0.5962 0.4249\n",
      "EarlyStopping counter: 6 out of 30\n",
      "Epoch: 220 Step: 42680 Index:0.6000 R2:0.8808 0.6000 0.6078 RMSE:0.4143 0.7317 0.7397 Tau:0.7840 0.5728 0.4316\n",
      "EarlyStopping counter: 7 out of 30\n",
      "Epoch: 221 Step: 42874 Index:0.6014 R2:0.8877 0.6014 0.6048 RMSE:0.3701 0.7056 0.7085 Tau:0.7912 0.5930 0.4394\n",
      "EarlyStopping counter: 8 out of 30\n",
      "Epoch: 222 Step: 43068 Index:0.5999 R2:0.8853 0.5999 0.6120 RMSE:0.3764 0.7114 0.7083 Tau:0.7885 0.5861 0.4317\n",
      "EarlyStopping counter: 9 out of 30\n",
      "Epoch: 223 Step: 43262 Index:0.5998 R2:0.8902 0.5998 0.6032 RMSE:0.3778 0.7136 0.7225 Tau:0.7956 0.5876 0.4403\n",
      "EarlyStopping counter: 10 out of 30\n",
      "Epoch: 224 Step: 43456 Index:0.5972 R2:0.8796 0.5972 0.5985 RMSE:0.3902 0.7113 0.7211 Tau:0.7819 0.5760 0.4286\n",
      "EarlyStopping counter: 11 out of 30\n",
      "Epoch: 225 Step: 43650 Index:0.6090 R2:0.8834 0.6090 0.6061 RMSE:0.3898 0.7034 0.7182 Tau:0.7865 0.5962 0.4291\n",
      "EarlyStopping counter: 12 out of 30\n",
      "Epoch: 226 Step: 43844 Index:0.5945 R2:0.8866 0.5945 0.6143 RMSE:0.3748 0.7097 0.6948 Tau:0.7888 0.5818 0.4359\n",
      "EarlyStopping counter: 13 out of 30\n",
      "Epoch: 227 Step: 44038 Index:0.5954 R2:0.8891 0.5954 0.5939 RMSE:0.3853 0.7099 0.7200 Tau:0.7941 0.5861 0.4349\n",
      "EarlyStopping counter: 14 out of 30\n",
      "Epoch: 228 Step: 44232 Index:0.5945 R2:0.8906 0.5945 0.6032 RMSE:0.3929 0.7219 0.7301 Tau:0.7940 0.5872 0.4396\n",
      "EarlyStopping counter: 15 out of 30\n",
      "Epoch: 229 Step: 44426 Index:0.5820 R2:0.8876 0.5820 0.6153 RMSE:0.3983 0.7479 0.7363 Tau:0.7918 0.5794 0.4369\n",
      "EarlyStopping counter: 16 out of 30\n",
      "Epoch: 230 Step: 44620 Index:0.5918 R2:0.8917 0.5918 0.6132 RMSE:0.3822 0.7197 0.7128 Tau:0.7951 0.5868 0.4351\n",
      "EarlyStopping counter: 17 out of 30\n",
      "Epoch: 231 Step: 44814 Index:0.5845 R2:0.8890 0.5845 0.5991 RMSE:0.3822 0.7239 0.7220 Tau:0.7919 0.5845 0.4428\n",
      "EarlyStopping counter: 18 out of 30\n",
      "Epoch: 232 Step: 45008 Index:0.6019 R2:0.8949 0.6019 0.6124 RMSE:0.3672 0.7014 0.6953 Tau:0.7992 0.5884 0.4343\n",
      "EarlyStopping counter: 19 out of 30\n",
      "Epoch: 233 Step: 45202 Index:0.5985 R2:0.8946 0.5985 0.6221 RMSE:0.3603 0.7078 0.6910 Tau:0.7981 0.5977 0.4441\n",
      "EarlyStopping counter: 20 out of 30\n",
      "Epoch: 234 Step: 45396 Index:0.5927 R2:0.8910 0.5927 0.6046 RMSE:0.3787 0.7240 0.7285 Tau:0.7939 0.5794 0.4370\n",
      "EarlyStopping counter: 21 out of 30\n",
      "Epoch: 235 Step: 45590 Index:0.5723 R2:0.8912 0.5723 0.6163 RMSE:0.3676 0.7309 0.6940 Tau:0.7957 0.5736 0.4422\n",
      "EarlyStopping counter: 22 out of 30\n",
      "Epoch: 236 Step: 45784 Index:0.6033 R2:0.8924 0.6033 0.6021 RMSE:0.3895 0.7082 0.7233 Tau:0.7972 0.5845 0.4405\n",
      "EarlyStopping counter: 23 out of 30\n",
      "Epoch: 237 Step: 45978 Index:0.6144 R2:0.8948 0.6144 0.6061 RMSE:0.3600 0.6923 0.7103 Tau:0.8001 0.6098 0.4414\n",
      "EarlyStopping counter: 24 out of 30\n",
      "Epoch: 238 Step: 46172 Index:0.5940 R2:0.8937 0.5940 0.6014 RMSE:0.3658 0.7098 0.7091 Tau:0.7994 0.5969 0.4513\n",
      "EarlyStopping counter: 25 out of 30\n",
      "Epoch: 239 Step: 46366 Index:0.6003 R2:0.8906 0.6003 0.5874 RMSE:0.3784 0.7114 0.7415 Tau:0.7937 0.6020 0.4401\n",
      "EarlyStopping counter: 26 out of 30\n",
      "Epoch: 240 Step: 46560 Index:0.6179 R2:0.8973 0.6179 0.6056 RMSE:0.3561 0.6881 0.7069 Tau:0.8018 0.6020 0.4387\n",
      "EarlyStopping counter: 27 out of 30\n",
      "Epoch: 241 Step: 46754 Index:0.5990 R2:0.8972 0.5990 0.6198 RMSE:0.3769 0.7193 0.7120 Tau:0.8016 0.5919 0.4469\n",
      "EarlyStopping counter: 28 out of 30\n",
      "Epoch: 242 Step: 46948 Index:0.6062 R2:0.8927 0.6062 0.6073 RMSE:0.3606 0.7025 0.7120 Tau:0.7967 0.5927 0.4452\n",
      "EarlyStopping counter: 29 out of 30\n",
      "Epoch: 243 Step: 47142 Index:0.6012 R2:0.8889 0.6012 0.5819 RMSE:0.3711 0.7026 0.7269 Tau:0.7915 0.5965 0.4334\n",
      "EarlyStopping counter: 30 out of 30\n",
      "Epoch: 244 Step: 47336 Index:0.6044 R2:0.8969 0.6044 0.6033 RMSE:0.3582 0.7004 0.7070 Tau:0.8008 0.6004 0.4353\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "    scheduler.step()\n",
    "    scheduler_AFSE.step()\n",
    "    scheduler_GRN.step()\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = val_r2\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 244 r2:0.6178 RMSE:0.7008 WTI:0.3558 AP:0.6104 Tau:0.4304 \n",
      " \n",
      " Top-1:0.4615 Top-1-fp:0.0000 \n",
      " Top-5:0.6119 Top-5-fp:0.0299 \n",
      " Top-10:0.6866 Top-10-fp:0.1194 \n",
      " Top-15:0.7065 Top-15-fp:0.1841 \n",
      " Top-20:0.6903 Top-20-fp:0.2388 \n",
      " Top-25:0.6716 Top-25-fp:0.3045 \n",
      " Top-30:0.6219 Top-30-fp:0.3756 \n",
      " Top-40:0.6738 Top-40-fp:0.4720 \n",
      " Top-50:0.7571 Top-50-fp:0.5261 \n",
      " \n",
      " Top50:0.6400 Top50-fp:0.0200 \n",
      " Top100:0.6500 Top100-fp:0.0700 \n",
      " Top150:0.6867 Top150-fp:0.1333 \n",
      " Top200:0.7050 Top200-fp:0.1850 \n",
      " Top250:0.6960 Top250-fp:0.2160 \n",
      " Top300:0.6833 Top300-fp:0.2633 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
